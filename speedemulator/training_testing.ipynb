{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb33cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from scipy.stats import dirichlet\n",
    "\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter, FileWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "tb_dir = \"tb_logs\"\n",
    "\n",
    "max_epochs = 100\n",
    "X_train = torch.randn(100, 8, requires_grad=True)\n",
    "F_train = torch.empty(100, dtype=torch.long).random_(1000)\n",
    "omegas = torch.tensor(dirichlet.rvs(np.ones(100)), dtype=torch.float).T\n",
    "omegas_0 = torch.ones_like(omegas) / len(omegas)\n",
    "dataset = torch.utils.data.TensorDataset(X_train, F_train, omegas, omegas_0)\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3648e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        V_hat,\n",
    "        n_p: int = 8,\n",
    "        n_e: int = 8,\n",
    "        n_hidden_1: int = 128,\n",
    "        n_hidden_2: int = 128,\n",
    "        n_hidden_3: int = 128,\n",
    "        n_hidden_4: int = 128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.l_1 = nn.Linear(n_p, n_hidden_1)\n",
    "        self.norm_1 = nn.LayerNorm(n_hidden_1)\n",
    "        self.dropout_1 = nn.Dropout(p=0.0)\n",
    "        self.l_2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.norm_2 = nn.LayerNorm(n_hidden_2)\n",
    "        self.dropout_2 = nn.Dropout(p=0.5)\n",
    "        self.l_3 = nn.Linear(n_hidden_2, n_hidden_3)\n",
    "        self.norm_3 = nn.LayerNorm(n_hidden_3)\n",
    "        self.dropout_3 = nn.Dropout(p=0.5)\n",
    "        self.l_4 = nn.Linear(n_hidden_3, n_hidden_4)\n",
    "        self.norm_4 = nn.LayerNorm(n_hidden_3)\n",
    "        self.dropout_4 = nn.Dropout(p=0.5)\n",
    "        self.l_5 = nn.Linear(n_hidden_4, n_e)\n",
    "\n",
    "        self.V_hat = torch.nn.Parameter(V_hat, requires_grad=False)\n",
    "\n",
    "\n",
    "    def forward(self, x, add_mean=False):\n",
    "        # Pass the input tensor through each of our operations\n",
    "\n",
    "        a_1 = self.l_1(x)\n",
    "        a_1 = self.norm_1(a_1)\n",
    "        a_1 = self.dropout_1(a_1)\n",
    "        z_1 = torch.relu(a_1)\n",
    "\n",
    "        a_2 = self.l_2(z_1)\n",
    "        a_2 = self.norm_2(a_2)\n",
    "        a_2 = self.dropout_2(a_2)\n",
    "        z_2 = torch.relu(a_2) + z_1\n",
    "\n",
    "        a_3 = self.l_3(z_2)\n",
    "        a_3 = self.norm_3(a_3)\n",
    "        a_3 = self.dropout_3(a_3)\n",
    "        z_3 = torch.relu(a_3) + z_2\n",
    "\n",
    "        a_4 = self.l_4(z_3)\n",
    "        a_4 = self.norm_3(a_4)\n",
    "        a_4 = self.dropout_3(a_4)\n",
    "        z_4 = torch.relu(a_4) + z_3\n",
    "\n",
    "        z_5 = self.l_5(z_4)\n",
    "\n",
    "        F_pred = z_5 @ self.V_hat.T\n",
    "\n",
    "        return F_pred\n",
    "    \n",
    "    def criterion_ae(self, F_pred, F_obs, omegas):\n",
    "        instance_misfit = torch.sum(torch.abs(F_pred - F_obs) ** 2, axis=1)\n",
    "        return torch.sum(instance_misfit * omegas.squeeze())\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = {\n",
    "        \"scheduler\": ReduceLROnPlateau(optimizer, verbose=True),\n",
    "        \"reduce_on_plateau\": True,\n",
    "        \"monitor\": \"loss\",\n",
    "        }\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, f, o, o_0 = batch\n",
    "        f_pred = self.forward(x)\n",
    "        loss = self.criterion_ae(f_pred, f, o)\n",
    "\n",
    "        return {\"loss\": loss, \"x\": x, \"f\": f, \"omegas\": o, \"omegas_0\": o_0}\n",
    "\n",
    "    def on_train_epoch_end(self, outputs):\n",
    "        x = []\n",
    "        f = []\n",
    "        omegas_0 = []\n",
    "        omegas = []\n",
    "        for k, out in enumerate(outputs[0]):\n",
    "            o = out[0][\"extra\"]\n",
    "            x.append(o[\"x\"])\n",
    "            f.append(o[\"f\"])\n",
    "            omegas.append(o[\"omegas\"])\n",
    "            omegas_0.append(o[\"omegas_0\"])\n",
    "        x = torch.vstack(x)\n",
    "        f = torch.vstack(f)\n",
    "        omegas = torch.vstack(omegas)\n",
    "        omegas_0 = torch.vstack(omegas_0)\n",
    "        self.trainer.model.eval()\n",
    "        f_pred_eval = self.forward(x)\n",
    "        self.trainer.model.train()\n",
    "        f_pred_train = self.forward(x)\n",
    "        train_loss_eval = self.criterion_ae(f_pred_eval, f, omegas)\n",
    "        train_loss_train = self.criterion_ae(f_pred_train, f, omegas)\n",
    "        test_loss_eval = self.criterion_ae(f_pred_eval, f, omegas_0)\n",
    "        test_loss_train = self.criterion_ae(f_pred_train, f, omegas_0)\n",
    "\n",
    "        self.log(\"train_loss_eval\", train_loss_eval, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_loss_train\", train_loss_train, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_loss_eval\", test_loss_eval, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_loss_train\", test_loss_train, on_step=False, on_epoch=True, prog_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e304d469",
   "metadata": {},
   "source": [
    "## Torch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2e40a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TestModel(X_train)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "writer = SummaryWriter(\"tb_logs\")\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    for x, f, o, o_0 in train_dataloader:\n",
    "        model.train()\n",
    "        f_hat = model(x)\n",
    "        loss = model.criterion_ae(f_hat, f, o)\n",
    "        writer.add_scalar(\"loss\", loss)\n",
    "\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    F_pred = model(X_train)\n",
    "    loss_train = model.criterion_ae(F_pred, F_train, omegas)\n",
    "    loss_test = model.criterion_ae(F_pred, F_train, omegas_0)\n",
    "    writer.add_scalar(\"loss_train\", loss_train, epoch)\n",
    "    writer.add_scalar(\"loss_test\", loss_test, epoch)\n",
    "    print(f\"epoch: {epoch}, train loss: {loss_train.item()}, test loss {loss_test.item()}\")\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7027a3e",
   "metadata": {},
   "source": [
    "## Set up Tensorboard Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5983b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"tb_logs\", name=\"Lightning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748cb2c3",
   "metadata": {},
   "source": [
    "# Lightning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TestModel(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d7917",
   "metadata": {},
   "source": [
    "## Set up LearningRate monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23705b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee9be3",
   "metadata": {},
   "source": [
    "## Run for max_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a88d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(callbacks=[lr_monitor], max_epochs=max_epochs, logger=logger)\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9df2ac",
   "metadata": {},
   "source": [
    "## Run with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"loss\", min_delta=0.0, patience=5, verbose=False, mode=\"min\", strict=True\n",
    "    )\n",
    "trainer = pl.Trainer(callbacks=[lr_monitor, early_stop_callback], logger=logger)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=max_epochs, logger=logger)\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d463f",
   "metadata": {},
   "source": [
    "## Load tensorboard extention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b4a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=\"tb_logs/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
