{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95516590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"Flux\")\n",
    "Pkg.add(\"NCDatasets\")\n",
    "Pkg.add(\"TSVD\")\n",
    "Pkg.add(\"Statistics\")\n",
    "Pkg.add(\"Compat\")\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "Pkg.add(\"Glob\")\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"Distributions\")\n",
    "Pkg.add(\"ProgressMeter\")\n",
    "Pkg.add(\"PyPlot\")\n",
    "Pkg.add(\"Random\")\n",
    "Pkg.add(\"SpecialFunctions\")\n",
    "Pkg.add(\"BSON\")\n",
    "using Turing\n",
    "using Flux\n",
    "using Flux: train!\n",
    "using TSVD\n",
    "using Statistics\n",
    "using LinearAlgebra\n",
    "using Compat\n",
    "using Glob\n",
    "using NCDatasets\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Distributions: Categorical, Dirichlet\n",
    "using ProgressMeter\n",
    "using PyPlot\n",
    "using Random\n",
    "using SpecialFunctions: loggamma\n",
    "using BSON: @save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbeac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_file =\"../data/observed_speeds/greenland_vel_mosaic250_v1_g9000m.nc\"\n",
    "d_obs = NCDataset(obs_file)\n",
    "v_obs = d_obs[\"velsurf_mag\"][:]\n",
    "v_obs = nomissing(v_obs, 0.0);\n",
    "idx = findall(v_obs .> 0)\n",
    "Obs = v_obs[idx];\n",
    "\n",
    "n_grid_points = size(idx)[1];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc033e",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = sort(glob(\"../tests/training_data/*.nc\"))\n",
    "\n",
    "nf = length(training_files)\n",
    "d = NCDataset(training_files[1], \"r\")\n",
    "v = d[\"velsurf_mag\"]\n",
    "nx, ny, nt = size(v)\n",
    "\n",
    "Data = zeros(n_grid_points, nf * nt)\n",
    "ids = zeros(Int64, nf)\n",
    "@showprogress for (k, training_file) in enumerate(training_files)\n",
    "    m_id = match(r\"id_(.+?)_\", training_file)\n",
    "    ids[k] = parse(Int, m_id[1])\n",
    "    d = NCDataset(training_file, \"r\")\n",
    "    v = d[\"velsurf_mag\"][:]\n",
    "    v = nomissing(v, 0.0)\n",
    "    Data[:, k] = v[idx]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031e1c0a",
   "metadata": {},
   "source": [
    "## Read training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee568a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = DataFrame(CSV.File(\"../data/samples/velocity_calibration_samples_50.csv\"))\n",
    "X_df = X_df[ [x in ids for x in X_df[!, :id]] ,:]\n",
    "X = transpose(Matrix(X_df[!, 2:9]))\n",
    "X_mean = mean(X, dims=2);\n",
    "X_std = std(X, dims=2);\n",
    "X_scaled = (X .- X_mean) ./ X_std;\n",
    "X_train = X_scaled;\n",
    "n_parameters, n_samples = size(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dd2995",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Log10-transform the training data and set -Inf to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = log10.(Data)\n",
    "F = replace!(F, -Inf=>0)\n",
    "dirichlet_dist = Dirichlet(n_samples, 1)\n",
    "\n",
    "area = ones(n_grid_points);\n",
    "area = area ./ sum(area);\n",
    "\n",
    "# Number of eigenglaciers\n",
    "q = 50;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c40cee",
   "metadata": {},
   "source": [
    "## Function to get Eigenglaciers using SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_eigenglaciers(omegas, F, q)\n",
    "    \n",
    "    F_mean = sum(F .* omegas, dims=2);\n",
    "    F_bar = F .- F_mean;\n",
    "\n",
    "    Z = diagm(sqrt.(omegas[1, :] * n_grid_points))\n",
    "    U, S, V = tsvd(Z * transpose(F_bar), q);\n",
    "    lamda = S.^2 / n_grid_points\n",
    "    V_hat = V * diagm(sqrt.(lamda));\n",
    "    \n",
    "    return V_hat, F_bar, F_mean\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f35efd",
   "metadata": {},
   "source": [
    "## Set up the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9370444",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "\n",
    "struct NNModel\n",
    "    chain::Chain\n",
    "    V_hat::AbstractArray\n",
    "    F_mean::AbstractArray\n",
    "end\n",
    "\n",
    "function (m::NNModel)(x, add_mean=false)\n",
    "    if add_mean\n",
    "        return V_hat * m.chain(x) .+ F_mean\n",
    "    else\n",
    "        return V_hat * m.chain(x)\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "# Call @functor to allow for training.\n",
    "Flux.@functor NNModel\n",
    "\n",
    "chain = Chain(\n",
    "    Dense(n_parameters, n_hidden),\n",
    "    LayerNorm(n_hidden),\n",
    "    Dropout(0.0),\n",
    "    Dense(n_hidden, n_hidden),\n",
    "    LayerNorm(n_hidden),\n",
    "    Dropout(0.5),\n",
    "    Dense(n_hidden, n_hidden),\n",
    "    LayerNorm(n_hidden),\n",
    "    Dropout(0.5),\n",
    "    Dense(n_hidden, n_hidden),\n",
    "    LayerNorm(n_hidden),\n",
    "    Dropout(0.3),\n",
    "    Dense(n_hidden, q, bias=false),\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_models = 1\n",
    "n_epochs = 101\n",
    "opt = Adam(0.1, (0.9, 0.8));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd855ed",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eefae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(y_pred, y, o) = sum(sum(abs.((y_pred - y)).^2 .* area, dims=1) .* o);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa23013",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = 1\n",
    "Random.seed!(model_index)\n",
    "omegas = transpose(rand(dirichlet_dist, 1))\n",
    "omegas_0 = omegas ./ size(omegas)[1];\n",
    "    \n",
    "V_hat, F_bar, F_mean = get_eigenglaciers(omegas, F, q);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975cf2ab",
   "metadata": {},
   "source": [
    "## We're ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bbf4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for model_index in 1:no_models\n",
    "    println(\"Training surrogate model \", model_index)\n",
    "    Random.seed!(model_index)\n",
    "    omegas = transpose(rand(dirichlet_dist, 1))\n",
    "    omegas_0 = omegas ./ size(omegas)[1];\n",
    "    \n",
    "    V_hat, F_bar, F_mean = get_eigenglaciers(omegas, F, q);\n",
    "    \n",
    "    train_loader = Flux.DataLoader((X_train, F_bar, omegas), batchsize = 128, shuffle = true)\n",
    "    model = NNModel(chain, V_hat, F_mean);\n",
    "    ps = Flux.params(model);\n",
    "    opt_state = Flux.setup(opt, model);\n",
    "    \n",
    "    println(\"  epoch, train_loss, test_loss\")\n",
    "    @showprogress for epoch in 1:n_epochs\n",
    "        for (x, y, o) in train_loader\n",
    "\n",
    "          # Calculate the gradient of the objective\n",
    "          # with respect to the parameters within the model:\n",
    "          grads = Flux.gradient(model) do m\n",
    "              y_pred = m(x)\n",
    "              loss(y_pred, y, o)\n",
    "          end\n",
    "\n",
    "          # Update the parameters so as to reduce the objective,\n",
    "          # according the chosen optimisation rule:\n",
    "          Flux.update!(opt_state, model, grads[1])\n",
    "        end\n",
    "        F_pred = model(X_scaled)\n",
    "        train_loss = loss(F_pred, F_bar, omegas)\n",
    "        test_loss = loss(F_pred, F_bar, omegas_0)\n",
    "        if epoch % 5 == 0\n",
    "            println(\"  \", epoch, \" \", train_loss, \" \", test_loss)\n",
    "        end\n",
    "    end\n",
    "    @save \"emulator_$model_index.bson\" model\n",
    "    push!(models, model)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e4257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_glaciers = 10\n",
    "p = ones(n_samples)\n",
    "p = p / sum(p)\n",
    "# This does sampling with replacement, need to figure out how to do\n",
    "# sampling without replacement\n",
    "P = Categorical(p)\n",
    "glaciers = rand(P, n_glaciers);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d80de5",
   "metadata": {},
   "source": [
    "## Now calcuate some metrics to assess the surrogate committee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b12c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_train = F\n",
    "maes = []\n",
    "for m in glaciers\n",
    "    for (model_index, model) in enumerate(models)\n",
    "        X_val = X_train[:, m]\n",
    "        Y_val = F_train[:, m]\n",
    "        Y_pred = model(X_val, true)\n",
    "        mae = Flux.mae(10 .^ mean(Y_pred, dims=2), 10 .^ mean(Y_val, dims=2))\n",
    "        push!(maes, mae)\n",
    "    end\n",
    "end\n",
    "mae = mean(maes)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130f140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
