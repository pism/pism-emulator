{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67cd721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "   | Name      | Type          | Params\n",
      "---------------------------------------------\n",
      "0  | l_1       | Linear        | 1.2 K \n",
      "1  | norm_1    | LayerNorm     | 256   \n",
      "2  | dropout_1 | Dropout       | 0     \n",
      "3  | l_2       | Linear        | 16.5 K\n",
      "4  | norm_2    | LayerNorm     | 256   \n",
      "5  | dropout_2 | Dropout       | 0     \n",
      "6  | l_3       | Linear        | 16.5 K\n",
      "7  | norm_3    | LayerNorm     | 256   \n",
      "8  | dropout_3 | Dropout       | 0     \n",
      "9  | l_4       | Linear        | 16.5 K\n",
      "10 | norm_4    | LayerNorm     | 256   \n",
      "11 | dropout_4 | Dropout       | 0     \n",
      "12 | l_5       | Linear        | 12.9 K\n",
      "13 | train_ae  | AbsoluteError | 0     \n",
      "14 | test_ae   | AbsoluteError | 0     \n",
      "---------------------------------------------\n",
      "64.6 K    Trainable params\n",
      "514 K     Non-trainable params\n",
      "579 K     Total params\n",
      "2.318     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 0:  44%|███████████████████████████████████████████████████████▌                                                                       | 7/16 [00:00<00:00, 106.33it/s, loss=20.5, v_num=14]Adjusting learning rate of group 0 to 9.9750e-03.\n",
      "Epoch 0:  50%|███████████████████████████████████████████████████████████████▌                                                               | 8/16 [00:00<00:00, 110.99it/s, loss=18.5, v_num=14]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 124.20it/s, loss=18.5, v_num=14, train_loss=32.50, test_loss=32.40]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████████████▎                                                   | 7/16 [00:00<00:00, 112.86it/s, loss=12.3, v_num=14, train_loss=32.50, test_loss=32.40]\u001b[AAdjusting learning rate of group 0 to 9.9501e-03.\n",
      "Epoch 1:  50%|██████████████████████████████████████████████                                              | 8/16 [00:00<00:00, 117.96it/s, loss=11.7, v_num=14, train_loss=32.50, test_loss=32.40]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 130.86it/s, loss=11.7, v_num=14, train_loss=12.70, test_loss=12.80]\u001b[A\n",
      "Epoch 2:  44%|████████████████████████████████████████▎                                                   | 7/16 [00:00<00:00, 109.55it/s, loss=5.32, v_num=14, train_loss=12.70, test_loss=12.80]\u001b[AAdjusting learning rate of group 0 to 9.9252e-03.\n",
      "Epoch 2:  50%|██████████████████████████████████████████████▌                                              | 8/16 [00:00<00:00, 114.21it/s, loss=4.9, v_num=14, train_loss=12.70, test_loss=12.80]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 130.31it/s, loss=4.9, v_num=14, train_loss=6.920, test_loss=6.900]\u001b[A\n",
      "Epoch 3:  44%|████████████████████████████████████████▎                                                   | 7/16 [00:00<00:00, 110.92it/s, loss=2.26, v_num=14, train_loss=6.920, test_loss=6.900]\u001b[AAdjusting learning rate of group 0 to 9.9004e-03.\n",
      "Epoch 3:  50%|██████████████████████████████████████████████                                              | 8/16 [00:00<00:00, 116.15it/s, loss=2.08, v_num=14, train_loss=6.920, test_loss=6.900]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 118.59it/s, loss=2.08, v_num=14, train_loss=4.030, test_loss=4.070]\u001b[A\n",
      "Epoch 4:  44%|████████████████████████████████████████▎                                                   | 7/16 [00:00<00:00, 100.97it/s, loss=1.23, v_num=14, train_loss=4.030, test_loss=4.070]\u001b[AAdjusting learning rate of group 0 to 9.8756e-03.\n",
      "Epoch 4:  50%|██████████████████████████████████████████████                                              | 8/16 [00:00<00:00, 106.99it/s, loss=1.13, v_num=14, train_loss=4.030, test_loss=4.070]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 121.99it/s, loss=1.13, v_num=14, train_loss=2.450, test_loss=2.460]\u001b[A\n",
      "Epoch 5:  44%|███████████████████████████████████████▊                                                   | 7/16 [00:00<00:00, 101.56it/s, loss=0.769, v_num=14, train_loss=2.450, test_loss=2.460]\u001b[AAdjusting learning rate of group 0 to 9.8509e-03.\n",
      "Epoch 5:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 106.64it/s, loss=0.732, v_num=14, train_loss=2.450, test_loss=2.460]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 119.72it/s, loss=0.732, v_num=14, train_loss=1.880, test_loss=1.900]\u001b[A\n",
      "Epoch 6:  44%|███████████████████████████████████████▊                                                   | 7/16 [00:00<00:00, 105.57it/s, loss=0.556, v_num=14, train_loss=1.880, test_loss=1.900]\u001b[AAdjusting learning rate of group 0 to 9.8263e-03.\n",
      "Epoch 6:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 109.70it/s, loss=0.529, v_num=14, train_loss=1.880, test_loss=1.900]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 124.65it/s, loss=0.529, v_num=14, train_loss=1.660, test_loss=1.670]\u001b[A\n",
      "Epoch 7:  44%|███████████████████████████████████████▊                                                   | 7/16 [00:00<00:00, 101.46it/s, loss=0.443, v_num=14, train_loss=1.660, test_loss=1.670]\u001b[AAdjusting learning rate of group 0 to 9.8017e-03.\n",
      "Epoch 7:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 107.33it/s, loss=0.422, v_num=14, train_loss=1.660, test_loss=1.670]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 123.51it/s, loss=0.422, v_num=14, train_loss=1.540, test_loss=1.560]\u001b[A\n",
      "Epoch 8:  44%|████████████████████████████████████████▎                                                   | 7/16 [00:00<00:00, 103.46it/s, loss=0.37, v_num=14, train_loss=1.540, test_loss=1.560]\u001b[AAdjusting learning rate of group 0 to 9.7772e-03.\n",
      "Epoch 8:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 107.95it/s, loss=0.357, v_num=14, train_loss=1.540, test_loss=1.560]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 114.45it/s, loss=0.357, v_num=14, train_loss=1.440, test_loss=1.460]\u001b[A\n",
      "Epoch 9:  44%|████████████████████████████████████████▎                                                   | 7/16 [00:00<00:00, 92.49it/s, loss=0.318, v_num=14, train_loss=1.440, test_loss=1.460]\u001b[AAdjusting learning rate of group 0 to 9.7528e-03.\n",
      "Epoch 9:  50%|██████████████████████████████████████████████                                              | 8/16 [00:00<00:00, 96.65it/s, loss=0.309, v_num=14, train_loss=1.440, test_loss=1.460]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 112.02it/s, loss=0.309, v_num=14, train_loss=1.380, test_loss=1.390]\u001b[A\n",
      "Epoch 10:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 105.54it/s, loss=0.296, v_num=14, train_loss=1.380, test_loss=1.390]\u001b[AAdjusting learning rate of group 0 to 9.7284e-03.\n",
      "Epoch 10:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 111.04it/s, loss=0.283, v_num=14, train_loss=1.380, test_loss=1.390]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 122.31it/s, loss=0.283, v_num=14, train_loss=1.340, test_loss=1.360]\u001b[A\n",
      "Epoch 11:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 109.95it/s, loss=0.269, v_num=14, train_loss=1.340, test_loss=1.360]\u001b[AAdjusting learning rate of group 0 to 9.7041e-03.\n",
      "Epoch 11:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 112.93it/s, loss=0.26, v_num=14, train_loss=1.340, test_loss=1.360]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 125.02it/s, loss=0.26, v_num=14, train_loss=1.310, test_loss=1.320]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 117.17it/s, loss=0.246, v_num=14, train_loss=1.310, test_loss=1.320]\u001b[AAdjusting learning rate of group 0 to 9.6798e-03.\n",
      "Epoch 12:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 121.35it/s, loss=0.241, v_num=14, train_loss=1.310, test_loss=1.320]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 129.86it/s, loss=0.241, v_num=14, train_loss=1.280, test_loss=1.290]\u001b[A\n",
      "Epoch 13:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 119.09it/s, loss=0.231, v_num=14, train_loss=1.280, test_loss=1.290]\u001b[AAdjusting learning rate of group 0 to 9.6556e-03.\n",
      "Epoch 13:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 123.83it/s, loss=0.225, v_num=14, train_loss=1.280, test_loss=1.290]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 142.71it/s, loss=0.225, v_num=14, train_loss=1.250, test_loss=1.270]\u001b[A\n",
      "Epoch 14:  44%|███████████████████████████████████████▊                                                   | 7/16 [00:00<00:00, 113.11it/s, loss=0.22, v_num=14, train_loss=1.250, test_loss=1.270]\u001b[AAdjusting learning rate of group 0 to 9.6315e-03.\n",
      "Epoch 14:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 118.77it/s, loss=0.213, v_num=14, train_loss=1.250, test_loss=1.270]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 132.97it/s, loss=0.213, v_num=14, train_loss=1.230, test_loss=1.250]\u001b[A\n",
      "Epoch 15:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 117.29it/s, loss=0.208, v_num=14, train_loss=1.230, test_loss=1.250]\u001b[AAdjusting learning rate of group 0 to 9.6074e-03.\n",
      "Epoch 15:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 122.48it/s, loss=0.205, v_num=14, train_loss=1.230, test_loss=1.250]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 136.68it/s, loss=0.205, v_num=14, train_loss=1.220, test_loss=1.230]\u001b[A\n",
      "Epoch 16:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 116.31it/s, loss=0.202, v_num=14, train_loss=1.220, test_loss=1.230]\u001b[AAdjusting learning rate of group 0 to 9.5834e-03.\n",
      "Epoch 16:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 120.95it/s, loss=0.198, v_num=14, train_loss=1.220, test_loss=1.230]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 137.36it/s, loss=0.198, v_num=14, train_loss=1.200, test_loss=1.210]\u001b[A\n",
      "Epoch 17:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 124.74it/s, loss=0.192, v_num=14, train_loss=1.200, test_loss=1.210]\u001b[AAdjusting learning rate of group 0 to 9.5594e-03.\n",
      "Epoch 17:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 126.47it/s, loss=0.19, v_num=14, train_loss=1.200, test_loss=1.210]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 139.00it/s, loss=0.19, v_num=14, train_loss=1.190, test_loss=1.200]\u001b[A\n",
      "Epoch 18:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 114.95it/s, loss=0.188, v_num=14, train_loss=1.190, test_loss=1.200]\u001b[AAdjusting learning rate of group 0 to 9.5355e-03.\n",
      "Epoch 18:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 118.32it/s, loss=0.185, v_num=14, train_loss=1.190, test_loss=1.200]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 133.03it/s, loss=0.185, v_num=14, train_loss=1.180, test_loss=1.190]\u001b[A\n",
      "Epoch 19:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 122.18it/s, loss=0.183, v_num=14, train_loss=1.180, test_loss=1.190]\u001b[AAdjusting learning rate of group 0 to 9.5117e-03.\n",
      "Epoch 19:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 126.30it/s, loss=0.178, v_num=14, train_loss=1.180, test_loss=1.190]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 138.25it/s, loss=0.178, v_num=14, train_loss=1.160, test_loss=1.180]\u001b[A\n",
      "Epoch 20:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 122.81it/s, loss=0.176, v_num=14, train_loss=1.160, test_loss=1.180]\u001b[AAdjusting learning rate of group 0 to 9.4879e-03.\n",
      "Epoch 20:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 126.61it/s, loss=0.173, v_num=14, train_loss=1.160, test_loss=1.180]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 141.67it/s, loss=0.173, v_num=14, train_loss=1.150, test_loss=1.170]\u001b[A\n",
      "Epoch 21:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 128.44it/s, loss=0.173, v_num=14, train_loss=1.150, test_loss=1.170]\u001b[AAdjusting learning rate of group 0 to 9.4642e-03.\n",
      "Epoch 21:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 132.42it/s, loss=0.168, v_num=14, train_loss=1.150, test_loss=1.170]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 146.91it/s, loss=0.168, v_num=14, train_loss=1.150, test_loss=1.160]\u001b[A\n",
      "Epoch 22:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 126.93it/s, loss=0.168, v_num=14, train_loss=1.150, test_loss=1.160]\u001b[AAdjusting learning rate of group 0 to 9.4405e-03.\n",
      "Epoch 22:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 130.54it/s, loss=0.164, v_num=14, train_loss=1.150, test_loss=1.160]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 141.15it/s, loss=0.164, v_num=14, train_loss=1.140, test_loss=1.150]\u001b[A\n",
      "Epoch 23:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 119.78it/s, loss=0.166, v_num=14, train_loss=1.140, test_loss=1.150]\u001b[AAdjusting learning rate of group 0 to 9.4169e-03.\n",
      "Epoch 23:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 124.37it/s, loss=0.161, v_num=14, train_loss=1.140, test_loss=1.150]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                           | 0/8 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andy/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | l_first       | Linear        | 1.2 K \n",
      "1 | norm_first    | LayerNorm     | 256   \n",
      "2 | dropout_first | Dropout       | 0     \n",
      "3 | dnn           | ModuleList    | 50.3 K\n",
      "4 | l_last        | Linear        | 12.9 K\n",
      "5 | train_ae      | AbsoluteError | 0     \n",
      "6 | test_ae       | AbsoluteError | 0     \n",
      "------------------------------------------------\n",
      "64.6 K    Trainable params\n",
      "514 K     Non-trainable params\n",
      "579 K     Total params\n",
      "2.318     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  44%|███████████████████████████████████████████████████████▌                                                                       | 7/16 [00:00<00:00, 112.30it/s, loss=18.7, v_num=15]Adjusting learning rate of group 0 to 9.9750e-03.\n",
      "Epoch 0:  50%|███████████████████████████████████████████████████████████████▌                                                               | 8/16 [00:00<00:00, 117.79it/s, loss=16.9, v_num=15]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 119.55it/s, loss=16.9, v_num=15, train_loss=34.40, test_loss=34.50]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████████████▎                                                   | 7/16 [00:00<00:00, 122.04it/s, loss=10.8, v_num=15, train_loss=34.40, test_loss=34.50]\u001b[AAdjusting learning rate of group 0 to 9.9501e-03.\n",
      "Epoch 1:  50%|██████████████████████████████████████████████                                              | 8/16 [00:00<00:00, 126.97it/s, loss=10.2, v_num=15, train_loss=34.40, test_loss=34.50]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 140.98it/s, loss=10.2, v_num=15, train_loss=13.80, test_loss=13.80]\u001b[A\n",
      "Epoch 2:  44%|████████████████████████████████████████▎                                                   | 7/16 [00:00<00:00, 121.14it/s, loss=4.25, v_num=15, train_loss=13.80, test_loss=13.80]\u001b[AAdjusting learning rate of group 0 to 9.9252e-03.\n",
      "Epoch 2:  50%|███████████████████████████████████████████████▌                                               | 8/16 [00:00<00:00, 126.35it/s, loss=4, v_num=15, train_loss=13.80, test_loss=13.80]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 143.52it/s, loss=4, v_num=15, train_loss=7.690, test_loss=7.740]\u001b[A\n",
      "Epoch 3:  44%|████████████████████████████████████████▎                                                   | 7/16 [00:00<00:00, 126.16it/s, loss=1.71, v_num=15, train_loss=7.690, test_loss=7.740]\u001b[AAdjusting learning rate of group 0 to 9.9004e-03.\n",
      "Epoch 3:  50%|██████████████████████████████████████████████                                              | 8/16 [00:00<00:00, 132.20it/s, loss=1.59, v_num=15, train_loss=7.690, test_loss=7.740]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 144.08it/s, loss=1.59, v_num=15, train_loss=4.240, test_loss=4.260]\u001b[A\n",
      "Epoch 4:  44%|████████████████████████████████████████▎                                                   | 7/16 [00:00<00:00, 107.03it/s, loss=0.99, v_num=15, train_loss=4.240, test_loss=4.260]\u001b[AAdjusting learning rate of group 0 to 9.8756e-03.\n",
      "Epoch 4:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 111.07it/s, loss=0.913, v_num=15, train_loss=4.240, test_loss=4.260]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 130.73it/s, loss=0.913, v_num=15, train_loss=2.730, test_loss=2.760]\u001b[A\n",
      "Epoch 5:  44%|███████████████████████████████████████▊                                                   | 7/16 [00:00<00:00, 108.31it/s, loss=0.633, v_num=15, train_loss=2.730, test_loss=2.760]\u001b[AAdjusting learning rate of group 0 to 9.8509e-03.\n",
      "Epoch 5:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 111.82it/s, loss=0.596, v_num=15, train_loss=2.730, test_loss=2.760]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 128.39it/s, loss=0.596, v_num=15, train_loss=2.190, test_loss=2.230]\u001b[A\n",
      "Epoch 6:  44%|███████████████████████████████████████▊                                                   | 7/16 [00:00<00:00, 121.57it/s, loss=0.494, v_num=15, train_loss=2.190, test_loss=2.230]\u001b[AAdjusting learning rate of group 0 to 9.8263e-03.\n",
      "Epoch 6:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 126.11it/s, loss=0.473, v_num=15, train_loss=2.190, test_loss=2.230]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 136.98it/s, loss=0.473, v_num=15, train_loss=1.900, test_loss=1.930]\u001b[A\n",
      "Epoch 7:  44%|███████████████████████████████████████▊                                                   | 7/16 [00:00<00:00, 110.75it/s, loss=0.414, v_num=15, train_loss=1.900, test_loss=1.930]\u001b[AAdjusting learning rate of group 0 to 9.8017e-03.\n",
      "Epoch 7:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 114.95it/s, loss=0.403, v_num=15, train_loss=1.900, test_loss=1.930]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 129.07it/s, loss=0.403, v_num=15, train_loss=1.700, test_loss=1.740]\u001b[A\n",
      "Epoch 8:  44%|████████████████████████████████████████▎                                                   | 7/16 [00:00<00:00, 114.99it/s, loss=0.37, v_num=15, train_loss=1.700, test_loss=1.740]\u001b[AAdjusting learning rate of group 0 to 9.7772e-03.\n",
      "Epoch 8:  50%|██████████████████████████████████████████████                                              | 8/16 [00:00<00:00, 120.37it/s, loss=0.36, v_num=15, train_loss=1.700, test_loss=1.740]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 137.99it/s, loss=0.36, v_num=15, train_loss=1.600, test_loss=1.630]\u001b[A\n",
      "Epoch 9:  44%|████████████████████████████████████████▎                                                   | 7/16 [00:00<00:00, 121.10it/s, loss=0.34, v_num=15, train_loss=1.600, test_loss=1.630]\u001b[AAdjusting learning rate of group 0 to 9.7528e-03.\n",
      "Epoch 9:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 124.64it/s, loss=0.333, v_num=15, train_loss=1.600, test_loss=1.630]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 136.81it/s, loss=0.333, v_num=15, train_loss=1.520, test_loss=1.550]\u001b[A\n",
      "Epoch 10:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 119.17it/s, loss=0.314, v_num=15, train_loss=1.520, test_loss=1.550]\u001b[AAdjusting learning rate of group 0 to 9.7284e-03.\n",
      "Epoch 10:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 123.44it/s, loss=0.308, v_num=15, train_loss=1.520, test_loss=1.550]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 138.55it/s, loss=0.308, v_num=15, train_loss=1.460, test_loss=1.490]\u001b[A\n",
      "Epoch 11:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 115.08it/s, loss=0.297, v_num=15, train_loss=1.460, test_loss=1.490]\u001b[AAdjusting learning rate of group 0 to 9.7041e-03.\n",
      "Epoch 11:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 118.63it/s, loss=0.29, v_num=15, train_loss=1.460, test_loss=1.490]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 132.03it/s, loss=0.29, v_num=15, train_loss=1.420, test_loss=1.440]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 113.10it/s, loss=0.282, v_num=15, train_loss=1.420, test_loss=1.440]\u001b[AAdjusting learning rate of group 0 to 9.6798e-03.\n",
      "Epoch 12:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 117.88it/s, loss=0.273, v_num=15, train_loss=1.420, test_loss=1.440]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 134.33it/s, loss=0.273, v_num=15, train_loss=1.370, test_loss=1.400]\u001b[A\n",
      "Epoch 13:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 121.22it/s, loss=0.265, v_num=15, train_loss=1.370, test_loss=1.400]\u001b[AAdjusting learning rate of group 0 to 9.6556e-03.\n",
      "Epoch 13:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 125.41it/s, loss=0.258, v_num=15, train_loss=1.370, test_loss=1.400]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 143.76it/s, loss=0.258, v_num=15, train_loss=1.340, test_loss=1.370]\u001b[A\n",
      "Epoch 14:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 117.10it/s, loss=0.256, v_num=15, train_loss=1.340, test_loss=1.370]\u001b[AAdjusting learning rate of group 0 to 9.6315e-03.\n",
      "Epoch 14:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 121.56it/s, loss=0.251, v_num=15, train_loss=1.340, test_loss=1.370]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 134.13it/s, loss=0.251, v_num=15, train_loss=1.320, test_loss=1.340]\u001b[A\n",
      "Epoch 15:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 115.21it/s, loss=0.249, v_num=15, train_loss=1.320, test_loss=1.340]\u001b[AAdjusting learning rate of group 0 to 9.6074e-03.\n",
      "Epoch 15:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 118.71it/s, loss=0.243, v_num=15, train_loss=1.320, test_loss=1.340]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 129.47it/s, loss=0.243, v_num=15, train_loss=1.290, test_loss=1.310]\u001b[A\n",
      "Epoch 16:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 117.02it/s, loss=0.235, v_num=15, train_loss=1.290, test_loss=1.310]\u001b[AAdjusting learning rate of group 0 to 9.5834e-03.\n",
      "Epoch 16:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 121.66it/s, loss=0.23, v_num=15, train_loss=1.290, test_loss=1.310]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 140.00it/s, loss=0.23, v_num=15, train_loss=1.270, test_loss=1.290]\u001b[A\n",
      "Epoch 17:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 119.26it/s, loss=0.232, v_num=15, train_loss=1.270, test_loss=1.290]\u001b[AAdjusting learning rate of group 0 to 9.5594e-03.\n",
      "Epoch 17:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 123.65it/s, loss=0.226, v_num=15, train_loss=1.270, test_loss=1.290]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 141.35it/s, loss=0.226, v_num=15, train_loss=1.250, test_loss=1.270]\u001b[A\n",
      "Epoch 18:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 120.16it/s, loss=0.222, v_num=15, train_loss=1.250, test_loss=1.270]\u001b[AAdjusting learning rate of group 0 to 9.5355e-03.\n",
      "Epoch 18:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 124.82it/s, loss=0.217, v_num=15, train_loss=1.250, test_loss=1.270]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 138.62it/s, loss=0.217, v_num=15, train_loss=1.230, test_loss=1.250]\u001b[A\n",
      "Epoch 19:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 105.24it/s, loss=0.218, v_num=15, train_loss=1.230, test_loss=1.250]\u001b[AAdjusting learning rate of group 0 to 9.5117e-03.\n",
      "Epoch 19:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 108.95it/s, loss=0.212, v_num=15, train_loss=1.230, test_loss=1.250]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 118.25it/s, loss=0.212, v_num=15, train_loss=1.220, test_loss=1.240]\u001b[A\n",
      "Epoch 20:  44%|███████████████████████████████████████▊                                                   | 7/16 [00:00<00:00, 83.27it/s, loss=0.207, v_num=15, train_loss=1.220, test_loss=1.240]\u001b[AAdjusting learning rate of group 0 to 9.4879e-03.\n",
      "Epoch 20:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 87.48it/s, loss=0.203, v_num=15, train_loss=1.220, test_loss=1.240]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 100.78it/s, loss=0.203, v_num=15, train_loss=1.210, test_loss=1.220]\u001b[A\n",
      "Epoch 21:  44%|███████████████████████████████████████▊                                                   | 7/16 [00:00<00:00, 94.23it/s, loss=0.208, v_num=15, train_loss=1.210, test_loss=1.220]\u001b[AAdjusting learning rate of group 0 to 9.4642e-03.\n",
      "Epoch 21:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 99.45it/s, loss=0.201, v_num=15, train_loss=1.210, test_loss=1.220]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 118.02it/s, loss=0.201, v_num=15, train_loss=1.200, test_loss=1.220]\u001b[A\n",
      "Epoch 22:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 105.90it/s, loss=0.199, v_num=15, train_loss=1.200, test_loss=1.220]\u001b[AAdjusting learning rate of group 0 to 9.4405e-03.\n",
      "Epoch 22:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 111.04it/s, loss=0.194, v_num=15, train_loss=1.200, test_loss=1.220]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 127.73it/s, loss=0.194, v_num=15, train_loss=1.180, test_loss=1.200]\u001b[A\n",
      "Epoch 23:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 112.03it/s, loss=0.195, v_num=15, train_loss=1.180, test_loss=1.200]\u001b[AAdjusting learning rate of group 0 to 9.4169e-03.\n",
      "Epoch 23:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 115.28it/s, loss=0.189, v_num=15, train_loss=1.180, test_loss=1.200]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 129.31it/s, loss=0.189, v_num=15, train_loss=1.170, test_loss=1.190]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 107.29it/s, loss=0.192, v_num=15, train_loss=1.170, test_loss=1.190]\u001b[AAdjusting learning rate of group 0 to 9.3934e-03.\n",
      "Epoch 24:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 111.47it/s, loss=0.188, v_num=15, train_loss=1.170, test_loss=1.190]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 130.46it/s, loss=0.188, v_num=15, train_loss=1.160, test_loss=1.180]\u001b[A\n",
      "Epoch 25:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 115.36it/s, loss=0.183, v_num=15, train_loss=1.160, test_loss=1.180]\u001b[AAdjusting learning rate of group 0 to 9.3699e-03.\n",
      "Epoch 25:  50%|█████████████████████████████████████████████                                             | 8/16 [00:00<00:00, 119.30it/s, loss=0.179, v_num=15, train_loss=1.160, test_loss=1.180]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 133.14it/s, loss=0.179, v_num=15, train_loss=1.150, test_loss=1.170]\u001b[A\n",
      "Epoch 26:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 114.53it/s, loss=0.183, v_num=15, train_loss=1.150, test_loss=1.170]\u001b[AAdjusting learning rate of group 0 to 9.3465e-03.\n",
      "Epoch 26:  50%|█████████████████████████████████████████████▌                                             | 8/16 [00:00<00:00, 119.30it/s, loss=0.18, v_num=15, train_loss=1.150, test_loss=1.170]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 134.33it/s, loss=0.18, v_num=15, train_loss=1.140, test_loss=1.160]\u001b[A\n",
      "Epoch 27:  44%|███████████████████████████████████████▍                                                  | 7/16 [00:00<00:00, 114.27it/s, loss=0.178, v_num=15, train_loss=1.140, test_loss=1.160]\u001b[A"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torchmetrics.utilities.checks import _check_same_shape\n",
    "from torchmetrics import Metric\n",
    "import pytorch_lightning as pl\n",
    "from scipy.stats import dirichlet\n",
    "import numpy as np\n",
    "\n",
    "def _absolute_error_update(\n",
    "    preds: Tensor, target: Tensor, omegas: Tensor, area: Tensor\n",
    ") -> Tensor:\n",
    "    _check_same_shape(preds, target)\n",
    "    diff = torch.abs(preds - target)\n",
    "    sum_abs_error = torch.sum(diff * diff * area, axis=1)\n",
    "    absolute_error = torch.sum(sum_abs_error * omegas.squeeze())\n",
    "    return absolute_error\n",
    "\n",
    "\n",
    "def _absolute_error_compute(absolute_error) -> Tensor:\n",
    "    return absolute_error\n",
    "\n",
    "\n",
    "def absolute_error(\n",
    "    preds: Tensor, target: Tensor, omegas: Tensor, area: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes squared absolute error\n",
    "    Args:\n",
    "        preds: estimated labels\n",
    "        target: ground truth labels\n",
    "        omegas: weights\n",
    "        area: area of each cell\n",
    "    Return:\n",
    "        Tensor with absolute error\n",
    "    Example:\n",
    "        >>> x = torch.tensor([[0, 1, 2, 3], [1, 2, 3, 4]]).T\n",
    "        >>> y = torch.tensor([[0, 1, 2, 1], [2, 3, 4, 4]]).T\n",
    "        >>> o = torch.tensor([0.25, 0.25, 0.3, 0.2])\n",
    "        >>> a = torch.tensor([0.25, 0.25])\n",
    "        >>> absolute_error(x, y, o, a)\n",
    "        tensor(0.4000)\n",
    "    \"\"\"\n",
    "    sum_abs_error = _absolute_error_update(preds, target, omegas, area)\n",
    "    return _absolute_error_compute(sum_abs_error)\n",
    "\n",
    "\n",
    "class AbsoluteError(Metric):\n",
    "    def __init__(self, compute_on_step: bool = True, dist_sync_on_step=False):\n",
    "        # call `self.add_state`for every internal state that is needed for the metrics computations\n",
    "        # dist_reduce_fx indicates the function that should be used to reduce\n",
    "        # state from multiple processes\n",
    "        super().__init__(\n",
    "            compute_on_step=compute_on_step, dist_sync_on_step=dist_sync_on_step\n",
    "        )\n",
    "\n",
    "        self.add_state(\"sum_abs_error\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: Tensor, target: Tensor, omegas: Tensor, area: Tensor):\n",
    "        \"\"\"\n",
    "        Update state with predictions and targets, and area.\n",
    "        Args:\n",
    "            preds: Predictions from model\n",
    "            target: Ground truth values\n",
    "            omegas: Weights\n",
    "            area: Area of each cell\n",
    "        \"\"\"\n",
    "        sum_abs_error = _absolute_error_update(preds, target, omegas, area)\n",
    "        self.sum_abs_error += sum_abs_error\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Computes absolute error over state.\n",
    "        \"\"\"\n",
    "        return _absolute_error_compute(self.sum_abs_error)\n",
    "\n",
    "    @property\n",
    "    def is_differentiable(self):\n",
    "        return True\n",
    "\n",
    "\n",
    "class NNEmulator(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_parameters,\n",
    "        n_eigenglaciers,\n",
    "        V_hat,\n",
    "        F_mean,\n",
    "        area,\n",
    "        hparams,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        n_hidden_1 = self.hparams.n_hidden_1\n",
    "        n_hidden_2 = self.hparams.n_hidden_2\n",
    "        n_hidden_3 = self.hparams.n_hidden_3\n",
    "        n_hidden_4 = self.hparams.n_hidden_4\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.l_1 = nn.Linear(n_parameters, n_hidden_1)\n",
    "        self.norm_1 = nn.LayerNorm(n_hidden_1)\n",
    "        self.dropout_1 = nn.Dropout(p=0.0)\n",
    "        self.l_2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.norm_2 = nn.LayerNorm(n_hidden_2)\n",
    "        self.dropout_2 = nn.Dropout(p=0.5)\n",
    "        self.l_3 = nn.Linear(n_hidden_2, n_hidden_3)\n",
    "        self.norm_3 = nn.LayerNorm(n_hidden_3)\n",
    "        self.dropout_3 = nn.Dropout(p=0.5)\n",
    "        self.l_4 = nn.Linear(n_hidden_3, n_hidden_4)\n",
    "        self.norm_4 = nn.LayerNorm(n_hidden_3)\n",
    "        self.dropout_4 = nn.Dropout(p=0.5)\n",
    "        self.l_5 = nn.Linear(n_hidden_4, n_eigenglaciers)\n",
    "\n",
    "        self.V_hat = torch.nn.Parameter(V_hat, requires_grad=False)\n",
    "        self.F_mean = torch.nn.Parameter(F_mean, requires_grad=False)\n",
    "\n",
    "        self.register_buffer(\"area\", area)\n",
    "\n",
    "        self.train_ae = AbsoluteError()\n",
    "        self.test_ae = AbsoluteError()\n",
    "\n",
    "    def forward(self, x, add_mean=False):\n",
    "        # Pass the input tensor through each of our operations\n",
    "\n",
    "        a_1 = self.l_1(x)\n",
    "        a_1 = self.norm_1(a_1)\n",
    "        a_1 = self.dropout_1(a_1)\n",
    "        z_1 = torch.relu(a_1)\n",
    "\n",
    "        a_2 = self.l_2(z_1)\n",
    "        a_2 = self.norm_2(a_2)\n",
    "        a_2 = self.dropout_2(a_2)\n",
    "        z_2 = torch.relu(a_2) + z_1\n",
    "\n",
    "        a_3 = self.l_3(z_2)\n",
    "        a_3 = self.norm_3(a_3)\n",
    "        a_3 = self.dropout_3(a_3)\n",
    "        z_3 = torch.relu(a_3) + z_2\n",
    "\n",
    "        a_4 = self.l_4(z_3)\n",
    "        a_4 = self.norm_3(a_4)\n",
    "        a_4 = self.dropout_3(a_4)\n",
    "        z_4 = torch.relu(a_4) + z_3\n",
    "\n",
    "        z_5 = self.l_5(z_4)\n",
    "        if add_mean:\n",
    "            F_pred = z_5 @ self.V_hat.T + self.F_mean\n",
    "        else:\n",
    "            F_pred = z_5 @ self.V_hat.T\n",
    "\n",
    "        return F_pred\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"NNEmulator\")\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "        parser.add_argument(\"--n_hidden_1\", type=int, default=128)\n",
    "        parser.add_argument(\"--n_hidden_2\", type=int, default=128)\n",
    "        parser.add_argument(\"--n_hidden_3\", type=int, default=128)\n",
    "        parser.add_argument(\"--n_hidden_4\", type=int, default=128)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=0.01)\n",
    "\n",
    "        return parent_parser\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), self.hparams.learning_rate, weight_decay=0.0\n",
    "        )\n",
    "        # This is an approximation to Doug's version:\n",
    "        scheduler = {\n",
    "            \"scheduler\": ExponentialLR(optimizer, 0.9975, verbose=True),\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, f, o, _ = batch\n",
    "        f_pred = self.forward(x)\n",
    "        loss = absolute_error(f_pred, f, o, self.area)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, f, o, o_0 = batch\n",
    "        f_pred = self.forward(x)\n",
    "\n",
    "        self.log(\"train_loss\", self.train_ae(f_pred, f, o, self.area))\n",
    "        self.log(\"test_loss\", self.test_ae(f_pred, f, o_0, self.area))\n",
    "\n",
    "        return {\"x\": x, \"f\": f, \"f_pred\": f_pred, \"o\": o, \"o_0\": o_0}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            self.train_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_loss\",\n",
    "            self.test_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "\n",
    "# As NNEmulator but number of hidden layers can be specified \n",
    "# with n_hidden_layers\n",
    "class DNNEmulator(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_parameters: int,\n",
    "        n_eigenglaciers: int,\n",
    "        V_hat: Tensor,\n",
    "        F_mean: Tensor,\n",
    "        area: Tensor,\n",
    "        hparams,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        n_layers = self.hparams.n_layers\n",
    "        n_hidden = self.hparams.n_hidden\n",
    "\n",
    "        if isinstance(n_hidden, int):\n",
    "            n_hidden = [n_hidden] * (n_layers - 1)\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.l_first = nn.Linear(n_parameters, n_hidden[0])\n",
    "        self.norm_first = nn.LayerNorm(n_hidden[0])\n",
    "        self.dropout_first = nn.Dropout(p=0.0)\n",
    "\n",
    "        models = []\n",
    "        for n in range(n_layers - 2):\n",
    "            models.append(\n",
    "                nn.Sequential(\n",
    "                    OrderedDict(\n",
    "                        [\n",
    "                            (\"Linear\", nn.Linear(n_hidden[n], n_hidden[n + 1])),\n",
    "                            (\"LayerNorm\", nn.LayerNorm(n_hidden[n + 1])),\n",
    "                            (\"Dropout\", nn.Dropout(p=0.1)),\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        self.dnn = nn.ModuleList(models)\n",
    "        self.l_last = nn.Linear(n_hidden[-1], n_eigenglaciers)\n",
    "\n",
    "        self.V_hat = torch.nn.Parameter(V_hat, requires_grad=False)\n",
    "        self.F_mean = torch.nn.Parameter(F_mean, requires_grad=False)\n",
    "\n",
    "        self.register_buffer(\"area\", area)\n",
    "\n",
    "        self.train_ae = AbsoluteError()\n",
    "        self.test_ae = AbsoluteError()\n",
    "\n",
    "    def forward(self, x, add_mean=False):\n",
    "        # Pass the input tensor through each of our operations\n",
    "\n",
    "        a = self.l_first(x)\n",
    "        a = self.norm_first(a)\n",
    "        a = self.dropout_first(a)\n",
    "        z = torch.relu(a)\n",
    "\n",
    "        for dnn in self.dnn:\n",
    "            a = dnn(z)\n",
    "            z = torch.relu(a) + z\n",
    "\n",
    "        z_last = self.l_last(z)\n",
    "\n",
    "        if add_mean:\n",
    "            F_pred = z_last @ self.V_hat.T + self.F_mean\n",
    "        else:\n",
    "            F_pred = z_last @ self.V_hat.T\n",
    "\n",
    "        return F_pred\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"NNEmulator\")\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "        parser.add_argument(\"--n_hidden\", default=128)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=0.01)\n",
    "\n",
    "        return parent_parser\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), self.hparams.learning_rate, weight_decay=0.0\n",
    "        )\n",
    "        # This is an approximation to Doug's version:\n",
    "        scheduler = {\n",
    "            \"scheduler\": ExponentialLR(optimizer, 0.9975, verbose=True),\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, f, o, _ = batch\n",
    "        f_pred = self.forward(x)\n",
    "        loss = absolute_error(f_pred, f, o, self.area)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, f, o, o_0 = batch\n",
    "        f_pred = self.forward(x)\n",
    "\n",
    "        self.log(\"train_loss\", self.train_ae(f_pred, f, o, self.area))\n",
    "        self.log(\"test_loss\", self.test_ae(f_pred, f, o_0, self.area))\n",
    "\n",
    "        return {\"x\": x, \"f\": f, \"f_pred\": f_pred, \"o\": o, \"o_0\": o_0}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            self.train_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_loss\",\n",
    "            self.test_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "max_epochs = 100\n",
    "num_workers = 4\n",
    "hparams = {\"n_hidden\": 128, \n",
    "           \"n_hidden_1\": 128, \n",
    "           \"n_hidden_2\": 128, \n",
    "           \"n_hidden_3\": 128, \n",
    "           \"n_hidden_4\": 128, \n",
    "           \"n_layers\": 5,\n",
    "           \"learning_rate\": 0.01}        \n",
    "\n",
    "n_eigenglaciers = 100\n",
    "n_samples = 979\n",
    "n_parameters = 8\n",
    "n_grid_points = 5097\n",
    "X_train = torch.randn(n_samples, n_parameters)\n",
    "Y_train = torch.randn(n_samples, n_grid_points)\n",
    "V_hat = torch.randn(n_grid_points, n_eigenglaciers)\n",
    "F_mean = torch.randn(n_grid_points)\n",
    "area = torch.ones_like(F_mean) / n_grid_points\n",
    "\n",
    "omegas = torch.Tensor(dirichlet.rvs(np.ones(n_samples))).T\n",
    "omegas = omegas.type_as(X_train)\n",
    "omegas_0 = torch.ones_like(omegas) / len(omegas)\n",
    "\n",
    "training_data = TensorDataset(X_train, Y_train, omegas, omegas_0)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# train and val data loader are the same because we use BayesBag/Bootstrapping to avoid overfitting\n",
    "# by generating 50 emulators, each with different weights \"omegas\"\n",
    "\n",
    "trainer_e = pl.Trainer(\n",
    "    deterministic=True,\n",
    "    num_sanity_val_steps=0,\n",
    "    max_epochs=max_epochs,\n",
    ")\n",
    "\n",
    "e = NNEmulator(\n",
    "    n_parameters,\n",
    "    n_eigenglaciers,\n",
    "    V_hat,\n",
    "    F_mean,\n",
    "    area,\n",
    "    hparams,\n",
    ")\n",
    "\n",
    "trainer_e.fit(e, train_loader, train_loader)\n",
    "\n",
    "trainer_de = pl.Trainer(\n",
    "    deterministic=True,\n",
    "    num_sanity_val_steps=0,\n",
    "    max_epochs=max_epochs,\n",
    ")\n",
    "\n",
    "de = DNNEmulator(\n",
    "    n_parameters,\n",
    "    n_eigenglaciers,\n",
    "    V_hat,\n",
    "    F_mean,\n",
    "    area,\n",
    "    hparams,\n",
    ")\n",
    "\n",
    "trainer_de.fit(de, train_loader, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "054b18fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "Y_pred_e = e(X_train, add_mean=True)\n",
    "Y_pred_de = de(X_train, add_mean=True)\n",
    "print(torch.allclose(Y_pred_e, Y_pred_de, rtol=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1dd1d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5687, -0.7332,  0.2741,  ..., -1.4356, -2.9116,  1.3660],\n",
       "        [ 0.1260, -0.3927,  0.9446,  ..., -0.5491, -2.8917,  1.5394],\n",
       "        [ 1.7578, -1.1101,  1.3278,  ..., -3.0206, -3.5944,  2.6618],\n",
       "        ...,\n",
       "        [ 0.7693, -0.9358,  0.5076,  ..., -2.9734, -3.7563,  2.7836],\n",
       "        [ 0.0236, -1.8986, -0.9425,  ..., -1.8011, -2.8463,  2.2073],\n",
       "        [ 0.6712, -0.8488,  0.3387,  ..., -2.4844, -3.3133,  1.8092]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68452206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5963, -0.7897,  1.4969,  ..., -1.6196, -2.9901,  4.3355],\n",
       "        [ 1.5535,  0.0247, -0.0095,  ..., -1.5606, -4.7245,  2.0926],\n",
       "        [ 0.0278, -1.4039,  1.0444,  ..., -1.6318, -3.6089,  2.5104],\n",
       "        ...,\n",
       "        [ 1.0046, -0.8259, -2.0149,  ..., -0.6841, -3.9244,  1.2808],\n",
       "        [ 1.0927,  0.3701, -0.9025,  ..., -1.2624, -2.7328,  3.2205],\n",
       "        [ 1.8100,  0.0811,  1.1136,  ..., -2.6391, -3.6600,  2.6641]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9d61c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
