{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b427af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../pismemulator/\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import dirichlet\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from nnemulator import NNEmulator, DNNEmulator, PISMDataset, PISMDataModule\n",
    "from utils import plot_eigenglaciers, plot_validation\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torchmetrics.utilities.checks import _check_same_shape\n",
    "from torchmetrics import Metric\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from pismemulator.metrics import AbsoluteError, absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5bb0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 2\n",
    "emulator_dir = \"test_dnn\"\n",
    "model_index = 0\n",
    "train_size = 1.0\n",
    "num_workers = 4\n",
    "hparams = {\"n_hidden\": 128, \n",
    "           \"n_hidden_1\": 128, \n",
    "           \"n_hidden_2\": 128, \n",
    "           \"n_hidden_3\": 128, \n",
    "           \"n_hidden_4\": 128, \n",
    "           \"n_layers\": 5,\n",
    "           \"learning_rate\": 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e3d0526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following simulations are missing:\n",
      "   [337, 595, 539, 542]\n",
      "  ... adjusting priors\n",
      "  Loading data sets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "996it [00:29, 34.10it/s]\n",
      "/Users/andy/base/pism-emulator/speedemulator/../pismemulator/nnemulator.py:471: RuntimeWarning: divide by zero encountered in log10\n",
      "  response = np.log10(response)\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 0\n",
      "Generating eigenglaciers\n",
      "...using the first 99 eigen values\n"
     ]
    }
   ],
   "source": [
    "    dataset = PISMDataset(\n",
    "        data_dir=\"../data/speeds_v2\",\n",
    "        samples_file=\"../data/samples/velocity_calibration_samples_100.csv\",\n",
    "        target_file=\"../data/observed_speeds/greenland_obs_g1800m.nc\",\n",
    "        thinning_factor=5,\n",
    "    )\n",
    "\n",
    "    X = dataset.X\n",
    "    F = dataset.Y\n",
    "    area = dataset.normed_area\n",
    "    n_grid_points = dataset.n_grid_points\n",
    "    n_parameters = dataset.n_parameters\n",
    "    n_samples = dataset.n_samples\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    pl.seed_everything(0)\n",
    "    np.random.seed(model_index)\n",
    "\n",
    "    if not os.path.isdir(emulator_dir):\n",
    "        os.makedirs(emulator_dir)\n",
    "        os.makedirs(os.path.join(emulator_dir, \"emulator\"))\n",
    "\n",
    "    print(f\"Training model {model_index}\")\n",
    "    omegas = torch.Tensor(dirichlet.rvs(np.ones(n_samples))).T\n",
    "    omegas = omegas.type_as(X)\n",
    "    omegas_0 = torch.ones_like(omegas) / len(omegas)\n",
    "\n",
    "    if train_size == 1.0:\n",
    "        data_loader = PISMDataModule(X, F, omegas, omegas_0, num_workers=num_workers)\n",
    "    else:\n",
    "        data_loader = PISMDataModule(X, F, omegas, omegas_0, train_size=train_size, num_workers=num_workers)\n",
    "\n",
    "    data_loader.prepare_data()\n",
    "    data_loader.setup(stage=\"fit\")\n",
    "    n_eigenglaciers = data_loader.n_eigenglaciers\n",
    "    V_hat = data_loader.V_hat\n",
    "    F_mean = data_loader.F_mean\n",
    "    F_train = data_loader.F_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be6cc828",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if train_size == 1.0:\n",
    "        train_loader = data_loader.train_all_loader\n",
    "        val_loader = data_loader.val_all_loader\n",
    "    else:\n",
    "        train_loader = data_loader.train_loader\n",
    "        val_loader = data_loader.val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "517b4e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "   | Name      | Type          | Params\n",
      "---------------------------------------------\n",
      "0  | l_1       | Linear        | 1.2 K \n",
      "1  | norm_1    | LayerNorm     | 256   \n",
      "2  | dropout_1 | Dropout       | 0     \n",
      "3  | l_2       | Linear        | 16.5 K\n",
      "4  | norm_2    | LayerNorm     | 256   \n",
      "5  | dropout_2 | Dropout       | 0     \n",
      "6  | l_3       | Linear        | 16.5 K\n",
      "7  | norm_3    | LayerNorm     | 256   \n",
      "8  | dropout_3 | Dropout       | 0     \n",
      "9  | l_4       | Linear        | 16.5 K\n",
      "10 | norm_4    | LayerNorm     | 256   \n",
      "11 | dropout_4 | Dropout       | 0     \n",
      "12 | l_5       | Linear        | 12.9 K\n",
      "13 | train_ae  | AbsoluteError | 0     \n",
      "14 | test_ae   | AbsoluteError | 0     \n",
      "---------------------------------------------\n",
      "64.6 K    Trainable params\n",
      "2.1 M     Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.506     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  44%|████▍     | 7/16 [00:00<00:00, 17.21it/s, loss=0.0401, v_num=10]Adjusting learning rate of group 0 to 9.9750e-03.\n",
      "Epoch 0:  56%|█████▋    | 9/16 [00:00<00:00, 17.69it/s, loss=0.0355, v_num=10]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  12%|█▎        | 1/8 [00:00<00:02,  3.15it/s]\u001b[A\n",
      "Epoch 0:  81%|████████▏ | 13/16 [00:01<00:00, 13.74it/s, loss=0.0355, v_num=10]\n",
      "Epoch 0: 100%|██████████| 16/16 [00:01<00:00, 13.14it/s, loss=0.0355, v_num=10, train_loss=0.0389, test_loss=0.0389]\n",
      "Epoch 1:  44%|████▍     | 7/16 [00:00<00:00, 18.11it/s, loss=0.0227, v_num=10, train_loss=0.0389, test_loss=0.0389]   Adjusting learning rate of group 0 to 9.9501e-03.\n",
      "Epoch 1:  50%|█████     | 8/16 [00:00<00:00, 15.26it/s, loss=0.0214, v_num=10, train_loss=0.0389, test_loss=0.0389]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  75%|███████▌  | 12/16 [00:01<00:00, 12.62it/s, loss=0.0214, v_num=10, train_loss=0.0389, test_loss=0.0389]\n",
      "Validating:  50%|█████     | 4/8 [00:00<00:00,  9.99it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 16/16 [00:01<00:00, 12.86it/s, loss=0.0214, v_num=10, train_loss=0.0289, test_loss=0.0287]\n",
      "Epoch 2:  44%|████▍     | 7/16 [00:00<00:00, 19.12it/s, loss=0.00945, v_num=10, train_loss=0.0289, test_loss=0.0287]  Adjusting learning rate of group 0 to 9.9252e-03.\n",
      "Epoch 2:  50%|█████     | 8/16 [00:00<00:00, 16.59it/s, loss=0.00873, v_num=10, train_loss=0.0289, test_loss=0.0287]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  75%|███████▌  | 12/16 [00:00<00:00, 13.51it/s, loss=0.00873, v_num=10, train_loss=0.0289, test_loss=0.0287]\n",
      "Validating:  50%|█████     | 4/8 [00:00<00:00, 10.34it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 16/16 [00:01<00:00, 13.10it/s, loss=0.00873, v_num=10, train_loss=0.018, test_loss=0.0182] \n",
      "Epoch 3:  44%|████▍     | 7/16 [00:00<00:00, 18.36it/s, loss=0.00446, v_num=10, train_loss=0.018, test_loss=0.0182]   Adjusting learning rate of group 0 to 9.9004e-03.\n",
      "Epoch 3:  50%|█████     | 8/16 [00:00<00:00, 15.96it/s, loss=0.00392, v_num=10, train_loss=0.018, test_loss=0.0182]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  75%|███████▌  | 12/16 [00:00<00:00, 13.36it/s, loss=0.00392, v_num=10, train_loss=0.018, test_loss=0.0182]\n",
      "Validating:  50%|█████     | 4/8 [00:00<00:00, 10.41it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 16/16 [00:01<00:00, 13.13it/s, loss=0.00392, v_num=10, train_loss=0.012, test_loss=0.0121]\n",
      "Epoch 4:  44%|████▍     | 7/16 [00:00<00:00, 18.15it/s, loss=0.00287, v_num=10, train_loss=0.012, test_loss=0.0121]   Adjusting learning rate of group 0 to 9.8756e-03.\n",
      "Epoch 4:  50%|█████     | 8/16 [00:00<00:00, 15.51it/s, loss=0.00264, v_num=10, train_loss=0.012, test_loss=0.0121]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  75%|███████▌  | 12/16 [00:00<00:00, 13.59it/s, loss=0.00264, v_num=10, train_loss=0.012, test_loss=0.0121]\n",
      "Validating:  50%|█████     | 4/8 [00:00<00:00, 11.37it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 16/16 [00:01<00:00, 13.61it/s, loss=0.00264, v_num=10, train_loss=0.0101, test_loss=0.0101]\n",
      "Epoch 5:  44%|████▍     | 7/16 [00:00<00:00, 17.81it/s, loss=0.00212, v_num=10, train_loss=0.0101, test_loss=0.0101]   Adjusting learning rate of group 0 to 9.8509e-03.\n",
      "Epoch 5:  50%|█████     | 8/16 [00:00<00:00, 15.28it/s, loss=0.00198, v_num=10, train_loss=0.0101, test_loss=0.0101]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  75%|███████▌  | 12/16 [00:00<00:00, 13.09it/s, loss=0.00198, v_num=10, train_loss=0.0101, test_loss=0.0101]\n",
      "Validating:  50%|█████     | 4/8 [00:00<00:00, 10.81it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 16/16 [00:01<00:00, 13.15it/s, loss=0.00198, v_num=10, train_loss=0.00978, test_loss=0.00988]\n",
      "Epoch 6:  44%|████▍     | 7/16 [00:00<00:00, 18.53it/s, loss=0.00168, v_num=10, train_loss=0.00978, test_loss=0.00988]   Adjusting learning rate of group 0 to 9.8263e-03.\n",
      "Epoch 6:  50%|█████     | 8/16 [00:00<00:00, 16.22it/s, loss=0.00161, v_num=10, train_loss=0.00978, test_loss=0.00988]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  75%|███████▌  | 12/16 [00:00<00:00, 13.43it/s, loss=0.00161, v_num=10, train_loss=0.00978, test_loss=0.00988]\n",
      "Validating:  50%|█████     | 4/8 [00:00<00:00, 10.34it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 16/16 [00:01<00:00, 13.39it/s, loss=0.00161, v_num=10, train_loss=0.00895, test_loss=0.00914]\n",
      "Epoch 7:  44%|████▍     | 7/16 [00:00<00:00, 18.33it/s, loss=0.00146, v_num=10, train_loss=0.00895, test_loss=0.00914]   Adjusting learning rate of group 0 to 9.8017e-03.\n",
      "Epoch 7:  50%|█████     | 8/16 [00:00<00:00, 16.06it/s, loss=0.00143, v_num=10, train_loss=0.00895, test_loss=0.00914]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  75%|███████▌  | 12/16 [00:01<00:00, 12.98it/s, loss=0.00143, v_num=10, train_loss=0.00895, test_loss=0.00914]\n",
      "Validating:  50%|█████     | 4/8 [00:00<00:00,  9.95it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 16/16 [00:01<00:00, 12.70it/s, loss=0.00143, v_num=10, train_loss=0.00888, test_loss=0.00898]\n",
      "Epoch 8:  44%|████▍     | 7/16 [00:00<00:00, 15.96it/s, loss=0.00138, v_num=10, train_loss=0.00888, test_loss=0.00898]   Adjusting learning rate of group 0 to 9.7772e-03.\n",
      "Epoch 8:  50%|█████     | 8/16 [00:00<00:00, 13.16it/s, loss=0.00135, v_num=10, train_loss=0.00888, test_loss=0.00898]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  75%|███████▌  | 12/16 [00:01<00:00, 11.74it/s, loss=0.00135, v_num=10, train_loss=0.00888, test_loss=0.00898]\n",
      "Validating:  50%|█████     | 4/8 [00:00<00:00, 10.05it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 16/16 [00:01<00:00, 11.91it/s, loss=0.00135, v_num=10, train_loss=0.00809, test_loss=0.00827]\n",
      "Epoch 9:  44%|████▍     | 7/16 [00:00<00:00, 16.24it/s, loss=0.00129, v_num=10, train_loss=0.00809, test_loss=0.00827]   Adjusting learning rate of group 0 to 9.7528e-03.\n",
      "Epoch 9:  50%|█████     | 8/16 [00:00<00:00, 13.46it/s, loss=0.00124, v_num=10, train_loss=0.00809, test_loss=0.00827]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  75%|███████▌  | 12/16 [00:01<00:00, 12.10it/s, loss=0.00124, v_num=10, train_loss=0.00809, test_loss=0.00827]\n",
      "Validating:  50%|█████     | 4/8 [00:00<00:00, 10.57it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 16/16 [00:01<00:00, 12.06it/s, loss=0.00124, v_num=10, train_loss=0.00798, test_loss=0.00817]\n",
      "Epoch 10:  44%|████▍     | 7/16 [00:00<00:00, 18.14it/s, loss=0.00121, v_num=10, train_loss=0.00798, test_loss=0.00817]  Adjusting learning rate of group 0 to 9.7284e-03.\n",
      "Epoch 10:  50%|█████     | 8/16 [00:00<00:00, 14.97it/s, loss=0.00116, v_num=10, train_loss=0.00798, test_loss=0.00817]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  75%|███████▌  | 12/16 [00:01<00:00, 12.95it/s, loss=0.00116, v_num=10, train_loss=0.00798, test_loss=0.00817]\n",
      "Validating:  50%|█████     | 4/8 [00:00<00:00, 10.72it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 16/16 [00:01<00:00, 13.14it/s, loss=0.00116, v_num=10, train_loss=0.00774, test_loss=0.00792]\n",
      "Epoch 11:  44%|████▍     | 7/16 [00:00<00:00, 17.70it/s, loss=0.00114, v_num=10, train_loss=0.00774, test_loss=0.00792]   Adjusting learning rate of group 0 to 9.7041e-03.\n",
      "Epoch 11:  50%|█████     | 8/16 [00:00<00:00, 15.75it/s, loss=0.00111, v_num=10, train_loss=0.00774, test_loss=0.00792]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f6b4ca24700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andy/miniconda3/envs/pytorch/lib/python3.8/logging/__init__.py\", line 227, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "    trainer_e = pl.Trainer(\n",
    "        deterministic=True,\n",
    "        num_sanity_val_steps=0,\n",
    "        max_epochs=max_epochs,\n",
    "    )\n",
    "\n",
    "    e = NNEmulator(\n",
    "        n_parameters,\n",
    "        n_eigenglaciers,\n",
    "        V_hat,\n",
    "        F_mean,\n",
    "        area,\n",
    "        hparams,\n",
    "    )\n",
    "\n",
    "    trainer_e.fit(e, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829f787e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | l_first       | Linear        | 1.2 K \n",
      "1 | norm_first    | LayerNorm     | 256   \n",
      "2 | dropout_first | Dropout       | 0     \n",
      "3 | dnn           | ModuleList    | 50.3 K\n",
      "4 | l_last        | Linear        | 12.8 K\n",
      "5 | train_ae      | AbsoluteError | 0     \n",
      "6 | test_ae       | AbsoluteError | 0     \n",
      "------------------------------------------------\n",
      "64.5 K    Trainable params\n",
      "2.0 M     Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.424     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd1773ece5e4dc9bcd8eb4d130eaef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Predictions and targets are expected to have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer_de \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      2\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m     num_sanity_val_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      4\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mmax_epochs,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m de \u001b[38;5;241m=\u001b[39m DNNEmulator(\n\u001b[1;32m      7\u001b[0m     n_parameters,\n\u001b[1;32m      8\u001b[0m     n_eigenglaciers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     hparams,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrainer_de\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mde\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:552\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    547\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    548\u001b[0m )\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_connector\u001b[38;5;241m.\u001b[39mresume_start()\n\u001b[0;32m--> 552\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:922\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m--> 922\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstart_predicting(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 990\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/accelerators/accelerator.py:92\u001b[0m, in \u001b[0;36mAccelerator.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:161\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:1000\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1000\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_global_zero \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_bar_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_bar_callback\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m-> 1035\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m# enable train mode\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:1122\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1122\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_sanity_check_end()\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/base.py:111\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:110\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28menumerate\u001b[39m(dataloader)\n\u001b[1;32m    108\u001b[0m dl_max_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_batches[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_dataloader_idx]\n\u001b[0;32m--> 110\u001b[0m dl_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_dataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_dataloaders\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs\u001b[38;5;241m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/base.py:111\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:111\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, dataloader_iter, dataloader_idx, dl_max_batches, num_dataloaders)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# lightning module methods\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_step_and_end\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 111\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_step_end(output)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:158\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/accelerators/accelerator.py:211\u001b[0m, in \u001b[0;36mAccelerator.validation_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"The actual validation step.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m          (only if multiple val dataloaders used)\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mval_step_context(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mval_step_context():\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:178\u001b[0m, in \u001b[0;36mTrainingTypePlugin.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/pism-emulator/speedemulator/../pismemulator/nnemulator.py:142\u001b[0m, in \u001b[0;36mDNNEmulator.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    139\u001b[0m x, f, o, o_0 \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    140\u001b[0m f_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_ae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marea\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_ae(f_pred, f, o_0, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marea))\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m: f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m: f_pred, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m: o, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo_0\u001b[39m\u001b[38;5;124m\"\u001b[39m: o_0}\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchmetrics/metric.py:192\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TorchMetricsUserError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Metric shouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be synced when performing ``update``. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHINT: Did you forget to call ``unsync`` ?.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_on_step:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_sync_on_step\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchmetrics/metric.py:250\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pismemulator/metrics.py:83\u001b[0m, in \u001b[0;36mAbsoluteError.update\u001b[0;34m(self, preds, target, omegas, area)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds: Tensor, target: Tensor, omegas: Tensor, area: Tensor):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    Update state with predictions and targets, and area.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m        area: Area of each cell\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     sum_abs_error \u001b[38;5;241m=\u001b[39m \u001b[43m_absolute_error_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momegas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marea\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_abs_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sum_abs_error\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pismemulator/metrics.py:28\u001b[0m, in \u001b[0;36m_absolute_error_update\u001b[0;34m(preds, target, omegas, area)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_absolute_error_update\u001b[39m(\n\u001b[1;32m     26\u001b[0m     preds: Tensor, target: Tensor, omegas: Tensor, area: Tensor\n\u001b[1;32m     27\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 28\u001b[0m     \u001b[43m_check_same_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     diff \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(preds \u001b[38;5;241m-\u001b[39m target)\n\u001b[1;32m     30\u001b[0m     sum_abs_error \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(diff \u001b[38;5;241m*\u001b[39m diff \u001b[38;5;241m*\u001b[39m area, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchmetrics/utilities/checks.py:26\u001b[0m, in \u001b[0;36m_check_same_shape\u001b[0;34m(preds, target)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"Check that predictions and target have the same shape, else raise error.\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m target\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and targets are expected to have the same shape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Predictions and targets are expected to have the same shape"
     ]
    }
   ],
   "source": [
    "    trainer_de = pl.Trainer(\n",
    "        deterministic=True,\n",
    "        num_sanity_val_steps=1,\n",
    "        max_epochs=max_epochs,\n",
    "    )\n",
    "    de = DNNEmulator(\n",
    "        n_parameters,\n",
    "        n_eigenglaciers,\n",
    "        V_hat,\n",
    "        F_mean,\n",
    "        area,\n",
    "        hparams,\n",
    "    )\n",
    "\n",
    "    trainer_de.fit(de, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3de48d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
