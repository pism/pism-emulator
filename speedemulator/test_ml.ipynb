{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c99d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torchmetrics.utilities.checks import _check_same_shape\n",
    "from torchmetrics import Metric\n",
    "import pytorch_lightning as pl\n",
    "from scipy.stats import dirichlet\n",
    "import numpy as np\n",
    "\n",
    "def _absolute_error_update(\n",
    "    preds: Tensor, target: Tensor, omegas: Tensor, area: Tensor\n",
    ") -> Tensor:\n",
    "    _check_same_shape(preds, target)\n",
    "    diff = torch.abs(preds - target)\n",
    "    sum_abs_error = torch.sum(diff * diff * area, axis=1)\n",
    "    absolute_error = torch.sum(sum_abs_error * omegas.squeeze())\n",
    "    return absolute_error\n",
    "\n",
    "\n",
    "def _absolute_error_compute(absolute_error) -> Tensor:\n",
    "    return absolute_error\n",
    "\n",
    "\n",
    "def absolute_error(\n",
    "    preds: Tensor, target: Tensor, omegas: Tensor, area: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes squared absolute error\n",
    "    Args:\n",
    "        preds: estimated labels\n",
    "        target: ground truth labels\n",
    "        omegas: weights\n",
    "        area: area of each cell\n",
    "    Return:\n",
    "        Tensor with absolute error\n",
    "    Example:\n",
    "        >>> x = torch.tensor([[0, 1, 2, 3], [1, 2, 3, 4]]).T\n",
    "        >>> y = torch.tensor([[0, 1, 2, 1], [2, 3, 4, 4]]).T\n",
    "        >>> o = torch.tensor([0.25, 0.25, 0.3, 0.2])\n",
    "        >>> a = torch.tensor([0.25, 0.25])\n",
    "        >>> absolute_error(x, y, o, a)\n",
    "        tensor(0.4000)\n",
    "    \"\"\"\n",
    "    sum_abs_error = _absolute_error_update(preds, target, omegas, area)\n",
    "    return _absolute_error_compute(sum_abs_error)\n",
    "\n",
    "\n",
    "class AbsoluteError(Metric):\n",
    "    def __init__(self, compute_on_step: bool = True, dist_sync_on_step=False):\n",
    "        # call `self.add_state`for every internal state that is needed for the metrics computations\n",
    "        # dist_reduce_fx indicates the function that should be used to reduce\n",
    "        # state from multiple processes\n",
    "        super().__init__(\n",
    "            compute_on_step=compute_on_step, dist_sync_on_step=dist_sync_on_step\n",
    "        )\n",
    "\n",
    "        self.add_state(\"sum_abs_error\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: Tensor, target: Tensor, omegas: Tensor, area: Tensor):\n",
    "        \"\"\"\n",
    "        Update state with predictions and targets, and area.\n",
    "        Args:\n",
    "            preds: Predictions from model\n",
    "            target: Ground truth values\n",
    "            omegas: Weights\n",
    "            area: Area of each cell\n",
    "        \"\"\"\n",
    "        sum_abs_error = _absolute_error_update(preds, target, omegas, area)\n",
    "        self.sum_abs_error += sum_abs_error\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Computes absolute error over state.\n",
    "        \"\"\"\n",
    "        return _absolute_error_compute(self.sum_abs_error)\n",
    "\n",
    "    @property\n",
    "    def is_differentiable(self):\n",
    "        return True\n",
    "\n",
    "\n",
    "class NNEmulator(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_parameters,\n",
    "        n_eigenglaciers,\n",
    "        V_hat,\n",
    "        F_mean,\n",
    "        area,\n",
    "        hparams,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        n_hidden_1 = self.hparams.n_hidden_1\n",
    "        n_hidden_2 = self.hparams.n_hidden_2\n",
    "        n_hidden_3 = self.hparams.n_hidden_3\n",
    "        n_hidden_4 = self.hparams.n_hidden_4\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.l_1 = nn.Linear(n_parameters, n_hidden_1)\n",
    "        self.norm_1 = nn.LayerNorm(n_hidden_1)\n",
    "        self.dropout_1 = nn.Dropout(p=0.0)\n",
    "        self.l_2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.norm_2 = nn.LayerNorm(n_hidden_2)\n",
    "        self.dropout_2 = nn.Dropout(p=0.5)\n",
    "        self.l_3 = nn.Linear(n_hidden_2, n_hidden_3)\n",
    "        self.norm_3 = nn.LayerNorm(n_hidden_3)\n",
    "        self.dropout_3 = nn.Dropout(p=0.5)\n",
    "        self.l_4 = nn.Linear(n_hidden_3, n_hidden_4)\n",
    "        self.norm_4 = nn.LayerNorm(n_hidden_3)\n",
    "        self.dropout_4 = nn.Dropout(p=0.5)\n",
    "        self.l_5 = nn.Linear(n_hidden_4, n_eigenglaciers)\n",
    "\n",
    "        self.V_hat = torch.nn.Parameter(V_hat, requires_grad=False)\n",
    "        self.F_mean = torch.nn.Parameter(F_mean, requires_grad=False)\n",
    "\n",
    "        self.register_buffer(\"area\", area)\n",
    "\n",
    "        self.train_ae = AbsoluteError()\n",
    "        self.test_ae = AbsoluteError()\n",
    "\n",
    "    def forward(self, x, add_mean=False):\n",
    "        # Pass the input tensor through each of our operations\n",
    "\n",
    "        a_1 = self.l_1(x)\n",
    "        a_1 = self.norm_1(a_1)\n",
    "        a_1 = self.dropout_1(a_1)\n",
    "        z_1 = torch.relu(a_1)\n",
    "\n",
    "        a_2 = self.l_2(z_1)\n",
    "        a_2 = self.norm_2(a_2)\n",
    "        a_2 = self.dropout_2(a_2)\n",
    "        z_2 = torch.relu(a_2) + z_1\n",
    "\n",
    "        a_3 = self.l_3(z_2)\n",
    "        a_3 = self.norm_3(a_3)\n",
    "        a_3 = self.dropout_3(a_3)\n",
    "        z_3 = torch.relu(a_3) + z_2\n",
    "\n",
    "        a_4 = self.l_4(z_3)\n",
    "        a_4 = self.norm_3(a_4)\n",
    "        a_4 = self.dropout_3(a_4)\n",
    "        z_4 = torch.relu(a_4) + z_3\n",
    "\n",
    "        z_5 = self.l_5(z_4)\n",
    "        if add_mean:\n",
    "            F_pred = z_5 @ self.V_hat.T + self.F_mean\n",
    "        else:\n",
    "            F_pred = z_5 @ self.V_hat.T\n",
    "\n",
    "        return F_pred\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"NNEmulator\")\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "        parser.add_argument(\"--n_hidden_1\", type=int, default=128)\n",
    "        parser.add_argument(\"--n_hidden_2\", type=int, default=128)\n",
    "        parser.add_argument(\"--n_hidden_3\", type=int, default=128)\n",
    "        parser.add_argument(\"--n_hidden_4\", type=int, default=128)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=0.01)\n",
    "\n",
    "        return parent_parser\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), self.hparams.learning_rate, weight_decay=0.0\n",
    "        )\n",
    "        # This is an approximation to Doug's version:\n",
    "        scheduler = {\n",
    "            \"scheduler\": ExponentialLR(optimizer, 0.9975, verbose=True),\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, f, o, _ = batch\n",
    "        f_pred = self.forward(x)\n",
    "        loss = absolute_error(f_pred, f, o, self.area)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, f, o, o_0 = batch\n",
    "        f_pred = self.forward(x)\n",
    "\n",
    "        self.log(\"train_loss\", self.train_ae(f_pred, f, o, self.area))\n",
    "        self.log(\"test_loss\", self.test_ae(f_pred, f, o_0, self.area))\n",
    "\n",
    "        return {\"x\": x, \"f\": f, \"f_pred\": f_pred, \"o\": o, \"o_0\": o_0}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            self.train_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_loss\",\n",
    "            self.test_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "\n",
    "# As NNEmulator but number of hidden layers can be specified \n",
    "# with n_hidden_layers\n",
    "class DNNEmulator(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_parameters: int,\n",
    "        n_eigenglaciers: int,\n",
    "        V_hat: Tensor,\n",
    "        F_mean: Tensor,\n",
    "        area: Tensor,\n",
    "        hparams,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        n_layers = self.hparams.n_layers\n",
    "        n_hidden = self.hparams.n_hidden\n",
    "\n",
    "        if isinstance(n_hidden, int):\n",
    "            n_hidden = [n_hidden] * (n_layers - 1)\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.l_first = nn.Linear(n_parameters, n_hidden[0])\n",
    "        self.norm_first = nn.LayerNorm(n_hidden[0])\n",
    "        self.dropout_first = nn.Dropout(p=0.0)\n",
    "\n",
    "        models = []\n",
    "        for n in range(n_layers - 2):\n",
    "            models.append(\n",
    "                nn.Sequential(\n",
    "                    OrderedDict(\n",
    "                        [\n",
    "                            (\"Linear\", nn.Linear(n_hidden[n], n_hidden[n + 1])),\n",
    "                            (\"LayerNorm\", nn.LayerNorm(n_hidden[n + 1])),\n",
    "                            (\"Dropout\", nn.Dropout(p=0.1)),\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        self.dnn = nn.ModuleList(models)\n",
    "        self.l_last = nn.Linear(n_hidden[-1], n_eigenglaciers)\n",
    "\n",
    "        self.V_hat = torch.nn.Parameter(V_hat, requires_grad=False)\n",
    "        self.F_mean = torch.nn.Parameter(F_mean, requires_grad=False)\n",
    "\n",
    "        self.register_buffer(\"area\", area)\n",
    "\n",
    "        self.train_ae = AbsoluteError()\n",
    "        self.test_ae = AbsoluteError()\n",
    "\n",
    "    def forward(self, x, add_mean=False):\n",
    "        # Pass the input tensor through each of our operations\n",
    "\n",
    "        a = self.l_first(x)\n",
    "        a = self.norm_first(a)\n",
    "        a = self.dropout_first(a)\n",
    "        z = torch.relu(a)\n",
    "\n",
    "        for dnn in self.dnn:\n",
    "            a = dnn(z)\n",
    "            z = torch.relu(a) + z\n",
    "\n",
    "        z_last = self.l_last(z)\n",
    "\n",
    "        if add_mean:\n",
    "            F_pred = z_last @ self.V_hat.T + self.F_mean\n",
    "        else:\n",
    "            F_pred = z_last @ self.V_hat.T\n",
    "\n",
    "        return F_pred\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"NNEmulator\")\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "        parser.add_argument(\"--n_hidden\", default=128)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=0.01)\n",
    "\n",
    "        return parent_parser\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), self.hparams.learning_rate, weight_decay=0.0\n",
    "        )\n",
    "        # This is an approximation to Doug's version:\n",
    "        scheduler = {\n",
    "            \"scheduler\": ExponentialLR(optimizer, 0.9975, verbose=True),\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, f, o, _ = batch\n",
    "        f_pred = self.forward(x)\n",
    "        loss = absolute_error(f_pred, f, o, self.area)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, f, o, o_0 = batch\n",
    "        f_pred = self.forward(x)\n",
    "\n",
    "        self.log(\"train_loss\", self.train_ae(f_pred, f, o, self.area))\n",
    "        self.log(\"test_loss\", self.test_ae(f_pred, f, o_0, self.area))\n",
    "\n",
    "        return {\"x\": x, \"f\": f, \"f_pred\": f_pred, \"o\": o, \"o_0\": o_0}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            self.train_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_loss\",\n",
    "            self.test_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "max_epochs = 100\n",
    "num_workers = 4\n",
    "hparams = {\"n_hidden\": 128, \n",
    "           \"n_hidden_1\": 128, \n",
    "           \"n_hidden_2\": 128, \n",
    "           \"n_hidden_3\": 128, \n",
    "           \"n_hidden_4\": 128, \n",
    "           \"n_layers\": 5,\n",
    "           \"learning_rate\": 0.01}        \n",
    "\n",
    "n_eigenglaciers = 100\n",
    "n_samples = 979\n",
    "n_parameters = 8\n",
    "n_grid_points = 5097\n",
    "X_train = torch.randn(n_samples, n_parameters)\n",
    "Y_train = torch.randn(n_samples, n_grid_points)\n",
    "V_hat = torch.randn(n_grid_points, n_eigenglaciers)\n",
    "F_mean = torch.randn(n_grid_points)\n",
    "area = torch.ones_like(F_mean) / n_grid_points\n",
    "\n",
    "omegas = torch.Tensor(dirichlet.rvs(np.ones(n_samples))).T\n",
    "omegas = omegas.type_as(X_train)\n",
    "omegas_0 = torch.ones_like(omegas) / len(omegas)\n",
    "\n",
    "training_data = TensorDataset(X_train, Y_train, omegas, omegas_0)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# train and val data loader are the same because we use BayesBag/Bootstrapping to avoid overfitting\n",
    "# by generating 50 emulators, each with different weights \"omegas\"\n",
    "\n",
    "trainer_e = pl.Trainer(\n",
    "    deterministic=True,\n",
    "    num_sanity_val_steps=0,\n",
    "    max_epochs=max_epochs,\n",
    ")\n",
    "\n",
    "e = NNEmulator(\n",
    "    n_parameters,\n",
    "    n_eigenglaciers,\n",
    "    V_hat,\n",
    "    F_mean,\n",
    "    area,\n",
    "    hparams,\n",
    ")\n",
    "\n",
    "trainer_e.fit(e, train_loader, train_loader)\n",
    "\n",
    "trainer_de = pl.Trainer(\n",
    "    deterministic=True,\n",
    "    num_sanity_val_steps=0,\n",
    "    max_epochs=max_epochs,\n",
    ")\n",
    "\n",
    "de = DNNEmulator(\n",
    "    n_parameters,\n",
    "    n_eigenglaciers,\n",
    "    V_hat,\n",
    "    F_mean,\n",
    "    area,\n",
    "    hparams,\n",
    ")\n",
    "\n",
    "trainer_de.fit(de, train_loader, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_e = e(X_train, add_mean=True)\n",
    "Y_pred_de = de(X_train, add_mean=True)\n",
    "print(torch.allclose(Y_pred_e, Y_pred_de, rtol=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ab1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a146c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae079d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
