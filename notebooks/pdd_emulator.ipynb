{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c11f2619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torchmetrics.utilities.checks import _check_same_shape\n",
    "from torchmetrics import Metric\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from pismemulator.metrics import AbsoluteError, absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d882c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDDEmulator(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_parameters: int,\n",
    "        n_outputs: int,\n",
    "        hparams,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        n_layers = self.hparams.n_layers\n",
    "        n_hidden = self.hparams.n_hidden\n",
    "\n",
    "        if isinstance(n_hidden, int):\n",
    "            n_hidden = [n_hidden] * (n_layers - 1)\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.l_first = nn.Linear(n_parameters, n_hidden[0])\n",
    "        self.norm_first = nn.LayerNorm(n_hidden[0])\n",
    "        self.dropout_first = nn.Dropout(p=0.0)\n",
    "\n",
    "        models = []\n",
    "        for n in range(n_layers - 2):\n",
    "            models.append(\n",
    "                nn.Sequential(\n",
    "                    OrderedDict(\n",
    "                        [\n",
    "                            (\"Linear\", nn.Linear(n_hidden[n], n_hidden[n + 1])),\n",
    "                            (\"LayerNorm\", nn.LayerNorm(n_hidden[n + 1])),\n",
    "                            (\"Dropout\", nn.Dropout(p=0.1)),\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        self.dnn = nn.ModuleList(models)\n",
    "        self.l_last = nn.Linear(n_hidden[-1], n_outputs)\n",
    "\n",
    "        self.train_ae = AbsoluteError()\n",
    "        self.test_ae = AbsoluteError()\n",
    "\n",
    "    def forward(self, x, add_mean=False):\n",
    "        # Pass the input tensor through each of our operations\n",
    "\n",
    "        a = self.l_first(x)\n",
    "        a = self.norm_first(a)\n",
    "        a = self.dropout_first(a)\n",
    "        z = torch.relu(a)\n",
    "\n",
    "        for dnn in self.dnn:\n",
    "            a = dnn(z)\n",
    "            z = torch.relu(a) + z\n",
    "\n",
    "        return self.l_last(z)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"NNEmulator\")\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "        parser.add_argument(\"--n_hidden\", default=128)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=0.1)\n",
    "\n",
    "        return parent_parser\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), self.hparams.learning_rate, weight_decay=0.0\n",
    "        )\n",
    "        # This is an approximation to Doug's version:\n",
    "        scheduler = {\n",
    "            \"scheduler\": ExponentialLR(optimizer, 0.9975, verbose=True),\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, f, o, _ = batch\n",
    "        f_pred = self.forward(x)\n",
    "        loss = absolute_error(f_pred, f, o)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, f, o, o_0 = batch\n",
    "        f_pred = self.forward(x)\n",
    "\n",
    "        self.log(\"train_loss\", self.train_ae(f_pred, f, o))\n",
    "        self.log(\"test_loss\", self.test_ae(f_pred, f, o_0))\n",
    "\n",
    "        return {\"x\": x, \"f\": f, \"f_pred\": f_pred, \"o\": o, \"o_0\": o_0}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            self.train_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_loss\",\n",
    "            self.test_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83bc210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchPDDModel(torch.nn.modules.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    # Copyright (c) 2013--2018, Julien Seguinot <seguinot@vaw.baug.ethz.ch>\n",
    "    # GNU General Public License v3.0+ (https://www.gnu.org/licenses/gpl-3.0.txt)\n",
    "\n",
    "    A positive degree day model for glacier surface mass balance\n",
    "\n",
    "    Return a callable Positive Degree Day (PDD) model instance.\n",
    "\n",
    "    Model parameters are held as public attributes, and can be set using\n",
    "    corresponding keyword arguments at initialization time:\n",
    "\n",
    "    *pdd_factor_snow* : float\n",
    "        Positive degree-day factor for snow.\n",
    "    *pdd_factor_ice* : float\n",
    "        Positive degree-day factor for ice.\n",
    "    *refreeze_snow* : float\n",
    "        Refreezing fraction of melted snow.\n",
    "    *refreeze_ice* : float\n",
    "        Refreezing fraction of melted ice.\n",
    "    *temp_snow* : float\n",
    "        Temperature at which all precipitation falls as snow.\n",
    "    *temp_rain* : float\n",
    "        Temperature at which all precipitation falls as rain.\n",
    "    *interpolate_rule* : [ 'linear' | 'nearest' | 'zero' |\n",
    "                           'slinear' | 'quadratic' | 'cubic' ]\n",
    "        Interpolation rule passed to `scipy.interpolate.interp1d`.\n",
    "    *interpolate_n*: int\n",
    "        Number of points used in interpolations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pdd_factor_snow=3,\n",
    "        pdd_factor_ice=8,\n",
    "        refreeze_snow=0.0,\n",
    "        refreeze_ice=0.0,\n",
    "        temp_snow=0.0,\n",
    "        temp_rain=2.0,\n",
    "        interpolate_rule=\"linear\",\n",
    "        interpolate_n=52,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # set pdd model parameters\n",
    "        self.pdd_factor_snow = pdd_factor_snow\n",
    "        self.pdd_factor_ice = pdd_factor_ice\n",
    "        self.refreeze_snow = refreeze_snow\n",
    "        self.refreeze_ice = refreeze_ice\n",
    "        self.temp_snow = temp_snow\n",
    "        self.temp_rain = temp_rain\n",
    "        self.interpolate_rule = interpolate_rule\n",
    "        self.interpolate_n = interpolate_n\n",
    "\n",
    "    def forward(self, temp, prec, stdv=0.0):\n",
    "        \"\"\"Run the positive degree day model.\n",
    "\n",
    "        Use temperature, precipitation, and standard deviation of temperature\n",
    "        to compute the number of positive degree days, accumulation and melt\n",
    "        surface mass fluxes, and the resulting surface mass balance.\n",
    "\n",
    "        *temp*: array_like\n",
    "            Input near-surface air temperature in degrees Celcius.\n",
    "        *prec*: array_like\n",
    "            Input precipitation rate in meter per year.\n",
    "        *stdv*: array_like (default 0.0)\n",
    "            Input standard deviation of near-surface air temperature in Kelvin.\n",
    "\n",
    "        By default, inputs are N-dimensional arrays whose first dimension is\n",
    "        interpreted as time and as periodic. Arrays of dimensions\n",
    "        N-1 are interpreted as constant in time and expanded to N dimensions.\n",
    "        Arrays of dimension 0 and numbers are interpreted as constant in time\n",
    "        and space and will be expanded too. The largest input array determines\n",
    "        the number of dimensions N.\n",
    "\n",
    "        Return the number of positive degree days ('pdd'), surface mass balance\n",
    "        ('smb'), and many other output variables in a dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        # ensure numpy arrays\n",
    "        temp = torch.asarray(temp)\n",
    "        prec = torch.asarray(prec)\n",
    "        stdv = torch.asarray(stdv)\n",
    "\n",
    "        # expand arrays to the largest shape\n",
    "        maxshape = max(temp.shape, prec.shape, stdv.shape)\n",
    "        temp = self._expand(temp, maxshape)\n",
    "        prec = self._expand(prec, maxshape)\n",
    "        stdv = self._expand(stdv, maxshape)\n",
    "\n",
    "        # interpolate time-series\n",
    "        if self.interpolate_n >= 1:\n",
    "            temp = self._interpolate(temp)\n",
    "            prec = self._interpolate(prec)\n",
    "            stdv = self._interpolate(stdv)\n",
    "\n",
    "        # compute accumulation and pdd\n",
    "        accu_rate = self.accu_rate(temp, prec)\n",
    "        inst_pdd = self.inst_pdd(temp, stdv)\n",
    "\n",
    "        # initialize snow depth, melt and refreeze rates\n",
    "        snow_depth = torch.zeros_like(temp)\n",
    "        snow_melt_rate = torch.zeros_like(temp)\n",
    "        ice_melt_rate = torch.zeros_like(temp)\n",
    "        snow_refreeze_rate = torch.zeros_like(temp)\n",
    "        ice_refreeze_rate = torch.zeros_like(temp)\n",
    "\n",
    "        snow_depth[:-1] = torch.clone(snow_depth[1:])\n",
    "        snow_depth = snow_depth + accu_rate\n",
    "        snow_melt_rate, ice_melt_rate = self.melt_rates(snow_depth, inst_pdd)\n",
    "        snow_depth = snow_depth - snow_melt_rate\n",
    "\n",
    "        melt_rate = snow_melt_rate + ice_melt_rate\n",
    "        snow_refreeze_rate = self.refreeze_snow * snow_melt_rate\n",
    "        ice_refreeze_rate = self.refreeze_ice * ice_melt_rate\n",
    "        refreeze_rate = snow_refreeze_rate + ice_refreeze_rate\n",
    "        runoff_rate = melt_rate - refreeze_rate\n",
    "        inst_smb = accu_rate - runoff_rate\n",
    "\n",
    "        # output\n",
    "        return {\n",
    "            \"temp\": temp,\n",
    "            \"prec\": prec,\n",
    "            \"stdv\": stdv,\n",
    "            \"inst_pdd\": inst_pdd,\n",
    "            \"accu_rate\": accu_rate,\n",
    "            \"snow_melt_rate\": snow_melt_rate,\n",
    "            \"ice_melt_rate\": ice_melt_rate,\n",
    "            \"melt_rate\": melt_rate,\n",
    "            \"snow_refreeze_rate\": snow_refreeze_rate,\n",
    "            \"ice_refreeze_rate\": ice_refreeze_rate,\n",
    "            \"refreeze_rate\": refreeze_rate,\n",
    "            \"runoff_rate\": runoff_rate,\n",
    "            \"inst_smb\": inst_smb,\n",
    "            \"snow_depth\": snow_depth,\n",
    "            \"pdd\": self._integrate(inst_pdd),\n",
    "            \"accu\": self._integrate(accu_rate),\n",
    "            \"snow_melt\": self._integrate(snow_melt_rate),\n",
    "            \"ice_melt\": self._integrate(ice_melt_rate),\n",
    "            \"melt\": self._integrate(melt_rate),\n",
    "            \"runoff\": self._integrate(runoff_rate),\n",
    "            \"refreeze\": self._integrate(refreeze_rate),\n",
    "            \"smb\": self._integrate(inst_smb),\n",
    "        }\n",
    "\n",
    "    def _expand(self, array, shape):\n",
    "        \"\"\"Expand an array to the given shape\"\"\"\n",
    "        if array.shape == shape:\n",
    "            res = array\n",
    "        elif array.shape == (1, shape[1], shape[2]):\n",
    "            res = np.asarray([array[0]] * shape[0])\n",
    "        elif array.shape == shape[1:]:\n",
    "            res = np.asarray([array] * shape[0])\n",
    "        elif array.shape == ():\n",
    "            res = array * torch.ones(shape)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"could not expand array of shape %s to %s\" % (array.shape, shape)\n",
    "            )\n",
    "        return res\n",
    "\n",
    "    def _integrate(self, array):\n",
    "        \"\"\"Integrate an array over one year\"\"\"\n",
    "        return torch.sum(array, axis=0) / (self.interpolate_n - 1)\n",
    "\n",
    "    def _interpolate(self, array):\n",
    "        \"\"\"Interpolate an array through one year.\"\"\"\n",
    "\n",
    "        from scipy.interpolate import interp1d\n",
    "\n",
    "        rule = self.interpolate_rule\n",
    "        npts = self.interpolate_n\n",
    "        oldx = (torch.arange(len(array) + 2) - 0.5) / len(array)\n",
    "        oldy = torch.vstack((array[-1], array, array[0]))\n",
    "        newx = (torch.arange(npts) + 0.5) / npts  # use 0.0 for PISM-like behaviour\n",
    "        newy = interp1d(oldx, oldy, kind=rule, axis=0)(newx)\n",
    "\n",
    "        return torch.from_numpy(newy)\n",
    "\n",
    "    def inst_pdd(self, temp, stdv):\n",
    "        \"\"\"Compute instantaneous positive degree days from temperature.\n",
    "\n",
    "        Use near-surface air temperature and standard deviation to compute\n",
    "        instantaneous positive degree days (effective temperature for melt,\n",
    "        unit degrees C) using an integral formulation (Calov and Greve, 2005).\n",
    "\n",
    "        *temp*: array_like\n",
    "            Near-surface air temperature in degrees Celcius.\n",
    "        *stdv*: array_like\n",
    "            Standard deviation of near-surface air temperature in Kelvin.\n",
    "        \"\"\"\n",
    "\n",
    "        # compute positive part of temperature everywhere\n",
    "        positivepart = torch.greater(temp, 0) * temp\n",
    "\n",
    "        # compute Calov and Greve (2005) integrand, ignoring division by zero\n",
    "        normtemp = temp / (torch.sqrt(torch.tensor(2)) * stdv)\n",
    "        calovgreve = stdv / torch.sqrt(torch.tensor(2) * torch.pi) * torch.exp(\n",
    "            -(normtemp**2)\n",
    "        ) + temp / 2 * torch.erfc(-normtemp)\n",
    "\n",
    "        # use positive part where sigma is zero and Calov and Greve elsewhere\n",
    "        teff = torch.where(stdv == 0.0, positivepart, calovgreve)\n",
    "\n",
    "        # convert to degree-days\n",
    "        return teff * 365.242198781\n",
    "\n",
    "    def accu_rate(self, temp, prec):\n",
    "        \"\"\"Compute accumulation rate from temperature and precipitation.\n",
    "\n",
    "        The fraction of precipitation that falls as snow decreases linearly\n",
    "        from one to zero between temperature thresholds defined by the\n",
    "        `temp_snow` and `temp_rain` attributes.\n",
    "\n",
    "        *temp*: array_like\n",
    "            Near-surface air temperature in degrees Celcius.\n",
    "        *prec*: array_like\n",
    "            Precipitation rate in meter per year.\n",
    "        \"\"\"\n",
    "\n",
    "        # compute snow fraction as a function of temperature\n",
    "        reduced_temp = (self.temp_rain - temp) / (self.temp_rain - self.temp_snow)\n",
    "        snowfrac = torch.clip(reduced_temp, 0, 1)\n",
    "\n",
    "        # return accumulation rate\n",
    "        return snowfrac * prec\n",
    "\n",
    "    def melt_rates(self, snow, pdd):\n",
    "        \"\"\"Compute melt rates from snow precipitation and pdd sum.\n",
    "\n",
    "        Snow melt is computed from the number of positive degree days (*pdd*)\n",
    "        and the `pdd_factor_snow` model attribute. If all snow is melted and\n",
    "        some energy (PDD) remains, ice melt is computed using `pdd_factor_ice`.\n",
    "\n",
    "        *snow*: array_like\n",
    "            Snow precipitation rate.\n",
    "        *pdd*: array_like\n",
    "            Number of positive degree days.\n",
    "        \"\"\"\n",
    "\n",
    "        # parse model parameters for readability\n",
    "        ddf_snow = self.pdd_factor_snow / 1e3\n",
    "        ddf_ice = self.pdd_factor_ice / 1e3\n",
    "\n",
    "        # compute a potential snow melt\n",
    "        pot_snow_melt = ddf_snow * pdd\n",
    "\n",
    "        # effective snow melt can't exceed amount of snow\n",
    "        snow_melt = torch.minimum(snow, pot_snow_melt)\n",
    "\n",
    "        # ice melt is proportional to excess snow melt\n",
    "        ice_melt = (pot_snow_melt - snow_melt) * ddf_ice / ddf_snow\n",
    "\n",
    "        # return melt rates\n",
    "        return (snow_melt, ice_melt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0859e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_observation(file=\"DMI-HIRHAM5_1980_MM.nc\", thinning_factor=1):\n",
    "    \"\"\"\n",
    "    Read and return Obs\n",
    "    \"\"\"\n",
    "\n",
    "    with xr.open_dataset(file) as Obs:\n",
    "\n",
    "        stacked = Obs.stack(z=(\"rlat\", \"rlon\"))\n",
    "        ncl_stacked = Obs.stack(z=(\"ncl4\", \"ncl5\"))\n",
    "\n",
    "        temp = stacked.tas.dropna(dim=\"z\").values\n",
    "        rainfall = stacked.rainfall.dropna(dim=\"z\").values / 1000\n",
    "        snowfall = stacked.snfall.dropna(dim=\"z\").values / 1000\n",
    "        smb = stacked.gld.dropna(dim=\"z\").values / 1000\n",
    "        refreeze = ncl_stacked.rfrz.dropna(dim=\"z\").values / 1000\n",
    "        melt = stacked.snmel.dropna(dim=\"z\").values / 1000\n",
    "        precip = rainfall + snowfall\n",
    "\n",
    "    return (\n",
    "        temp[..., ::thinning_factor] - 273.15,\n",
    "        precip[..., ::thinning_factor],\n",
    "        refreeze.sum(axis=0)[..., ::thinning_factor],\n",
    "        snowfall.sum(axis=0)[..., ::thinning_factor],\n",
    "        melt.sum(axis=0)[..., ::thinning_factor],\n",
    "        smb.sum(axis=0)[..., ::thinning_factor],\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3095b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, precip, _, _, _, _ = read_observation(thinning_factor=100)\n",
    "std_dev = np.zeros_like(temp) + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5bdf0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.sample import saltelli\n",
    "from pyDOE import lhs\n",
    "from scipy.stats.distributions import truncnorm, gamma, uniform, randint\n",
    "method = \"lhs\"\n",
    "n_prior_samples = 250\n",
    "np.random.seed(2)\n",
    "\n",
    "distributions = {\n",
    "    \"f_snow\": uniform(\n",
    "        loc=2.0, scale=4.0\n",
    "    ),  # uniform between 2 and 6\n",
    "    \"f_ice\": uniform(\n",
    "        loc=3.0, scale=9\n",
    "    ),  # uniform between 3 and 12\n",
    "    \"refreeze\": uniform(loc=0, scale=1.0),  # uniform between 0 and 1\n",
    "}\n",
    "# Names of all the variables\n",
    "keys = [x for x in distributions.keys()]\n",
    "\n",
    "# Describe the Problem\n",
    "problem = {\"num_vars\": len(keys), \"names\": keys, \"bounds\": [[0, 1]] * len(keys)}\n",
    "\n",
    "# Generate uniform samples (i.e. one unit hypercube)\n",
    "if method == \"saltelli\":\n",
    "    unif_sample = saltelli.sample(problem, n_prior_samples, calc_second_order=False)\n",
    "elif method == \"lhs\":\n",
    "    unif_sample = lhs(len(keys), n_prior_samples)\n",
    "else:\n",
    "    print(f\"Method {method} not available\")\n",
    "\n",
    "# To hold the transformed variables\n",
    "dist_sample = np.zeros_like(unif_sample)\n",
    "\n",
    "# Now transform the unit hypercube to the prescribed distributions\n",
    "# For each variable, transform with the inverse of the CDF (inv(CDF)=ppf)\n",
    "for i, key in enumerate(keys):\n",
    "    dist_sample[:, i] = distributions[key].ppf(unif_sample[:, i])\n",
    "\n",
    "# Save to CSV file using Pandas DataFrame and to_csv method\n",
    "header = keys\n",
    "# Convert to Pandas dataframe, append column headers, output as csv\n",
    "df = pd.DataFrame(data=dist_sample, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d52677e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X = []\n",
    "    Y = []\n",
    "    for k, row in df.iterrows():   \n",
    "        m_f_snow = row[\"f_snow\"]\n",
    "        m_f_ice = row[\"f_ice\"]\n",
    "        m_refreeze = row[\"refreeze\"]\n",
    "\n",
    "        pdd = TorchPDDModel(\n",
    "            pdd_factor_snow=m_f_snow,\n",
    "            pdd_factor_ice=m_f_ice,\n",
    "            refreeze_snow=m_refreeze,\n",
    "            refreeze_ice=m_refreeze,\n",
    "        )\n",
    "        result = pdd(temp, precip, std_dev)\n",
    "\n",
    "        M_train = result[\"melt\"]\n",
    "        A_train = result[\"accu\"]\n",
    "        R_train = result[\"refreeze\"]\n",
    "        m_Y = torch.vstack((M_train, A_train, R_train,)).T\n",
    "        Y.append(m_Y)\n",
    "        X.append(torch.from_numpy(np.hstack((temp.T, precip.T, np.tile(m_f_snow, (temp.shape[1], 1)), np.tile(m_f_ice, (temp.shape[1], 1)), np.tile(m_refreeze, (temp.shape[1], 1))))))\n",
    "\n",
    "    X_train = torch.vstack(X).type(torch.FloatTensor)\n",
    "    Y_train = torch.vstack(Y).type(torch.FloatTensor)\n",
    "    n_samples, n_parameters = X_train.shape\n",
    "    n_outputs = Y_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c62a5f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2a45752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "X_train_mean = X_train.mean(axis=0)\n",
    "X_train_std = X_train.std(axis=0)\n",
    "X_train_norm = (X_train - X_train_mean) / X_train_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "99141a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([166000, 27])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e2e05bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([166000, 27]) torch.Size([27])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_train_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "237942c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "    import os\n",
    "    from scipy.stats import dirichlet\n",
    "\n",
    "    model_index = 0\n",
    "    torch.manual_seed(0)\n",
    "    pl.seed_everything(0)\n",
    "    np.random.seed(model_index)\n",
    "    emulator_dir = \"pddemulator\"\n",
    "\n",
    "    if not os.path.isdir(emulator_dir):\n",
    "        os.makedirs(emulator_dir)\n",
    "        os.makedirs(os.path.join(emulator_dir, \"emulator\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0f4d63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) 2021 Andy Aschwanden, Douglas C Brinkerhoff\n",
    "#\n",
    "# This file is part of pism-emulator.\n",
    "#\n",
    "# PISM-EMULATOR is free software; you can redistribute it and/or modify it under the\n",
    "# terms of the GNU General Public License as published by the Free Software\n",
    "# Foundation; either version 3 of the License, or (at your option) any later\n",
    "# version.\n",
    "#\n",
    "# PISM-EMULATOR is distributed in the hope that it will be useful, but WITHOUT ANY\n",
    "# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n",
    "# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n",
    "# details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with PISM; if not, write to the Free Software\n",
    "# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torchmetrics.utilities.checks import _check_same_shape\n",
    "from torchmetrics import Metric\n",
    "\n",
    "\n",
    "def _absolute_error_update(\n",
    "    preds: Tensor, target: Tensor, omegas: Tensor) -> Tensor:\n",
    "    _check_same_shape(preds, target)\n",
    "    diff = torch.abs(preds - target)\n",
    "    sum_abs_error = torch.sum(diff * diff, axis=1)\n",
    "    absolute_error = torch.sum(sum_abs_error * omegas.squeeze())\n",
    "    return absolute_error\n",
    "\n",
    "\n",
    "def _absolute_error_compute(absolute_error) -> Tensor:\n",
    "    return absolute_error\n",
    "\n",
    "\n",
    "def absolute_error(\n",
    "    preds: Tensor, target: Tensor, omegas: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes squared absolute error\n",
    "    Args:\n",
    "        preds: estimated labels\n",
    "        target: ground truth labels\n",
    "        omegas: weights\n",
    "        area: area of each cell\n",
    "    Return:\n",
    "        Tensor with absolute error\n",
    "    Example:\n",
    "        >>> x = torch.tensor([[0, 1, 2, 3], [1, 2, 3, 4]]).T\n",
    "        >>> y = torch.tensor([[0, 1, 2, 1], [2, 3, 4, 4]]).T\n",
    "        >>> o = torch.tensor([0.25, 0.25, 0.3, 0.2])\n",
    "        >>> a = torch.tensor([0.25, 0.25])\n",
    "        >>> absolute_error(x, y, o, a)\n",
    "        tensor(0.4000)\n",
    "    \"\"\"\n",
    "    sum_abs_error = _absolute_error_update(preds, target, omegas)\n",
    "    return _absolute_error_compute(sum_abs_error)\n",
    "\n",
    "\n",
    "class AbsoluteError(Metric):\n",
    "    def __init__(self, compute_on_step: bool = True, dist_sync_on_step=False):\n",
    "        # call `self.add_state`for every internal state that is needed for the metrics computations\n",
    "        # dist_reduce_fx indicates the function that should be used to reduce\n",
    "        # state from multiple processes\n",
    "        super().__init__(\n",
    "            compute_on_step=compute_on_step, dist_sync_on_step=dist_sync_on_step\n",
    "        )\n",
    "\n",
    "        self.add_state(\"sum_abs_error\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: Tensor, target: Tensor, omegas: Tensor):\n",
    "        \"\"\"\n",
    "        Update state with predictions and targets, and area.\n",
    "        Args:\n",
    "            preds: Predictions from model\n",
    "            target: Ground truth values\n",
    "            omegas: Weights\n",
    "            area: Area of each cell\n",
    "        \"\"\"\n",
    "        sum_abs_error = _absolute_error_update(preds, target, omegas)\n",
    "        self.sum_abs_error += sum_abs_error\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Computes absolute error over state.\n",
    "        \"\"\"\n",
    "        return _absolute_error_compute(self.sum_abs_error)\n",
    "\n",
    "    @property\n",
    "    def is_differentiable(self):\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "05a2f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDDDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X,\n",
    "        Y,\n",
    "        omegas,\n",
    "        omegas_0,\n",
    "        batch_size: int = 128,\n",
    "        train_size: float = 0.9,\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.omegas = omegas\n",
    "        self.omegas_0 = omegas_0\n",
    "        self.batch_size = batch_size\n",
    "        self.train_size = train_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "\n",
    "        all_data = TensorDataset(self.X, self.Y, self.omegas, self.omegas_0)\n",
    "        self.all_data = all_data\n",
    "\n",
    "        training_data, val_data = train_test_split(\n",
    "            all_data, train_size=self.train_size, random_state=0\n",
    "        )\n",
    "        self.training_data = training_data\n",
    "        self.test_data = training_data\n",
    "\n",
    "        self.val_data = val_data\n",
    "        train_all_loader = DataLoader(\n",
    "            dataset=all_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.train_all_loader = train_all_loader\n",
    "        val_all_loader = DataLoader(\n",
    "            dataset=all_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.val_all_loader = val_all_loader\n",
    "        train_loader = DataLoader(\n",
    "            dataset=training_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = train_loader\n",
    "        val_loader = DataLoader(\n",
    "            dataset=val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def prepare_data(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self.val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "64b35190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | l_first       | Linear        | 3.6 K \n",
      "1 | norm_first    | LayerNorm     | 256   \n",
      "2 | dropout_first | Dropout       | 0     \n",
      "3 | dnn           | ModuleList    | 50.3 K\n",
      "4 | l_last        | Linear        | 387   \n",
      "5 | train_ae      | AbsoluteError | 0     \n",
      "6 | test_ae       | AbsoluteError | 0     \n",
      "------------------------------------------------\n",
      "54.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "54.5 K    Total params\n",
      "0.218     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 0:  50%|████████████████████████████████████████████████████████████████████████▍                                                                        | 1296/2594 [00:06<00:06, 197.68it/s, loss=1.35e-05, v_num=32]Adjusting learning rate of group 0 to 9.9750e-03.\n",
      "Epoch 0:  50%|████████████████████████████████████████████████████████████████████████▌                                                                        | 1297/2594 [00:06<00:06, 194.82it/s, loss=1.28e-05, v_num=32]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  50%|████████████████████████████████████████████████████████████████████████▋                                                                        | 1300/2594 [00:06<00:06, 188.99it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  52%|███████████████████████████████████████████████████████████████████████████▏                                                                     | 1345/2594 [00:06<00:06, 192.68it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  54%|█████████████████████████████████████████████████████████████████████████████▉                                                                   | 1394/2594 [00:07<00:06, 196.85it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  56%|████████████████████████████████████████████████████████████████████████████████▋                                                                | 1443/2594 [00:07<00:05, 200.92it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  58%|███████████████████████████████████████████████████████████████████████████████████▍                                                             | 1492/2594 [00:07<00:05, 204.86it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  59%|██████████████████████████████████████████████████████████████████████████████████████▏                                                          | 1542/2594 [00:07<00:05, 208.82it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  61%|████████████████████████████████████████████████████████████████████████████████████████▉                                                        | 1592/2594 [00:07<00:04, 212.68it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  63%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                     | 1643/2594 [00:07<00:04, 216.55it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  65%|██████████████████████████████████████████████████████████████████████████████████████████████▋                                                  | 1694/2594 [00:07<00:04, 220.26it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  67%|█████████████████████████████████████████████████████████████████████████████████████████████████▌                                               | 1745/2594 [00:07<00:03, 223.88it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  69%|████████████████████████████████████████████████████████████████████████████████████████████████████▍                                            | 1796/2594 [00:07<00:03, 227.44it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  71%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 1847/2594 [00:08<00:03, 230.87it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  73%|██████████████████████████████████████████████████████████████████████████████████████████████████████████                                       | 1898/2594 [00:08<00:02, 234.19it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                    | 1949/2594 [00:08<00:02, 237.43it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  77%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                 | 2000/2594 [00:08<00:02, 240.70it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  79%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                              | 2051/2594 [00:08<00:02, 243.83it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 2102/2594 [00:08<00:01, 246.91it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                        | 2153/2594 [00:08<00:01, 249.97it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                     | 2204/2594 [00:08<00:01, 252.81it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                   | 2255/2594 [00:08<00:01, 255.59it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                | 2306/2594 [00:08<00:01, 258.24it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 2357/2594 [00:09<00:00, 260.91it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 2408/2594 [00:09<00:00, 263.46it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍       | 2459/2594 [00:09<00:00, 266.08it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0:  97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎    | 2510/2594 [00:09<00:00, 268.65it/s, loss=1.28e-05, v_num=32]\u001b[A\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.37it/s, loss=1.28e-05, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  50%|█████████████████████████████████████████████████████▉                                                      | 1296/2594 [00:06<00:06, 195.68it/s, loss=9.36e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[AAdjusting learning rate of group 0 to 9.9501e-03.\n",
      "Epoch 1:  50%|██████████████████████████████████████████████████████                                                      | 1297/2594 [00:06<00:06, 193.97it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  51%|███████████████████████████████████████████████████████▏                                                    | 1326/2594 [00:06<00:06, 190.13it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  53%|█████████████████████████████████████████████████████████▎                                                  | 1377/2594 [00:07<00:06, 194.52it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  55%|███████████████████████████████████████████████████████████▍                                                | 1428/2594 [00:07<00:05, 198.78it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  57%|█████████████████████████████████████████████████████████████▌                                              | 1479/2594 [00:07<00:05, 202.98it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  59%|███████████████████████████████████████████████████████████████▋                                            | 1530/2594 [00:07<00:05, 206.97it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  61%|█████████████████████████████████████████████████████████████████▊                                          | 1581/2594 [00:07<00:04, 210.88it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  63%|███████████████████████████████████████████████████████████████████▉                                        | 1632/2594 [00:07<00:04, 214.65it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████████████████████████████████████████                                      | 1683/2594 [00:07<00:04, 218.31it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████████████████████████████▏                                   | 1734/2594 [00:07<00:03, 221.94it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Validating:  34%|████████████████████████████████████████████████████████▌                                                                                                               | 437/1297 [00:01<00:01, 473.72it/s]\u001b[A\n",
      "Epoch 1:  69%|██████████████████████████████████████████████████████████████████████████▎                                 | 1785/2594 [00:07<00:03, 225.43it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  71%|████████████████████████████████████████████████████████████████████████████▍                               | 1836/2594 [00:08<00:03, 228.96it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  73%|██████████████████████████████████████████████████████████████████████████████▌                             | 1887/2594 [00:08<00:03, 232.29it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  75%|████████████████████████████████████████████████████████████████████████████████▋                           | 1938/2594 [00:08<00:02, 235.54it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  77%|██████████████████████████████████████████████████████████████████████████████████▊                         | 1989/2594 [00:08<00:02, 238.66it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  79%|████████████████████████████████████████████████████████████████████████████████████▉                       | 2040/2594 [00:08<00:02, 241.75it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  81%|███████████████████████████████████████████████████████████████████████████████████████                     | 2091/2594 [00:08<00:02, 244.88it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  83%|█████████████████████████████████████████████████████████████████████████████████████████▏                  | 2142/2594 [00:08<00:01, 247.89it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  85%|███████████████████████████████████████████████████████████████████████████████████████████▎                | 2193/2594 [00:08<00:01, 250.75it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  87%|█████████████████████████████████████████████████████████████████████████████████████████████▍              | 2244/2594 [00:08<00:01, 253.53it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████████████████████████████████████▌            | 2295/2594 [00:08<00:01, 256.33it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████▋          | 2346/2594 [00:09<00:00, 259.12it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████████████████████████████████████████████████████████▊        | 2397/2594 [00:09<00:00, 261.69it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉      | 2448/2594 [00:09<00:00, 264.21it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████    | 2499/2594 [00:09<00:00, 266.76it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2550/2594 [00:09<00:00, 269.25it/s, loss=9.23e-06, v_num=32, train_loss=0.0112, test_loss=0.0115]\u001b[A\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.99it/s, loss=9.23e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  50%|██████████████████████████████████████████████████████▍                                                      | 1296/2594 [00:06<00:06, 196.49it/s, loss=6.36e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[AAdjusting learning rate of group 0 to 9.9252e-03.\n",
      "Epoch 2:  50%|██████████████████████████████████████████████████████▌                                                      | 1297/2594 [00:06<00:06, 194.87it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  51%|███████████████████████████████████████████████████████▋                                                     | 1326/2594 [00:06<00:06, 191.16it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  53%|█████████████████████████████████████████████████████████▊                                                   | 1377/2594 [00:07<00:06, 195.48it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  55%|████████████████████████████████████████████████████████████                                                 | 1428/2594 [00:07<00:05, 199.73it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  57%|██████████████████████████████████████████████████████████████▏                                              | 1479/2594 [00:07<00:05, 203.99it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  59%|████████████████████████████████████████████████████████████████▎                                            | 1530/2594 [00:07<00:05, 208.07it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  61%|██████████████████████████████████████████████████████████████████▍                                          | 1581/2594 [00:07<00:04, 212.02it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  63%|████████████████████████████████████████████████████████████████████▌                                        | 1632/2594 [00:07<00:04, 215.89it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  65%|██████████████████████████████████████████████████████████████████████▋                                      | 1683/2594 [00:07<00:04, 219.63it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  67%|████████████████████████████████████████████████████████████████████████▊                                    | 1734/2594 [00:07<00:03, 223.32it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  69%|███████████████████████████████████████████████████████████████████████████                                  | 1785/2594 [00:07<00:03, 226.90it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  71%|█████████████████████████████████████████████████████████████████████████████▏                               | 1836/2594 [00:07<00:03, 230.43it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  73%|███████████████████████████████████████████████████████████████████████████████▎                             | 1887/2594 [00:08<00:03, 233.84it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  75%|█████████████████████████████████████████████████████████████████████████████████▍                           | 1938/2594 [00:08<00:02, 237.18it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  77%|███████████████████████████████████████████████████████████████████████████████████▌                         | 1989/2594 [00:08<00:02, 240.42it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  79%|█████████████████████████████████████████████████████████████████████████████████████▋                       | 2040/2594 [00:08<00:02, 243.60it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  81%|███████████████████████████████████████████████████████████████████████████████████████▉                     | 2092/2594 [00:08<00:02, 246.82it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  83%|██████████████████████████████████████████████████████████████████████████████████████████                   | 2144/2594 [00:08<00:01, 249.91it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Validating:  65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                          | 847/1297 [00:01<00:00, 503.50it/s]\u001b[A\n",
      "Epoch 2:  85%|████████████████████████████████████████████████████████████████████████████████████████████▎                | 2196/2594 [00:08<00:01, 252.86it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  87%|██████████████████████████████████████████████████████████████████████████████████████████████▍              | 2248/2594 [00:08<00:01, 255.81it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  89%|████████████████████████████████████████████████████████████████████████████████████████████████▋            | 2300/2594 [00:08<00:01, 258.61it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████▊          | 2352/2594 [00:09<00:00, 261.42it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  93%|█████████████████████████████████████████████████████████████████████████████████████████████████████        | 2404/2594 [00:09<00:00, 264.13it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 2456/2594 [00:09<00:00, 266.81it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2:  97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 2508/2594 [00:09<00:00, 269.42it/s, loss=6.01e-06, v_num=32, train_loss=0.011, test_loss=0.0114]\u001b[A\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 272.12it/s, loss=6.01e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  50%|█████████████████████████████████████████████████████▍                                                     | 1296/2594 [00:06<00:06, 196.01it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[AAdjusting learning rate of group 0 to 9.9004e-03.\n",
      "Epoch 3:  50%|█████████████████████████████████████████████████████▌                                                     | 1297/2594 [00:06<00:06, 194.29it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  50%|█████████████████████████████████████████████████████▌                                                     | 1300/2594 [00:06<00:06, 188.45it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  52%|███████████████████████████████████████████████████████▊                                                   | 1352/2594 [00:07<00:06, 192.65it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  54%|█████████████████████████████████████████████████████████▉                                                 | 1404/2594 [00:07<00:06, 196.93it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  56%|████████████████████████████████████████████████████████████                                               | 1456/2594 [00:07<00:05, 201.27it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  58%|██████████████████████████████████████████████████████████████▏                                            | 1508/2594 [00:07<00:05, 205.54it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  60%|████████████████████████████████████████████████████████████████▎                                          | 1560/2594 [00:07<00:04, 209.62it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  62%|██████████████████████████████████████████████████████████████████▍                                        | 1612/2594 [00:07<00:04, 213.67it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  64%|████████████████████████████████████████████████████████████████████▋                                      | 1664/2594 [00:07<00:04, 217.53it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  66%|██████████████████████████████████████████████████████████████████████▊                                    | 1716/2594 [00:07<00:03, 221.28it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  68%|████████████████████████████████████████████████████████████████████████▉                                  | 1768/2594 [00:07<00:03, 225.04it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  70%|███████████████████████████████████████████████████████████████████████████                                | 1820/2594 [00:07<00:03, 228.66it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  72%|█████████████████████████████████████████████████████████████████████████████▏                             | 1872/2594 [00:08<00:03, 232.12it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  74%|███████████████████████████████████████████████████████████████████████████████▎                           | 1924/2594 [00:08<00:02, 235.56it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  76%|█████████████████████████████████████████████████████████████████████████████████▌                         | 1976/2594 [00:08<00:02, 238.86it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  78%|███████████████████████████████████████████████████████████████████████████████████▋                       | 2028/2594 [00:08<00:02, 242.09it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  80%|█████████████████████████████████████████████████████████████████████████████████████▊                     | 2080/2594 [00:08<00:02, 245.24it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  82%|███████████████████████████████████████████████████████████████████████████████████████▉                   | 2132/2594 [00:08<00:01, 248.30it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  84%|██████████████████████████████████████████████████████████████████████████████████████████                 | 2184/2594 [00:08<00:01, 251.26it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  86%|████████████████████████████████████████████████████████████████████████████████████████████▏              | 2236/2594 [00:08<00:01, 254.20it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  88%|██████████████████████████████████████████████████████████████████████████████████████████████▍            | 2288/2594 [00:08<00:01, 257.06it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  90%|████████████████████████████████████████████████████████████████████████████████████████████████▌          | 2340/2594 [00:09<00:00, 259.97it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████▋        | 2392/2594 [00:09<00:00, 262.75it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  94%|████████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2444/2594 [00:09<00:00, 265.45it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2496/2594 [00:09<00:00, 268.11it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████  | 2548/2594 [00:09<00:00, 270.66it/s, loss=2.4e-06, v_num=32, train_loss=0.00473, test_loss=0.00479]\u001b[A\n",
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.69it/s, loss=2.4e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  50%|████████████████████████████████████████████████████▉                                                     | 1296/2594 [00:06<00:06, 194.82it/s, loss=5.26e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[AAdjusting learning rate of group 0 to 9.8756e-03.\n",
      "Epoch 4:  50%|█████████████████████████████████████████████████████                                                     | 1297/2594 [00:06<00:06, 192.92it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  50%|█████████████████████████████████████████████████████                                                     | 1300/2594 [00:06<00:06, 187.00it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  52%|███████████████████████████████████████████████████████▏                                                  | 1352/2594 [00:07<00:06, 191.34it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  54%|█████████████████████████████████████████████████████████▎                                                | 1404/2594 [00:07<00:06, 195.77it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  56%|███████████████████████████████████████████████████████████▍                                              | 1456/2594 [00:07<00:05, 200.06it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  58%|█████████████████████████████████████████████████████████████▌                                            | 1508/2594 [00:07<00:05, 204.20it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  60%|███████████████████████████████████████████████████████████████▋                                          | 1560/2594 [00:07<00:04, 208.27it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  62%|█████████████████████████████████████████████████████████████████▊                                        | 1612/2594 [00:07<00:04, 212.22it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  64%|███████████████████████████████████████████████████████████████████▉                                      | 1664/2594 [00:07<00:04, 216.05it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  66%|██████████████████████████████████████████████████████████████████████                                    | 1716/2594 [00:07<00:03, 219.78it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  68%|████████████████████████████████████████████████████████████████████████▏                                 | 1768/2594 [00:07<00:03, 223.49it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  70%|██████████████████████████████████████████████████████████████████████████▎                               | 1820/2594 [00:08<00:03, 227.01it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  72%|████████████████████████████████████████████████████████████████████████████▍                             | 1872/2594 [00:08<00:03, 230.46it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  74%|██████████████████████████████████████████████████████████████████████████████▌                           | 1924/2594 [00:08<00:02, 233.83it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  76%|████████████████████████████████████████████████████████████████████████████████▋                         | 1976/2594 [00:08<00:02, 237.15it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  78%|██████████████████████████████████████████████████████████████████████████████████▊                       | 2028/2594 [00:08<00:02, 240.45it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  80%|████████████████████████████████████████████████████████████████████████████████████▉                     | 2080/2594 [00:08<00:02, 243.63it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  82%|███████████████████████████████████████████████████████████████████████████████████████                   | 2132/2594 [00:08<00:01, 246.68it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  84%|█████████████████████████████████████████████████████████████████████████████████████████▏                | 2184/2594 [00:08<00:01, 249.64it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  86%|███████████████████████████████████████████████████████████████████████████████████████████▎              | 2236/2594 [00:08<00:01, 252.56it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  88%|█████████████████████████████████████████████████████████████████████████████████████████████▍            | 2288/2594 [00:08<00:01, 255.42it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  90%|███████████████████████████████████████████████████████████████████████████████████████████████▌          | 2340/2594 [00:09<00:00, 258.25it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████▋        | 2392/2594 [00:09<00:00, 260.95it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Epoch 4:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2444/2594 [00:09<00:00, 263.68it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2496/2594 [00:09<00:00, 266.26it/s, loss=5.11e-06, v_num=32, train_loss=0.00461, test_loss=0.00467]\u001b[A\n",
      "Validating:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍            | 1199/1297 [00:02<00:00, 499.23it/s]\u001b[A\n",
      "Epoch 4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.57it/s, loss=5.11e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  50%|█████████████████████████████████████████████████████▍                                                     | 1296/2594 [00:06<00:06, 196.13it/s, loss=3.48e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[AAdjusting learning rate of group 0 to 9.8509e-03.\n",
      "Epoch 5:  50%|█████████████████████████████████████████████████████▌                                                     | 1297/2594 [00:06<00:06, 194.42it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  50%|█████████████████████████████████████████████████████▌                                                     | 1300/2594 [00:06<00:06, 188.39it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  52%|███████████████████████████████████████████████████████▊                                                   | 1352/2594 [00:07<00:06, 192.91it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  54%|█████████████████████████████████████████████████████████▉                                                 | 1404/2594 [00:07<00:06, 197.31it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  56%|████████████████████████████████████████████████████████████                                               | 1456/2594 [00:07<00:05, 201.63it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  58%|██████████████████████████████████████████████████████████████▏                                            | 1508/2594 [00:07<00:05, 205.91it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  60%|████████████████████████████████████████████████████████████████▎                                          | 1560/2594 [00:07<00:04, 209.99it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  62%|██████████████████████████████████████████████████████████████████▍                                        | 1612/2594 [00:07<00:04, 213.97it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  64%|████████████████████████████████████████████████████████████████████▋                                      | 1664/2594 [00:07<00:04, 217.87it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  66%|██████████████████████████████████████████████████████████████████████▊                                    | 1716/2594 [00:07<00:03, 221.55it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  68%|████████████████████████████████████████████████████████████████████████▉                                  | 1768/2594 [00:07<00:03, 225.24it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  70%|███████████████████████████████████████████████████████████████████████████                                | 1820/2594 [00:07<00:03, 228.83it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  72%|█████████████████████████████████████████████████████████████████████████████▏                             | 1872/2594 [00:08<00:03, 232.30it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  74%|███████████████████████████████████████████████████████████████████████████████▎                           | 1924/2594 [00:08<00:02, 235.68it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  76%|█████████████████████████████████████████████████████████████████████████████████▌                         | 1976/2594 [00:08<00:02, 238.95it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  78%|███████████████████████████████████████████████████████████████████████████████████▋                       | 2028/2594 [00:08<00:02, 242.12it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  80%|█████████████████████████████████████████████████████████████████████████████████████▊                     | 2080/2594 [00:08<00:02, 245.29it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  82%|███████████████████████████████████████████████████████████████████████████████████████▉                   | 2132/2594 [00:08<00:01, 248.31it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  84%|██████████████████████████████████████████████████████████████████████████████████████████                 | 2184/2594 [00:08<00:01, 251.40it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  86%|████████████████████████████████████████████████████████████████████████████████████████████▏              | 2236/2594 [00:08<00:01, 254.32it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  88%|██████████████████████████████████████████████████████████████████████████████████████████████▍            | 2288/2594 [00:08<00:01, 257.13it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  90%|████████████████████████████████████████████████████████████████████████████████████████████████▌          | 2340/2594 [00:09<00:00, 259.90it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████▋        | 2392/2594 [00:09<00:00, 262.62it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  94%|████████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2444/2594 [00:09<00:00, 265.19it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2496/2594 [00:09<00:00, 267.82it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████  | 2548/2594 [00:09<00:00, 270.39it/s, loss=3.53e-06, v_num=32, train_loss=0.0048, test_loss=0.00485]\u001b[A\n",
      "Epoch 5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.23it/s, loss=3.53e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  50%|████████████████████████████████████████████████████▉                                                     | 1296/2594 [00:06<00:06, 195.14it/s, loss=4.34e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[AAdjusting learning rate of group 0 to 9.8263e-03.\n",
      "Epoch 6:  50%|█████████████████████████████████████████████████████                                                     | 1297/2594 [00:06<00:06, 193.54it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  50%|█████████████████████████████████████████████████████                                                     | 1300/2594 [00:06<00:06, 187.42it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  52%|███████████████████████████████████████████████████████▏                                                  | 1352/2594 [00:07<00:06, 191.85it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  54%|█████████████████████████████████████████████████████████▎                                                | 1404/2594 [00:07<00:06, 196.26it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  56%|███████████████████████████████████████████████████████████▍                                              | 1456/2594 [00:07<00:05, 200.58it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  58%|█████████████████████████████████████████████████████████████▌                                            | 1508/2594 [00:07<00:05, 204.80it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  60%|███████████████████████████████████████████████████████████████▋                                          | 1560/2594 [00:07<00:04, 208.88it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  62%|█████████████████████████████████████████████████████████████████▊                                        | 1612/2594 [00:07<00:04, 212.75it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  64%|███████████████████████████████████████████████████████████████████▉                                      | 1664/2594 [00:07<00:04, 216.66it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  66%|██████████████████████████████████████████████████████████████████████                                    | 1716/2594 [00:07<00:03, 220.45it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  68%|████████████████████████████████████████████████████████████████████████▏                                 | 1768/2594 [00:07<00:03, 224.21it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  70%|██████████████████████████████████████████████████████████████████████████▎                               | 1820/2594 [00:07<00:03, 227.82it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  72%|████████████████████████████████████████████████████████████████████████████▍                             | 1872/2594 [00:08<00:03, 231.31it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  74%|██████████████████████████████████████████████████████████████████████████████▌                           | 1924/2594 [00:08<00:02, 234.77it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  76%|████████████████████████████████████████████████████████████████████████████████▋                         | 1976/2594 [00:08<00:02, 238.08it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  78%|██████████████████████████████████████████████████████████████████████████████████▊                       | 2028/2594 [00:08<00:02, 241.35it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  80%|████████████████████████████████████████████████████████████████████████████████████▉                     | 2080/2594 [00:08<00:02, 244.56it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  82%|███████████████████████████████████████████████████████████████████████████████████████                   | 2132/2594 [00:08<00:01, 247.60it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  84%|█████████████████████████████████████████████████████████████████████████████████████████▏                | 2184/2594 [00:08<00:01, 250.60it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  86%|███████████████████████████████████████████████████████████████████████████████████████████▎              | 2236/2594 [00:08<00:01, 253.54it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  88%|█████████████████████████████████████████████████████████████████████████████████████████████▍            | 2288/2594 [00:08<00:01, 256.45it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  90%|███████████████████████████████████████████████████████████████████████████████████████████████▌          | 2340/2594 [00:09<00:00, 259.31it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████▋        | 2392/2594 [00:09<00:00, 262.04it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2444/2594 [00:09<00:00, 264.69it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2496/2594 [00:09<00:00, 267.33it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████  | 2548/2594 [00:09<00:00, 269.95it/s, loss=4.21e-06, v_num=32, train_loss=0.00423, test_loss=0.00427]\u001b[A\n",
      "Epoch 6: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.05it/s, loss=4.21e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  50%|████████████████████████████████████████████████████▉                                                     | 1296/2594 [00:06<00:06, 196.57it/s, loss=5.53e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[AAdjusting learning rate of group 0 to 9.8017e-03.\n",
      "Epoch 7:  50%|█████████████████████████████████████████████████████                                                     | 1297/2594 [00:06<00:06, 194.97it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  50%|█████████████████████████████████████████████████████                                                     | 1300/2594 [00:06<00:06, 189.14it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  52%|███████████████████████████████████████████████████████▏                                                  | 1352/2594 [00:07<00:06, 193.26it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  54%|█████████████████████████████████████████████████████████▎                                                | 1404/2594 [00:07<00:06, 197.60it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  56%|███████████████████████████████████████████████████████████▍                                              | 1456/2594 [00:07<00:05, 201.75it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  58%|█████████████████████████████████████████████████████████████▌                                            | 1508/2594 [00:07<00:05, 205.86it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  60%|███████████████████████████████████████████████████████████████▋                                          | 1560/2594 [00:07<00:04, 209.92it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  62%|█████████████████████████████████████████████████████████████████▊                                        | 1612/2594 [00:07<00:04, 213.85it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  64%|███████████████████████████████████████████████████████████████████▉                                      | 1664/2594 [00:07<00:04, 217.76it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  66%|██████████████████████████████████████████████████████████████████████                                    | 1716/2594 [00:07<00:03, 221.52it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  68%|████████████████████████████████████████████████████████████████████████▏                                 | 1768/2594 [00:07<00:03, 225.24it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  70%|██████████████████████████████████████████████████████████████████████████▎                               | 1820/2594 [00:07<00:03, 228.85it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  72%|████████████████████████████████████████████████████████████████████████████▍                             | 1872/2594 [00:08<00:03, 232.37it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  74%|██████████████████████████████████████████████████████████████████████████████▌                           | 1924/2594 [00:08<00:02, 235.84it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  76%|████████████████████████████████████████████████████████████████████████████████▋                         | 1976/2594 [00:08<00:02, 239.17it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  78%|██████████████████████████████████████████████████████████████████████████████████▊                       | 2028/2594 [00:08<00:02, 242.42it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  80%|████████████████████████████████████████████████████████████████████████████████████▉                     | 2080/2594 [00:08<00:02, 245.62it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  82%|███████████████████████████████████████████████████████████████████████████████████████                   | 2132/2594 [00:08<00:01, 248.67it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  84%|█████████████████████████████████████████████████████████████████████████████████████████▏                | 2184/2594 [00:08<00:01, 251.65it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  86%|███████████████████████████████████████████████████████████████████████████████████████████▎              | 2236/2594 [00:08<00:01, 254.64it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  88%|█████████████████████████████████████████████████████████████████████████████████████████████▍            | 2288/2594 [00:08<00:01, 257.47it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  90%|███████████████████████████████████████████████████████████████████████████████████████████████▌          | 2340/2594 [00:08<00:00, 260.32it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████▋        | 2392/2594 [00:09<00:00, 263.00it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2444/2594 [00:09<00:00, 265.70it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2496/2594 [00:09<00:00, 268.38it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████  | 2548/2594 [00:09<00:00, 270.94it/s, loss=5.44e-06, v_num=32, train_loss=0.00673, test_loss=0.00681]\u001b[A\n",
      "Epoch 7: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.83it/s, loss=5.44e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  50%|████████████████████████████████████████████████████▉                                                     | 1296/2594 [00:07<00:07, 181.87it/s, loss=1.96e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[AAdjusting learning rate of group 0 to 9.7772e-03.\n",
      "Epoch 8:  50%|█████████████████████████████████████████████████████                                                     | 1297/2594 [00:07<00:07, 179.48it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  50%|█████████████████████████████████████████████████████                                                     | 1300/2594 [00:07<00:07, 174.54it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  52%|███████████████████████████████████████████████████████▏                                                  | 1352/2594 [00:07<00:06, 178.75it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  54%|█████████████████████████████████████████████████████████▎                                                | 1404/2594 [00:07<00:06, 183.09it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  56%|███████████████████████████████████████████████████████████▍                                              | 1456/2594 [00:07<00:06, 187.30it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  58%|█████████████████████████████████████████████████████████████▌                                            | 1508/2594 [00:07<00:05, 191.47it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  60%|███████████████████████████████████████████████████████████████▋                                          | 1560/2594 [00:07<00:05, 195.43it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  62%|█████████████████████████████████████████████████████████████████▊                                        | 1612/2594 [00:08<00:04, 199.29it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  64%|███████████████████████████████████████████████████████████████████▉                                      | 1664/2594 [00:08<00:04, 203.12it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  66%|██████████████████████████████████████████████████████████████████████                                    | 1716/2594 [00:08<00:04, 206.78it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  68%|████████████████████████████████████████████████████████████████████████▏                                 | 1768/2594 [00:08<00:03, 210.36it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  70%|██████████████████████████████████████████████████████████████████████████▎                               | 1820/2594 [00:08<00:03, 213.94it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  72%|████████████████████████████████████████████████████████████████████████████▍                             | 1872/2594 [00:08<00:03, 217.41it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  74%|██████████████████████████████████████████████████████████████████████████████▌                           | 1924/2594 [00:08<00:03, 220.83it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  76%|████████████████████████████████████████████████████████████████████████████████▋                         | 1976/2594 [00:09<00:02, 212.01it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  78%|██████████████████████████████████████████████████████████████████████████████████▊                       | 2028/2594 [00:09<00:02, 215.18it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  80%|████████████████████████████████████████████████████████████████████████████████████▉                     | 2080/2594 [00:09<00:02, 218.28it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  82%|███████████████████████████████████████████████████████████████████████████████████████                   | 2132/2594 [00:09<00:02, 221.31it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  84%|█████████████████████████████████████████████████████████████████████████████████████████▏                | 2184/2594 [00:09<00:01, 224.30it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  86%|███████████████████████████████████████████████████████████████████████████████████████████▎              | 2236/2594 [00:09<00:01, 227.23it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  88%|█████████████████████████████████████████████████████████████████████████████████████████████▍            | 2288/2594 [00:09<00:01, 229.99it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  90%|███████████████████████████████████████████████████████████████████████████████████████████████▌          | 2340/2594 [00:10<00:01, 232.80it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████▋        | 2392/2594 [00:10<00:00, 235.48it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2444/2594 [00:10<00:00, 238.13it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Epoch 8:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2496/2594 [00:10<00:00, 240.75it/s, loss=2.59e-06, v_num=32, train_loss=0.00492, test_loss=0.00502]\u001b[A\n",
      "Validating:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍            | 1199/1297 [00:03<00:00, 481.33it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:10<00:00, 244.15it/s, loss=2.59e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  50%|████████████████████████████████████████████████████▉                                                     | 1296/2594 [00:07<00:07, 180.82it/s, loss=1.79e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[AAdjusting learning rate of group 0 to 9.7528e-03.\n",
      "Epoch 9:  50%|█████████████████████████████████████████████████████                                                     | 1297/2594 [00:07<00:07, 179.05it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  50%|█████████████████████████████████████████████████████                                                     | 1300/2594 [00:07<00:07, 174.03it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  52%|███████████████████████████████████████████████████████▏                                                  | 1352/2594 [00:08<00:07, 166.41it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  54%|█████████████████████████████████████████████████████████▎                                                | 1404/2594 [00:08<00:06, 170.55it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  56%|███████████████████████████████████████████████████████████▍                                              | 1456/2594 [00:08<00:06, 174.60it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  58%|█████████████████████████████████████████████████████████████▌                                            | 1508/2594 [00:08<00:06, 178.55it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  60%|███████████████████████████████████████████████████████████████▋                                          | 1560/2594 [00:08<00:05, 182.47it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  62%|█████████████████████████████████████████████████████████████████▊                                        | 1612/2594 [00:08<00:05, 186.27it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  64%|███████████████████████████████████████████████████████████████████▉                                      | 1664/2594 [00:08<00:04, 190.00it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  66%|██████████████████████████████████████████████████████████████████████                                    | 1716/2594 [00:08<00:04, 193.62it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  68%|████████████████████████████████████████████████████████████████████████▏                                 | 1768/2594 [00:08<00:04, 197.18it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Validating:  36%|█████████████████████████████████████████████████████████████▎                                                                                                          | 473/1297 [00:01<00:01, 457.01it/s]\u001b[A\n",
      "Epoch 9:  70%|██████████████████████████████████████████████████████████████████████████▎                               | 1820/2594 [00:09<00:03, 195.58it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  72%|████████████████████████████████████████████████████████████████████████████▍                             | 1872/2594 [00:09<00:03, 198.92it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  74%|██████████████████████████████████████████████████████████████████████████████▌                           | 1924/2594 [00:09<00:03, 202.20it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  76%|████████████████████████████████████████████████████████████████████████████████▋                         | 1976/2594 [00:09<00:03, 205.44it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  78%|██████████████████████████████████████████████████████████████████████████████████▊                       | 2028/2594 [00:09<00:02, 208.55it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  80%|████████████████████████████████████████████████████████████████████████████████████▉                     | 2080/2594 [00:09<00:02, 211.66it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  82%|███████████████████████████████████████████████████████████████████████████████████████                   | 2132/2594 [00:09<00:02, 214.60it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  84%|█████████████████████████████████████████████████████████████████████████████████████████▏                | 2184/2594 [00:10<00:01, 217.51it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  86%|███████████████████████████████████████████████████████████████████████████████████████████▎              | 2236/2594 [00:10<00:01, 220.42it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  88%|█████████████████████████████████████████████████████████████████████████████████████████████▍            | 2288/2594 [00:10<00:01, 223.26it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  90%|███████████████████████████████████████████████████████████████████████████████████████████████▌          | 2340/2594 [00:10<00:01, 225.94it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████▋        | 2392/2594 [00:10<00:00, 228.64it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2444/2594 [00:10<00:00, 231.24it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2496/2594 [00:10<00:00, 233.87it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████  | 2548/2594 [00:10<00:00, 236.38it/s, loss=1.85e-06, v_num=32, train_loss=0.00304, test_loss=0.00305]\u001b[A\n",
      "Epoch 9: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:10<00:00, 237.34it/s, loss=1.85e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 194.07it/s, loss=2.28e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[AAdjusting learning rate of group 0 to 9.7284e-03.\n",
      "Epoch 10:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 192.42it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  50%|████████████████████████████████████████████████████▌                                                    | 1300/2594 [00:06<00:06, 186.64it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  52%|██████████████████████████████████████████████████████▋                                                  | 1352/2594 [00:07<00:06, 191.07it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  54%|████████████████████████████████████████████████████████▊                                                | 1404/2594 [00:07<00:06, 195.42it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  56%|██████████████████████████████████████████████████████████▉                                              | 1456/2594 [00:07<00:05, 199.70it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  58%|█████████████████████████████████████████████████████████████                                            | 1508/2594 [00:07<00:05, 203.88it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  60%|███████████████████████████████████████████████████████████████▏                                         | 1560/2594 [00:07<00:04, 208.04it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  62%|█████████████████████████████████████████████████████████████████▎                                       | 1612/2594 [00:07<00:04, 212.03it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  64%|███████████████████████████████████████████████████████████████████▎                                     | 1664/2594 [00:07<00:04, 215.93it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  66%|█████████████████████████████████████████████████████████████████████▍                                   | 1716/2594 [00:07<00:03, 219.68it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  68%|███████████████████████████████████████████████████████████████████████▌                                 | 1768/2594 [00:07<00:03, 223.32it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  70%|█████████████████████████████████████████████████████████████████████████▋                               | 1820/2594 [00:08<00:03, 226.81it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  72%|███████████████████████████████████████████████████████████████████████████▊                             | 1872/2594 [00:08<00:03, 230.24it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  74%|█████████████████████████████████████████████████████████████████████████████▉                           | 1924/2594 [00:08<00:02, 233.61it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  76%|███████████████████████████████████████████████████████████████████████████████▉                         | 1976/2594 [00:08<00:02, 236.96it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  78%|██████████████████████████████████████████████████████████████████████████████████                       | 2028/2594 [00:08<00:02, 240.20it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  80%|████████████████████████████████████████████████████████████████████████████████████▏                    | 2080/2594 [00:08<00:02, 243.40it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  82%|██████████████████████████████████████████████████████████████████████████████████████▎                  | 2132/2594 [00:08<00:01, 246.50it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  84%|████████████████████████████████████████████████████████████████████████████████████████▍                | 2184/2594 [00:08<00:01, 249.55it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  86%|██████████████████████████████████████████████████████████████████████████████████████████▌              | 2236/2594 [00:08<00:01, 252.58it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  88%|████████████████████████████████████████████████████████████████████████████████████████████▌            | 2288/2594 [00:08<00:01, 255.48it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▋          | 2340/2594 [00:09<00:00, 258.26it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▊        | 2392/2594 [00:09<00:00, 261.02it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▉      | 2444/2594 [00:09<00:00, 263.70it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 266.29it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2548/2594 [00:09<00:00, 268.84it/s, loss=2.26e-06, v_num=32, train_loss=0.00188, test_loss=0.00192]\u001b[A\n",
      "Epoch 10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.76it/s, loss=2.26e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.51it/s, loss=2.35e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[AAdjusting learning rate of group 0 to 9.7041e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.83it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  50%|████████████████████████████████████████████████████▌                                                    | 1300/2594 [00:06<00:06, 188.09it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  52%|██████████████████████████████████████████████████████▋                                                  | 1352/2594 [00:07<00:06, 192.48it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  54%|████████████████████████████████████████████████████████▊                                                | 1404/2594 [00:07<00:06, 197.00it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  56%|██████████████████████████████████████████████████████████▉                                              | 1456/2594 [00:07<00:05, 201.40it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  58%|█████████████████████████████████████████████████████████████                                            | 1508/2594 [00:07<00:05, 205.64it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  60%|███████████████████████████████████████████████████████████████▏                                         | 1560/2594 [00:07<00:04, 209.72it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  62%|█████████████████████████████████████████████████████████████████▎                                       | 1612/2594 [00:07<00:04, 213.69it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  64%|███████████████████████████████████████████████████████████████████▎                                     | 1664/2594 [00:07<00:04, 217.56it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  66%|█████████████████████████████████████████████████████████████████████▍                                   | 1716/2594 [00:07<00:03, 221.39it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  68%|███████████████████████████████████████████████████████████████████████▌                                 | 1768/2594 [00:07<00:03, 225.06it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  70%|█████████████████████████████████████████████████████████████████████████▋                               | 1820/2594 [00:07<00:03, 228.69it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  72%|███████████████████████████████████████████████████████████████████████████▊                             | 1872/2594 [00:08<00:03, 232.21it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  74%|█████████████████████████████████████████████████████████████████████████████▉                           | 1924/2594 [00:08<00:02, 235.63it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  76%|███████████████████████████████████████████████████████████████████████████████▉                         | 1976/2594 [00:08<00:02, 238.94it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  78%|██████████████████████████████████████████████████████████████████████████████████                       | 2028/2594 [00:08<00:02, 242.11it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  80%|████████████████████████████████████████████████████████████████████████████████████▏                    | 2080/2594 [00:08<00:02, 245.21it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  82%|██████████████████████████████████████████████████████████████████████████████████████▎                  | 2132/2594 [00:08<00:01, 248.31it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  84%|████████████████████████████████████████████████████████████████████████████████████████▍                | 2184/2594 [00:08<00:01, 251.26it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  86%|██████████████████████████████████████████████████████████████████████████████████████████▌              | 2236/2594 [00:08<00:01, 254.23it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  88%|████████████████████████████████████████████████████████████████████████████████████████████▌            | 2288/2594 [00:08<00:01, 257.05it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▋          | 2340/2594 [00:09<00:00, 259.89it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▊        | 2392/2594 [00:09<00:00, 262.67it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▉      | 2444/2594 [00:09<00:00, 265.33it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 267.92it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2548/2594 [00:09<00:00, 270.47it/s, loss=2.08e-06, v_num=32, train_loss=0.00287, test_loss=0.00288]\u001b[A\n",
      "Epoch 11: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.32it/s, loss=2.08e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.97it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[AAdjusting learning rate of group 0 to 9.6798e-03.\n",
      "Epoch 12:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 194.25it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  50%|████████████████████████████████████████████████████▌                                                    | 1300/2594 [00:06<00:06, 188.14it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  52%|██████████████████████████████████████████████████████▋                                                  | 1352/2594 [00:07<00:06, 192.48it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  54%|████████████████████████████████████████████████████████▊                                                | 1404/2594 [00:07<00:06, 196.89it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  56%|██████████████████████████████████████████████████████████▉                                              | 1456/2594 [00:07<00:05, 201.21it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  58%|█████████████████████████████████████████████████████████████                                            | 1508/2594 [00:07<00:05, 205.36it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  60%|███████████████████████████████████████████████████████████████▏                                         | 1560/2594 [00:07<00:04, 209.42it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  62%|█████████████████████████████████████████████████████████████████▎                                       | 1612/2594 [00:07<00:04, 213.34it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  64%|███████████████████████████████████████████████████████████████████▎                                     | 1664/2594 [00:07<00:04, 217.15it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  66%|█████████████████████████████████████████████████████████████████████▍                                   | 1716/2594 [00:07<00:03, 220.83it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  68%|███████████████████████████████████████████████████████████████████████▌                                 | 1768/2594 [00:07<00:03, 224.44it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  70%|█████████████████████████████████████████████████████████████████████████▋                               | 1820/2594 [00:07<00:03, 228.03it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  72%|███████████████████████████████████████████████████████████████████████████▊                             | 1872/2594 [00:08<00:03, 231.53it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  74%|█████████████████████████████████████████████████████████████████████████████▉                           | 1924/2594 [00:08<00:02, 234.94it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  76%|███████████████████████████████████████████████████████████████████████████████▉                         | 1976/2594 [00:08<00:02, 238.30it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  78%|██████████████████████████████████████████████████████████████████████████████████                       | 2028/2594 [00:08<00:02, 241.44it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  80%|████████████████████████████████████████████████████████████████████████████████████▏                    | 2080/2594 [00:08<00:02, 244.58it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  82%|██████████████████████████████████████████████████████████████████████████████████████▎                  | 2132/2594 [00:08<00:01, 247.73it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  84%|████████████████████████████████████████████████████████████████████████████████████████▍                | 2184/2594 [00:08<00:01, 250.71it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  86%|██████████████████████████████████████████████████████████████████████████████████████████▌              | 2236/2594 [00:08<00:01, 253.66it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  88%|████████████████████████████████████████████████████████████████████████████████████████████▌            | 2288/2594 [00:08<00:01, 256.47it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▋          | 2340/2594 [00:09<00:00, 259.26it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▊        | 2392/2594 [00:09<00:00, 262.06it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▉      | 2444/2594 [00:09<00:00, 264.79it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Validating:  88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                   | 1147/1297 [00:02<00:00, 503.42it/s]\u001b[A\n",
      "Epoch 12:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 267.31it/s, loss=1.16e-06, v_num=32, train_loss=0.00219, test_loss=0.00221]\u001b[A\n",
      "Epoch 12: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.71it/s, loss=1.16e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  50%|████████████████████████████████████████████████████▉                                                     | 1296/2594 [00:06<00:06, 194.78it/s, loss=1.43e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[AAdjusting learning rate of group 0 to 9.6556e-03.\n",
      "Epoch 13:  50%|█████████████████████████████████████████████████████                                                     | 1297/2594 [00:06<00:06, 193.11it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  50%|█████████████████████████████████████████████████████                                                     | 1300/2594 [00:06<00:06, 187.46it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  52%|███████████████████████████████████████████████████████▏                                                  | 1352/2594 [00:07<00:06, 191.83it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  54%|█████████████████████████████████████████████████████████▎                                                | 1404/2594 [00:07<00:06, 196.29it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  56%|███████████████████████████████████████████████████████████▍                                              | 1456/2594 [00:07<00:05, 200.57it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  58%|█████████████████████████████████████████████████████████████▌                                            | 1508/2594 [00:07<00:05, 204.70it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  60%|███████████████████████████████████████████████████████████████▋                                          | 1560/2594 [00:07<00:04, 208.76it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  62%|█████████████████████████████████████████████████████████████████▊                                        | 1612/2594 [00:07<00:04, 212.66it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  64%|███████████████████████████████████████████████████████████████████▉                                      | 1664/2594 [00:07<00:04, 216.53it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  66%|██████████████████████████████████████████████████████████████████████                                    | 1716/2594 [00:07<00:03, 220.13it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  68%|████████████████████████████████████████████████████████████████████████▏                                 | 1768/2594 [00:07<00:03, 223.80it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  70%|██████████████████████████████████████████████████████████████████████████▎                               | 1820/2594 [00:08<00:03, 227.43it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  72%|████████████████████████████████████████████████████████████████████████████▍                             | 1872/2594 [00:08<00:03, 230.91it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  74%|██████████████████████████████████████████████████████████████████████████████▌                           | 1924/2594 [00:08<00:02, 234.30it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  76%|████████████████████████████████████████████████████████████████████████████████▋                         | 1976/2594 [00:08<00:02, 237.59it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  78%|██████████████████████████████████████████████████████████████████████████████████▊                       | 2028/2594 [00:08<00:02, 240.84it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  80%|████████████████████████████████████████████████████████████████████████████████████▉                     | 2080/2594 [00:08<00:02, 244.03it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  82%|███████████████████████████████████████████████████████████████████████████████████████                   | 2132/2594 [00:08<00:01, 247.12it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  84%|█████████████████████████████████████████████████████████████████████████████████████████▏                | 2184/2594 [00:08<00:01, 250.09it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  86%|███████████████████████████████████████████████████████████████████████████████████████████▎              | 2236/2594 [00:08<00:01, 253.05it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  88%|█████████████████████████████████████████████████████████████████████████████████████████████▍            | 2288/2594 [00:08<00:01, 255.88it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  90%|███████████████████████████████████████████████████████████████████████████████████████████████▌          | 2340/2594 [00:09<00:00, 258.70it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████▋        | 2392/2594 [00:09<00:00, 261.45it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2444/2594 [00:09<00:00, 264.12it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2496/2594 [00:09<00:00, 266.78it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████  | 2548/2594 [00:09<00:00, 269.29it/s, loss=1.47e-06, v_num=32, train_loss=0.0019, test_loss=0.00193]\u001b[A\n",
      "Epoch 13: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.96it/s, loss=1.47e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  50%|████████████████████████████████████████████████████▉                                                     | 1296/2594 [00:06<00:06, 195.92it/s, loss=2.26e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[AAdjusting learning rate of group 0 to 9.6315e-03.\n",
      "Epoch 14:  50%|█████████████████████████████████████████████████████                                                     | 1297/2594 [00:06<00:06, 194.22it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  50%|█████████████████████████████████████████████████████                                                     | 1300/2594 [00:06<00:06, 188.47it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  52%|███████████████████████████████████████████████████████▏                                                  | 1352/2594 [00:07<00:06, 192.70it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  54%|█████████████████████████████████████████████████████████▎                                                | 1404/2594 [00:07<00:06, 197.06it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  56%|███████████████████████████████████████████████████████████▍                                              | 1456/2594 [00:07<00:05, 201.43it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  58%|█████████████████████████████████████████████████████████████▌                                            | 1508/2594 [00:07<00:05, 205.66it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  60%|███████████████████████████████████████████████████████████████▋                                          | 1560/2594 [00:07<00:04, 209.78it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  62%|█████████████████████████████████████████████████████████████████▊                                        | 1612/2594 [00:07<00:04, 213.71it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  64%|███████████████████████████████████████████████████████████████████▉                                      | 1664/2594 [00:07<00:04, 217.59it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  66%|██████████████████████████████████████████████████████████████████████                                    | 1716/2594 [00:07<00:03, 221.44it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  68%|████████████████████████████████████████████████████████████████████████▏                                 | 1768/2594 [00:07<00:03, 225.08it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  70%|██████████████████████████████████████████████████████████████████████████▎                               | 1820/2594 [00:07<00:03, 228.59it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  72%|████████████████████████████████████████████████████████████████████████████▍                             | 1872/2594 [00:08<00:03, 232.04it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  74%|██████████████████████████████████████████████████████████████████████████████▌                           | 1924/2594 [00:08<00:02, 235.46it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  76%|████████████████████████████████████████████████████████████████████████████████▋                         | 1976/2594 [00:08<00:02, 238.85it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  78%|██████████████████████████████████████████████████████████████████████████████████▊                       | 2028/2594 [00:08<00:02, 242.09it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  80%|████████████████████████████████████████████████████████████████████████████████████▉                     | 2080/2594 [00:08<00:02, 245.23it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  82%|███████████████████████████████████████████████████████████████████████████████████████                   | 2132/2594 [00:08<00:01, 248.33it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  84%|█████████████████████████████████████████████████████████████████████████████████████████▏                | 2184/2594 [00:08<00:01, 251.37it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:  86%|███████████████████████████████████████████████████████████████████████████████████████████▎              | 2236/2594 [00:08<00:01, 254.27it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  88%|█████████████████████████████████████████████████████████████████████████████████████████████▍            | 2288/2594 [00:08<00:01, 257.15it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  90%|███████████████████████████████████████████████████████████████████████████████████████████████▌          | 2340/2594 [00:09<00:00, 259.89it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████▋        | 2392/2594 [00:09<00:00, 262.67it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2444/2594 [00:09<00:00, 265.32it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2496/2594 [00:09<00:00, 267.89it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████  | 2548/2594 [00:09<00:00, 270.43it/s, loss=2.28e-06, v_num=32, train_loss=0.0023, test_loss=0.00232]\u001b[A\n",
      "Epoch 14: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.20it/s, loss=2.28e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 194.58it/s, loss=1.21e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[AAdjusting learning rate of group 0 to 9.6074e-03.\n",
      "Epoch 15:  50%|█████████████████████████████████████████████████████                                                     | 1297/2594 [00:06<00:06, 192.88it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  50%|█████████████████████████████████████████████████████                                                     | 1300/2594 [00:06<00:06, 187.06it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  52%|███████████████████████████████████████████████████████▏                                                  | 1352/2594 [00:07<00:06, 191.27it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  54%|█████████████████████████████████████████████████████████▎                                                | 1404/2594 [00:07<00:06, 195.70it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  56%|███████████████████████████████████████████████████████████▍                                              | 1456/2594 [00:07<00:05, 199.89it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  58%|█████████████████████████████████████████████████████████████▌                                            | 1508/2594 [00:07<00:05, 204.01it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  60%|███████████████████████████████████████████████████████████████▋                                          | 1560/2594 [00:07<00:04, 208.06it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  62%|█████████████████████████████████████████████████████████████████▊                                        | 1612/2594 [00:07<00:04, 212.01it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  64%|███████████████████████████████████████████████████████████████████▉                                      | 1664/2594 [00:07<00:04, 215.80it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  66%|██████████████████████████████████████████████████████████████████████                                    | 1716/2594 [00:07<00:03, 219.57it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  68%|████████████████████████████████████████████████████████████████████████▏                                 | 1768/2594 [00:07<00:03, 223.21it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  70%|██████████████████████████████████████████████████████████████████████████▎                               | 1820/2594 [00:08<00:03, 226.65it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  72%|████████████████████████████████████████████████████████████████████████████▍                             | 1872/2594 [00:08<00:03, 230.17it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  74%|██████████████████████████████████████████████████████████████████████████████▌                           | 1924/2594 [00:08<00:02, 233.53it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  76%|████████████████████████████████████████████████████████████████████████████████▋                         | 1976/2594 [00:08<00:02, 236.81it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  78%|██████████████████████████████████████████████████████████████████████████████████▊                       | 2028/2594 [00:08<00:02, 240.07it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  80%|████████████████████████████████████████████████████████████████████████████████████▉                     | 2080/2594 [00:08<00:02, 243.18it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  82%|███████████████████████████████████████████████████████████████████████████████████████                   | 2132/2594 [00:08<00:01, 246.24it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  84%|█████████████████████████████████████████████████████████████████████████████████████████▏                | 2184/2594 [00:08<00:01, 249.21it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Validating:  68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                     | 887/1297 [00:02<00:00, 493.02it/s]\u001b[A\n",
      "Epoch 15:  86%|███████████████████████████████████████████████████████████████████████████████████████████▎              | 2236/2594 [00:08<00:01, 252.08it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  88%|█████████████████████████████████████████████████████████████████████████████████████████████▍            | 2288/2594 [00:08<00:01, 254.99it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  90%|███████████████████████████████████████████████████████████████████████████████████████████████▌          | 2340/2594 [00:09<00:00, 257.78it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████▋        | 2392/2594 [00:09<00:00, 260.49it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2444/2594 [00:09<00:00, 263.25it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n",
      "Epoch 15:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2496/2594 [00:09<00:00, 265.83it/s, loss=1.2e-06, v_num=32, train_loss=0.00238, test_loss=0.00246]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.27it/s, loss=1.2e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 194.39it/s, loss=1.22e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[AAdjusting learning rate of group 0 to 9.5834e-03.\n",
      "Epoch 16:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 192.78it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  50%|████████████████████████████████████████████████████▌                                                    | 1300/2594 [00:06<00:06, 186.88it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  52%|██████████████████████████████████████████████████████▋                                                  | 1352/2594 [00:07<00:06, 191.32it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  54%|████████████████████████████████████████████████████████▊                                                | 1404/2594 [00:07<00:06, 195.77it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  56%|██████████████████████████████████████████████████████████▉                                              | 1456/2594 [00:07<00:05, 200.00it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  58%|█████████████████████████████████████████████████████████████                                            | 1508/2594 [00:07<00:05, 204.19it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  60%|███████████████████████████████████████████████████████████████▏                                         | 1560/2594 [00:07<00:04, 208.25it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  62%|█████████████████████████████████████████████████████████████████▎                                       | 1612/2594 [00:07<00:04, 212.14it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  64%|███████████████████████████████████████████████████████████████████▎                                     | 1664/2594 [00:07<00:04, 216.01it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  66%|█████████████████████████████████████████████████████████████████████▍                                   | 1716/2594 [00:07<00:03, 219.80it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  68%|███████████████████████████████████████████████████████████████████████▌                                 | 1768/2594 [00:07<00:03, 223.48it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  70%|█████████████████████████████████████████████████████████████████████████▋                               | 1820/2594 [00:08<00:03, 227.04it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  72%|███████████████████████████████████████████████████████████████████████████▊                             | 1872/2594 [00:08<00:03, 230.63it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  74%|█████████████████████████████████████████████████████████████████████████████▉                           | 1924/2594 [00:08<00:02, 234.01it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  76%|███████████████████████████████████████████████████████████████████████████████▉                         | 1976/2594 [00:08<00:02, 237.30it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  78%|██████████████████████████████████████████████████████████████████████████████████                       | 2028/2594 [00:08<00:02, 240.57it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  80%|████████████████████████████████████████████████████████████████████████████████████▏                    | 2080/2594 [00:08<00:02, 243.74it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  82%|██████████████████████████████████████████████████████████████████████████████████████▎                  | 2132/2594 [00:08<00:01, 246.80it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  84%|████████████████████████████████████████████████████████████████████████████████████████▍                | 2184/2594 [00:08<00:01, 249.79it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  86%|██████████████████████████████████████████████████████████████████████████████████████████▌              | 2236/2594 [00:08<00:01, 252.70it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  88%|████████████████████████████████████████████████████████████████████████████████████████████▌            | 2288/2594 [00:08<00:01, 255.58it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▋          | 2340/2594 [00:09<00:00, 258.39it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▊        | 2392/2594 [00:09<00:00, 261.06it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▉      | 2444/2594 [00:09<00:00, 263.77it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 266.32it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2548/2594 [00:09<00:00, 268.91it/s, loss=1.24e-06, v_num=32, train_loss=0.00122, test_loss=0.00124]\u001b[A\n",
      "Epoch 16: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.77it/s, loss=1.24e-06, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.92it/s, loss=7.91e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[AAdjusting learning rate of group 0 to 9.5594e-03.\n",
      "Epoch 17:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 194.28it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  50%|████████████████████████████████████████████████████▌                                                    | 1300/2594 [00:06<00:06, 188.49it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  52%|██████████████████████████████████████████████████████▋                                                  | 1352/2594 [00:07<00:06, 192.94it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  54%|████████████████████████████████████████████████████████▊                                                | 1404/2594 [00:07<00:06, 197.29it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  56%|██████████████████████████████████████████████████████████▉                                              | 1456/2594 [00:07<00:05, 201.60it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  58%|█████████████████████████████████████████████████████████████                                            | 1508/2594 [00:07<00:05, 205.77it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  60%|███████████████████████████████████████████████████████████████▏                                         | 1560/2594 [00:07<00:04, 209.89it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  62%|█████████████████████████████████████████████████████████████████▎                                       | 1612/2594 [00:07<00:04, 213.86it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  64%|███████████████████████████████████████████████████████████████████▎                                     | 1664/2594 [00:07<00:04, 217.68it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  66%|█████████████████████████████████████████████████████████████████████▍                                   | 1716/2594 [00:07<00:03, 221.40it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  68%|███████████████████████████████████████████████████████████████████████▌                                 | 1768/2594 [00:07<00:03, 224.98it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  70%|█████████████████████████████████████████████████████████████████████████▋                               | 1820/2594 [00:07<00:03, 228.53it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  72%|███████████████████████████████████████████████████████████████████████████▊                             | 1872/2594 [00:08<00:03, 231.96it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  74%|█████████████████████████████████████████████████████████████████████████████▉                           | 1924/2594 [00:08<00:02, 235.30it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  76%|███████████████████████████████████████████████████████████████████████████████▉                         | 1976/2594 [00:08<00:02, 238.62it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  78%|██████████████████████████████████████████████████████████████████████████████████                       | 2028/2594 [00:08<00:02, 241.87it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  80%|████████████████████████████████████████████████████████████████████████████████████▏                    | 2080/2594 [00:08<00:02, 245.09it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  82%|██████████████████████████████████████████████████████████████████████████████████████▎                  | 2132/2594 [00:08<00:01, 248.15it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  84%|████████████████████████████████████████████████████████████████████████████████████████▍                | 2184/2594 [00:08<00:01, 251.08it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  86%|██████████████████████████████████████████████████████████████████████████████████████████▌              | 2236/2594 [00:08<00:01, 253.93it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  88%|████████████████████████████████████████████████████████████████████████████████████████████▌            | 2288/2594 [00:08<00:01, 256.75it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▋          | 2340/2594 [00:09<00:00, 259.58it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▊        | 2392/2594 [00:09<00:00, 262.37it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▉      | 2444/2594 [00:09<00:00, 265.00it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 267.61it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2548/2594 [00:09<00:00, 270.25it/s, loss=7.29e-07, v_num=32, train_loss=0.00145, test_loss=0.00148]\u001b[A\n",
      "Epoch 17: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.04it/s, loss=7.29e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 196.34it/s, loss=1.04e-06, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[AAdjusting learning rate of group 0 to 9.5355e-03.\n",
      "Epoch 18:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.51it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  50%|███████████████████████████████████████████████████▌                                                   | 1300/2594 [00:06<00:06, 187.91it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  52%|█████████████████████████████████████████████████████▋                                                 | 1352/2594 [00:07<00:06, 192.14it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  54%|███████████████████████████████████████████████████████▋                                               | 1404/2594 [00:07<00:06, 196.58it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  56%|█████████████████████████████████████████████████████████▊                                             | 1456/2594 [00:07<00:05, 200.91it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  58%|███████████████████████████████████████████████████████████▉                                           | 1508/2594 [00:07<00:05, 205.07it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  60%|█████████████████████████████████████████████████████████████▉                                         | 1560/2594 [00:07<00:04, 209.13it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  62%|████████████████████████████████████████████████████████████████                                       | 1612/2594 [00:07<00:04, 213.17it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  64%|██████████████████████████████████████████████████████████████████                                     | 1664/2594 [00:07<00:04, 217.03it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  66%|████████████████████████████████████████████████████████████████████▏                                  | 1716/2594 [00:07<00:03, 220.81it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  68%|██████████████████████████████████████████████████████████████████████▏                                | 1768/2594 [00:07<00:03, 224.51it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  70%|████████████████████████████████████████████████████████████████████████▎                              | 1820/2594 [00:07<00:03, 228.11it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  72%|██████████████████████████████████████████████████████████████████████████▎                            | 1872/2594 [00:08<00:03, 231.54it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  74%|████████████████████████████████████████████████████████████████████████████▍                          | 1924/2594 [00:08<00:02, 234.85it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  76%|██████████████████████████████████████████████████████████████████████████████▍                        | 1976/2594 [00:08<00:02, 238.21it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  78%|████████████████████████████████████████████████████████████████████████████████▌                      | 2028/2594 [00:08<00:02, 241.52it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  80%|██████████████████████████████████████████████████████████████████████████████████▌                    | 2080/2594 [00:08<00:02, 244.70it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  82%|████████████████████████████████████████████████████████████████████████████████████▋                  | 2132/2594 [00:08<00:01, 247.81it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  84%|██████████████████████████████████████████████████████████████████████████████████████▋                | 2184/2594 [00:08<00:01, 250.85it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  86%|████████████████████████████████████████████████████████████████████████████████████████▊              | 2236/2594 [00:08<00:01, 253.76it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  88%|██████████████████████████████████████████████████████████████████████████████████████████▊            | 2288/2594 [00:08<00:01, 256.64it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  90%|████████████████████████████████████████████████████████████████████████████████████████████▉          | 2340/2594 [00:09<00:00, 259.47it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▉        | 2392/2594 [00:09<00:00, 262.25it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████      | 2444/2594 [00:09<00:00, 264.95it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 267.60it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2548/2594 [00:09<00:00, 270.17it/s, loss=9.87e-07, v_num=32, train_loss=0.000898, test_loss=0.000918]\u001b[A\n",
      "Epoch 18: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.99it/s, loss=9.87e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.07it/s, loss=6.65e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[AAdjusting learning rate of group 0 to 9.5117e-03.\n",
      "Epoch 19:  50%|█████████████████████████████████████████████████████                                                     | 1297/2594 [00:06<00:06, 193.41it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  50%|█████████████████████████████████████████████████████                                                     | 1300/2594 [00:06<00:06, 187.29it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  52%|███████████████████████████████████████████████████████▏                                                  | 1352/2594 [00:07<00:06, 191.70it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  54%|█████████████████████████████████████████████████████████▎                                                | 1404/2594 [00:07<00:06, 196.06it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  56%|███████████████████████████████████████████████████████████▍                                              | 1456/2594 [00:07<00:05, 200.33it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  58%|█████████████████████████████████████████████████████████████▌                                            | 1508/2594 [00:07<00:05, 204.59it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  60%|███████████████████████████████████████████████████████████████▋                                          | 1560/2594 [00:07<00:04, 208.65it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  62%|█████████████████████████████████████████████████████████████████▊                                        | 1612/2594 [00:07<00:04, 212.54it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  64%|███████████████████████████████████████████████████████████████████▉                                      | 1664/2594 [00:07<00:04, 216.36it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  66%|██████████████████████████████████████████████████████████████████████                                    | 1716/2594 [00:07<00:03, 220.10it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  68%|████████████████████████████████████████████████████████████████████████▏                                 | 1768/2594 [00:07<00:03, 223.69it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  70%|██████████████████████████████████████████████████████████████████████████▎                               | 1820/2594 [00:08<00:03, 227.17it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  72%|████████████████████████████████████████████████████████████████████████████▍                             | 1872/2594 [00:08<00:03, 230.59it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  74%|██████████████████████████████████████████████████████████████████████████████▌                           | 1924/2594 [00:08<00:02, 233.98it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  76%|████████████████████████████████████████████████████████████████████████████████▋                         | 1976/2594 [00:08<00:02, 237.24it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  78%|██████████████████████████████████████████████████████████████████████████████████▊                       | 2028/2594 [00:08<00:02, 240.49it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  80%|████████████████████████████████████████████████████████████████████████████████████▉                     | 2080/2594 [00:08<00:02, 243.65it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:  60%|█████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                  | 783/1297 [00:01<00:01, 493.58it/s]\u001b[A\n",
      "Epoch 19:  82%|███████████████████████████████████████████████████████████████████████████████████████                   | 2132/2594 [00:08<00:01, 246.69it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  84%|█████████████████████████████████████████████████████████████████████████████████████████▏                | 2184/2594 [00:08<00:01, 249.60it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  86%|███████████████████████████████████████████████████████████████████████████████████████████▎              | 2236/2594 [00:08<00:01, 252.50it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  88%|█████████████████████████████████████████████████████████████████████████████████████████████▍            | 2288/2594 [00:08<00:01, 255.27it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  90%|███████████████████████████████████████████████████████████████████████████████████████████████▌          | 2340/2594 [00:09<00:00, 258.10it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████▋        | 2392/2594 [00:09<00:00, 260.72it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2444/2594 [00:09<00:00, 263.29it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2496/2594 [00:09<00:00, 265.76it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████  | 2548/2594 [00:09<00:00, 268.27it/s, loss=8.5e-07, v_num=32, train_loss=0.00125, test_loss=0.00127]\u001b[A\n",
      "Epoch 19: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.43it/s, loss=8.5e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.91it/s, loss=7.03e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[AAdjusting learning rate of group 0 to 9.4879e-03.\n",
      "Epoch 20:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 194.19it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  50%|████████████████████████████████████████████████████▌                                                    | 1300/2594 [00:06<00:06, 188.41it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  52%|██████████████████████████████████████████████████████▋                                                  | 1352/2594 [00:07<00:06, 192.66it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  54%|████████████████████████████████████████████████████████▊                                                | 1404/2594 [00:07<00:06, 197.00it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  56%|██████████████████████████████████████████████████████████▉                                              | 1456/2594 [00:07<00:05, 201.29it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  58%|█████████████████████████████████████████████████████████████                                            | 1508/2594 [00:07<00:05, 205.52it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  60%|███████████████████████████████████████████████████████████████▏                                         | 1560/2594 [00:07<00:04, 209.65it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  62%|█████████████████████████████████████████████████████████████████▎                                       | 1612/2594 [00:07<00:04, 213.72it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  64%|███████████████████████████████████████████████████████████████████▎                                     | 1664/2594 [00:07<00:04, 217.55it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  66%|█████████████████████████████████████████████████████████████████████▍                                   | 1716/2594 [00:07<00:03, 221.35it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  68%|███████████████████████████████████████████████████████████████████████▌                                 | 1768/2594 [00:07<00:03, 225.12it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  70%|█████████████████████████████████████████████████████████████████████████▋                               | 1820/2594 [00:07<00:03, 228.76it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  72%|███████████████████████████████████████████████████████████████████████████▊                             | 1872/2594 [00:08<00:03, 232.26it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  74%|█████████████████████████████████████████████████████████████████████████████▉                           | 1924/2594 [00:08<00:02, 235.67it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  76%|███████████████████████████████████████████████████████████████████████████████▉                         | 1976/2594 [00:08<00:02, 239.00it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  78%|██████████████████████████████████████████████████████████████████████████████████                       | 2028/2594 [00:08<00:02, 242.33it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  80%|████████████████████████████████████████████████████████████████████████████████████▏                    | 2080/2594 [00:08<00:02, 245.52it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  82%|██████████████████████████████████████████████████████████████████████████████████████▎                  | 2132/2594 [00:08<00:01, 248.65it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  84%|████████████████████████████████████████████████████████████████████████████████████████▍                | 2184/2594 [00:08<00:01, 251.63it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  86%|██████████████████████████████████████████████████████████████████████████████████████████▌              | 2236/2594 [00:08<00:01, 254.55it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  88%|████████████████████████████████████████████████████████████████████████████████████████████▌            | 2288/2594 [00:08<00:01, 257.44it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▋          | 2340/2594 [00:08<00:00, 260.26it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▊        | 2392/2594 [00:09<00:00, 263.00it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▉      | 2444/2594 [00:09<00:00, 265.58it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 268.17it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2548/2594 [00:09<00:00, 270.75it/s, loss=6.93e-07, v_num=32, train_loss=0.00136, test_loss=0.00136]\u001b[A\n",
      "Epoch 20: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.89it/s, loss=6.93e-07, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 195.69it/s, loss=1.5e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[AAdjusting learning rate of group 0 to 9.4642e-03.\n",
      "Epoch 21:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 194.01it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  50%|███████████████████████████████████████████████████▌                                                   | 1300/2594 [00:06<00:06, 188.06it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  52%|█████████████████████████████████████████████████████▋                                                 | 1352/2594 [00:07<00:06, 192.46it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  54%|███████████████████████████████████████████████████████▋                                               | 1404/2594 [00:07<00:06, 196.82it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  56%|█████████████████████████████████████████████████████████▊                                             | 1456/2594 [00:07<00:05, 201.03it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  58%|███████████████████████████████████████████████████████████▉                                           | 1508/2594 [00:07<00:05, 205.13it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  60%|█████████████████████████████████████████████████████████████▉                                         | 1560/2594 [00:07<00:04, 209.18it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  62%|████████████████████████████████████████████████████████████████                                       | 1612/2594 [00:07<00:04, 213.12it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  64%|██████████████████████████████████████████████████████████████████                                     | 1664/2594 [00:07<00:04, 216.99it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  66%|████████████████████████████████████████████████████████████████████▏                                  | 1716/2594 [00:07<00:03, 220.74it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  68%|██████████████████████████████████████████████████████████████████████▏                                | 1768/2594 [00:07<00:03, 224.38it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  70%|████████████████████████████████████████████████████████████████████████▎                              | 1820/2594 [00:07<00:03, 227.91it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  72%|██████████████████████████████████████████████████████████████████████████▎                            | 1872/2594 [00:08<00:03, 231.38it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  74%|████████████████████████████████████████████████████████████████████████████▍                          | 1924/2594 [00:08<00:02, 234.82it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  76%|██████████████████████████████████████████████████████████████████████████████▍                        | 1976/2594 [00:08<00:02, 238.11it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  78%|████████████████████████████████████████████████████████████████████████████████▌                      | 2028/2594 [00:08<00:02, 241.30it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  80%|██████████████████████████████████████████████████████████████████████████████████▌                    | 2080/2594 [00:08<00:02, 244.44it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  82%|████████████████████████████████████████████████████████████████████████████████████▋                  | 2132/2594 [00:08<00:01, 247.45it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  84%|██████████████████████████████████████████████████████████████████████████████████████▋                | 2184/2594 [00:08<00:01, 250.50it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Validating:  68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                     | 887/1297 [00:02<00:00, 496.64it/s]\u001b[A\n",
      "Epoch 21:  86%|████████████████████████████████████████████████████████████████████████████████████████▊              | 2236/2594 [00:08<00:01, 253.38it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  88%|██████████████████████████████████████████████████████████████████████████████████████████▊            | 2288/2594 [00:08<00:01, 256.23it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  90%|████████████████████████████████████████████████████████████████████████████████████████████▉          | 2340/2594 [00:09<00:00, 259.11it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▉        | 2392/2594 [00:09<00:00, 261.86it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████      | 2444/2594 [00:09<00:00, 264.50it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 267.14it/s, loss=1.47e-06, v_num=32, train_loss=0.000838, test_loss=0.000849]\u001b[A\n",
      "Epoch 21: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.46it/s, loss=1.47e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 194.95it/s, loss=1.09e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[AAdjusting learning rate of group 0 to 9.4405e-03.\n",
      "Epoch 22:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.21it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22:  50%|████████████████████████████████████████████████████▌                                                    | 1300/2594 [00:06<00:06, 187.41it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  52%|██████████████████████████████████████████████████████▋                                                  | 1352/2594 [00:07<00:06, 191.75it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  54%|████████████████████████████████████████████████████████▊                                                | 1404/2594 [00:07<00:06, 196.13it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  56%|██████████████████████████████████████████████████████████▉                                              | 1456/2594 [00:07<00:05, 200.36it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  58%|█████████████████████████████████████████████████████████████                                            | 1508/2594 [00:07<00:05, 204.57it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  60%|███████████████████████████████████████████████████████████████▏                                         | 1560/2594 [00:07<00:04, 208.63it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  62%|█████████████████████████████████████████████████████████████████▎                                       | 1612/2594 [00:07<00:04, 212.58it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  64%|███████████████████████████████████████████████████████████████████▎                                     | 1664/2594 [00:07<00:04, 216.49it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  66%|█████████████████████████████████████████████████████████████████████▍                                   | 1716/2594 [00:07<00:03, 220.31it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  68%|███████████████████████████████████████████████████████████████████████▌                                 | 1768/2594 [00:07<00:03, 223.98it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  70%|█████████████████████████████████████████████████████████████████████████▋                               | 1820/2594 [00:08<00:03, 227.57it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  72%|███████████████████████████████████████████████████████████████████████████▊                             | 1872/2594 [00:08<00:03, 231.01it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  74%|█████████████████████████████████████████████████████████████████████████████▉                           | 1924/2594 [00:08<00:02, 234.42it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  76%|███████████████████████████████████████████████████████████████████████████████▉                         | 1976/2594 [00:08<00:02, 237.73it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  78%|██████████████████████████████████████████████████████████████████████████████████                       | 2028/2594 [00:08<00:02, 240.94it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  80%|████████████████████████████████████████████████████████████████████████████████████▏                    | 2080/2594 [00:08<00:02, 244.12it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  82%|██████████████████████████████████████████████████████████████████████████████████████▎                  | 2132/2594 [00:08<00:01, 247.21it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  84%|████████████████████████████████████████████████████████████████████████████████████████▍                | 2184/2594 [00:08<00:01, 250.27it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  86%|██████████████████████████████████████████████████████████████████████████████████████████▌              | 2236/2594 [00:08<00:01, 253.18it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  88%|████████████████████████████████████████████████████████████████████████████████████████████▌            | 2288/2594 [00:08<00:01, 256.02it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▋          | 2340/2594 [00:09<00:00, 258.88it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▊        | 2392/2594 [00:09<00:00, 261.60it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▉      | 2444/2594 [00:09<00:00, 264.29it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 266.99it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2548/2594 [00:09<00:00, 269.57it/s, loss=1.03e-06, v_num=32, train_loss=0.00171, test_loss=0.00174]\u001b[A\n",
      "Epoch 22: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.40it/s, loss=1.03e-06, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.02it/s, loss=8.07e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[AAdjusting learning rate of group 0 to 9.4169e-03.\n",
      "Epoch 23:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.43it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23:  50%|████████████████████████████████████████████████████▌                                                    | 1300/2594 [00:06<00:06, 187.31it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  52%|██████████████████████████████████████████████████████▋                                                  | 1352/2594 [00:07<00:06, 191.62it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  54%|████████████████████████████████████████████████████████▊                                                | 1404/2594 [00:07<00:06, 196.11it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  56%|██████████████████████████████████████████████████████████▉                                              | 1456/2594 [00:07<00:05, 200.39it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  58%|█████████████████████████████████████████████████████████████                                            | 1508/2594 [00:07<00:05, 204.54it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  60%|███████████████████████████████████████████████████████████████▏                                         | 1560/2594 [00:07<00:04, 208.55it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:  62%|█████████████████████████████████████████████████████████████████▎                                       | 1612/2594 [00:07<00:04, 212.44it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  64%|███████████████████████████████████████████████████████████████████▎                                     | 1664/2594 [00:07<00:04, 216.34it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  66%|█████████████████████████████████████████████████████████████████████▍                                   | 1716/2594 [00:07<00:03, 220.08it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  68%|███████████████████████████████████████████████████████████████████████▌                                 | 1768/2594 [00:07<00:03, 223.71it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  70%|█████████████████████████████████████████████████████████████████████████▋                               | 1820/2594 [00:08<00:03, 227.36it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  72%|███████████████████████████████████████████████████████████████████████████▊                             | 1872/2594 [00:08<00:03, 230.80it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  74%|█████████████████████████████████████████████████████████████████████████████▉                           | 1924/2594 [00:08<00:02, 234.26it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  76%|███████████████████████████████████████████████████████████████████████████████▉                         | 1976/2594 [00:08<00:02, 237.62it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  78%|██████████████████████████████████████████████████████████████████████████████████                       | 2028/2594 [00:08<00:02, 240.86it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  80%|████████████████████████████████████████████████████████████████████████████████████▏                    | 2080/2594 [00:08<00:02, 244.05it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  82%|██████████████████████████████████████████████████████████████████████████████████████▎                  | 2132/2594 [00:08<00:01, 247.06it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  84%|████████████████████████████████████████████████████████████████████████████████████████▍                | 2184/2594 [00:08<00:01, 250.07it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  86%|██████████████████████████████████████████████████████████████████████████████████████████▌              | 2236/2594 [00:08<00:01, 252.98it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  88%|████████████████████████████████████████████████████████████████████████████████████████████▌            | 2288/2594 [00:08<00:01, 255.84it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▋          | 2340/2594 [00:09<00:00, 258.65it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▊        | 2392/2594 [00:09<00:00, 261.38it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▉      | 2444/2594 [00:09<00:00, 264.07it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 266.71it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2548/2594 [00:09<00:00, 269.23it/s, loss=7.86e-07, v_num=32, train_loss=0.00103, test_loss=0.00105]\u001b[A\n",
      "Epoch 23: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.37it/s, loss=7.86e-07, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 195.00it/s, loss=2.71e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[AAdjusting learning rate of group 0 to 9.3934e-03.\n",
      "Epoch 24:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 193.31it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24:  50%|████████████████████████████████████████████████████                                                    | 1300/2594 [00:06<00:06, 187.14it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  52%|██████████████████████████████████████████████████████▏                                                 | 1352/2594 [00:07<00:06, 191.53it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  54%|████████████████████████████████████████████████████████▎                                               | 1404/2594 [00:07<00:06, 195.93it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  56%|██████████████████████████████████████████████████████████▎                                             | 1456/2594 [00:07<00:05, 200.26it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  58%|████████████████████████████████████████████████████████████▍                                           | 1508/2594 [00:07<00:05, 204.52it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  60%|██████████████████████████████████████████████████████████████▌                                         | 1560/2594 [00:07<00:04, 208.64it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  62%|████████████████████████████████████████████████████████████████▋                                       | 1612/2594 [00:07<00:04, 212.66it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  64%|██████████████████████████████████████████████████████████████████▋                                     | 1664/2594 [00:07<00:04, 216.60it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  66%|████████████████████████████████████████████████████████████████████▊                                   | 1716/2594 [00:07<00:03, 220.33it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  68%|██████████████████████████████████████████████████████████████████████▉                                 | 1768/2594 [00:07<00:03, 223.93it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  70%|████████████████████████████████████████████████████████████████████████▉                               | 1820/2594 [00:08<00:03, 227.48it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  72%|███████████████████████████████████████████████████████████████████████████                             | 1872/2594 [00:08<00:03, 230.88it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  74%|█████████████████████████████████████████████████████████████████████████████▏                          | 1924/2594 [00:08<00:02, 234.22it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24:  76%|███████████████████████████████████████████████████████████████████████████████▏                        | 1976/2594 [00:08<00:02, 237.54it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  78%|█████████████████████████████████████████████████████████████████████████████████▎                      | 2028/2594 [00:08<00:02, 240.71it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  80%|███████████████████████████████████████████████████████████████████████████████████▍                    | 2080/2594 [00:08<00:02, 243.87it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  82%|█████████████████████████████████████████████████████████████████████████████████████▍                  | 2132/2594 [00:08<00:01, 246.88it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  84%|███████████████████████████████████████████████████████████████████████████████████████▌                | 2184/2594 [00:08<00:01, 249.90it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  86%|█████████████████████████████████████████████████████████████████████████████████████████▋              | 2236/2594 [00:08<00:01, 252.83it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  88%|███████████████████████████████████████████████████████████████████████████████████████████▋            | 2288/2594 [00:08<00:01, 255.72it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▊          | 2340/2594 [00:09<00:00, 258.52it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▉        | 2392/2594 [00:09<00:00, 261.28it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▉      | 2444/2594 [00:09<00:00, 263.96it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 266.64it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2548/2594 [00:09<00:00, 269.25it/s, loss=2.79e-06, v_num=32, train_loss=0.000757, test_loss=0.00077]\u001b[A\n",
      "Epoch 24: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.12it/s, loss=2.79e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  50%|████████████████████████████████████████████████████▉                                                     | 1296/2594 [00:06<00:06, 195.41it/s, loss=1.17e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[AAdjusting learning rate of group 0 to 9.3699e-03.\n",
      "Epoch 25:  50%|█████████████████████████████████████████████████████                                                     | 1297/2594 [00:06<00:06, 193.85it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25:  50%|█████████████████████████████████████████████████████                                                     | 1300/2594 [00:06<00:06, 187.79it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  52%|███████████████████████████████████████████████████████▏                                                  | 1352/2594 [00:07<00:06, 192.23it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  54%|█████████████████████████████████████████████████████████▎                                                | 1404/2594 [00:07<00:06, 196.62it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  56%|███████████████████████████████████████████████████████████▍                                              | 1456/2594 [00:07<00:05, 200.93it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  58%|█████████████████████████████████████████████████████████████▌                                            | 1508/2594 [00:07<00:05, 205.08it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  60%|███████████████████████████████████████████████████████████████▋                                          | 1560/2594 [00:07<00:04, 209.12it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  62%|█████████████████████████████████████████████████████████████████▊                                        | 1612/2594 [00:07<00:04, 213.07it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  64%|███████████████████████████████████████████████████████████████████▉                                      | 1664/2594 [00:07<00:04, 217.01it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  66%|██████████████████████████████████████████████████████████████████████                                    | 1716/2594 [00:07<00:03, 220.75it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  68%|████████████████████████████████████████████████████████████████████████▏                                 | 1768/2594 [00:07<00:03, 224.48it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  70%|██████████████████████████████████████████████████████████████████████████▎                               | 1820/2594 [00:07<00:03, 228.10it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  72%|████████████████████████████████████████████████████████████████████████████▍                             | 1872/2594 [00:08<00:03, 231.60it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  74%|██████████████████████████████████████████████████████████████████████████████▌                           | 1924/2594 [00:08<00:02, 234.99it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  76%|████████████████████████████████████████████████████████████████████████████████▋                         | 1976/2594 [00:08<00:02, 238.41it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  78%|██████████████████████████████████████████████████████████████████████████████████▊                       | 2028/2594 [00:08<00:02, 241.67it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  80%|████████████████████████████████████████████████████████████████████████████████████▉                     | 2080/2594 [00:08<00:02, 244.90it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  82%|███████████████████████████████████████████████████████████████████████████████████████                   | 2132/2594 [00:08<00:01, 248.02it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  84%|█████████████████████████████████████████████████████████████████████████████████████████▏                | 2184/2594 [00:08<00:01, 251.04it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  86%|███████████████████████████████████████████████████████████████████████████████████████████▎              | 2236/2594 [00:08<00:01, 253.96it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  88%|█████████████████████████████████████████████████████████████████████████████████████████████▍            | 2288/2594 [00:08<00:01, 256.88it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:  90%|███████████████████████████████████████████████████████████████████████████████████████████████▌          | 2340/2594 [00:09<00:00, 259.65it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████▋        | 2392/2594 [00:09<00:00, 262.41it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2444/2594 [00:09<00:00, 265.12it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2496/2594 [00:09<00:00, 267.78it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████  | 2548/2594 [00:09<00:00, 270.35it/s, loss=1.09e-06, v_num=32, train_loss=0.0025, test_loss=0.00257]\u001b[A\n",
      "Epoch 25: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.06it/s, loss=1.09e-06, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.47it/s, loss=7.83e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[AAdjusting learning rate of group 0 to 9.3465e-03.\n",
      "Epoch 26:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.39it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26:  50%|████████████████████████████████████████████████████▌                                                    | 1300/2594 [00:06<00:06, 187.44it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  52%|██████████████████████████████████████████████████████▋                                                  | 1352/2594 [00:07<00:06, 191.79it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  54%|████████████████████████████████████████████████████████▊                                                | 1404/2594 [00:07<00:06, 196.31it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  56%|██████████████████████████████████████████████████████████▉                                              | 1456/2594 [00:07<00:05, 200.66it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  58%|█████████████████████████████████████████████████████████████                                            | 1508/2594 [00:07<00:05, 204.86it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  60%|███████████████████████████████████████████████████████████████▏                                         | 1560/2594 [00:07<00:04, 208.94it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  62%|█████████████████████████████████████████████████████████████████▎                                       | 1612/2594 [00:07<00:04, 212.77it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  64%|███████████████████████████████████████████████████████████████████▎                                     | 1664/2594 [00:07<00:04, 216.61it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  66%|█████████████████████████████████████████████████████████████████████▍                                   | 1716/2594 [00:07<00:03, 220.39it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  68%|███████████████████████████████████████████████████████████████████████▌                                 | 1768/2594 [00:07<00:03, 224.07it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  70%|█████████████████████████████████████████████████████████████████████████▋                               | 1820/2594 [00:07<00:03, 227.65it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  72%|███████████████████████████████████████████████████████████████████████████▊                             | 1872/2594 [00:08<00:03, 231.08it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  74%|█████████████████████████████████████████████████████████████████████████████▉                           | 1924/2594 [00:08<00:02, 234.38it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  76%|███████████████████████████████████████████████████████████████████████████████▉                         | 1976/2594 [00:08<00:02, 237.72it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  78%|██████████████████████████████████████████████████████████████████████████████████                       | 2028/2594 [00:08<00:02, 240.92it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  80%|████████████████████████████████████████████████████████████████████████████████████▏                    | 2080/2594 [00:08<00:02, 244.06it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  82%|██████████████████████████████████████████████████████████████████████████████████████▎                  | 2132/2594 [00:08<00:01, 247.19it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  84%|████████████████████████████████████████████████████████████████████████████████████████▍                | 2184/2594 [00:08<00:01, 250.18it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  86%|██████████████████████████████████████████████████████████████████████████████████████████▌              | 2236/2594 [00:08<00:01, 253.14it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  88%|████████████████████████████████████████████████████████████████████████████████████████████▌            | 2288/2594 [00:08<00:01, 255.98it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▋          | 2340/2594 [00:09<00:00, 258.80it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▊        | 2392/2594 [00:09<00:00, 261.61it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▉      | 2444/2594 [00:09<00:00, 264.31it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 266.95it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2548/2594 [00:09<00:00, 269.55it/s, loss=7.68e-07, v_num=32, train_loss=0.00101, test_loss=0.00102]\u001b[A\n",
      "Epoch 26: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.31it/s, loss=7.68e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.10it/s, loss=8.41e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[AAdjusting learning rate of group 0 to 9.3231e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.47it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27:  50%|████████████████████████████████████████████████████▌                                                    | 1300/2594 [00:06<00:06, 187.31it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  52%|██████████████████████████████████████████████████████▋                                                  | 1352/2594 [00:07<00:06, 191.83it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  54%|████████████████████████████████████████████████████████▊                                                | 1404/2594 [00:07<00:06, 196.27it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  56%|██████████████████████████████████████████████████████████▉                                              | 1456/2594 [00:07<00:05, 200.50it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  58%|█████████████████████████████████████████████████████████████                                            | 1508/2594 [00:07<00:05, 204.74it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  60%|███████████████████████████████████████████████████████████████▏                                         | 1560/2594 [00:07<00:04, 208.83it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  62%|█████████████████████████████████████████████████████████████████▎                                       | 1612/2594 [00:07<00:04, 212.82it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  64%|███████████████████████████████████████████████████████████████████▎                                     | 1664/2594 [00:07<00:04, 216.66it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  66%|█████████████████████████████████████████████████████████████████████▍                                   | 1716/2594 [00:07<00:03, 220.47it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  68%|███████████████████████████████████████████████████████████████████████▌                                 | 1768/2594 [00:07<00:03, 224.12it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  70%|█████████████████████████████████████████████████████████████████████████▋                               | 1820/2594 [00:07<00:03, 227.69it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  72%|███████████████████████████████████████████████████████████████████████████▊                             | 1872/2594 [00:08<00:03, 231.20it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  74%|█████████████████████████████████████████████████████████████████████████████▉                           | 1924/2594 [00:08<00:02, 234.59it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  76%|███████████████████████████████████████████████████████████████████████████████▉                         | 1976/2594 [00:08<00:02, 237.92it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  78%|██████████████████████████████████████████████████████████████████████████████████                       | 2028/2594 [00:08<00:02, 241.15it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  80%|████████████████████████████████████████████████████████████████████████████████████▏                    | 2080/2594 [00:08<00:02, 244.24it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  82%|██████████████████████████████████████████████████████████████████████████████████████▎                  | 2132/2594 [00:08<00:01, 247.40it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  84%|████████████████████████████████████████████████████████████████████████████████████████▍                | 2184/2594 [00:08<00:01, 250.43it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  86%|██████████████████████████████████████████████████████████████████████████████████████████▌              | 2236/2594 [00:08<00:01, 253.45it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  88%|████████████████████████████████████████████████████████████████████████████████████████████▌            | 2288/2594 [00:08<00:01, 256.34it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▋          | 2340/2594 [00:09<00:00, 259.15it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▊        | 2392/2594 [00:09<00:00, 261.93it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▉      | 2444/2594 [00:09<00:00, 264.62it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 267.27it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2548/2594 [00:09<00:00, 269.84it/s, loss=8.54e-07, v_num=32, train_loss=0.00103, test_loss=0.00104]\u001b[A\n",
      "Epoch 27: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.69it/s, loss=8.54e-07, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 195.71it/s, loss=1.44e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[AAdjusting learning rate of group 0 to 9.2998e-03.\n",
      "Epoch 28:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 193.98it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28:  50%|████████████████████████████████████████████████████                                                    | 1300/2594 [00:06<00:06, 187.84it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  52%|██████████████████████████████████████████████████████▏                                                 | 1352/2594 [00:07<00:06, 192.19it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  54%|████████████████████████████████████████████████████████▎                                               | 1404/2594 [00:07<00:06, 196.64it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  56%|██████████████████████████████████████████████████████████▎                                             | 1456/2594 [00:07<00:05, 201.01it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28:  58%|████████████████████████████████████████████████████████████▍                                           | 1508/2594 [00:07<00:05, 205.25it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  60%|██████████████████████████████████████████████████████████████▌                                         | 1560/2594 [00:07<00:04, 209.40it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  62%|████████████████████████████████████████████████████████████████▋                                       | 1612/2594 [00:07<00:04, 213.37it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  64%|██████████████████████████████████████████████████████████████████▋                                     | 1664/2594 [00:07<00:04, 217.27it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  66%|████████████████████████████████████████████████████████████████████▊                                   | 1716/2594 [00:07<00:03, 221.12it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  68%|██████████████████████████████████████████████████████████████████████▉                                 | 1768/2594 [00:07<00:03, 224.79it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  70%|████████████████████████████████████████████████████████████████████████▉                               | 1820/2594 [00:07<00:03, 228.43it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  72%|███████████████████████████████████████████████████████████████████████████                             | 1872/2594 [00:08<00:03, 231.94it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  74%|█████████████████████████████████████████████████████████████████████████████▏                          | 1924/2594 [00:08<00:02, 235.41it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  76%|███████████████████████████████████████████████████████████████████████████████▏                        | 1976/2594 [00:08<00:02, 238.79it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  78%|█████████████████████████████████████████████████████████████████████████████████▎                      | 2028/2594 [00:08<00:02, 242.01it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  80%|███████████████████████████████████████████████████████████████████████████████████▍                    | 2080/2594 [00:08<00:02, 245.18it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  82%|█████████████████████████████████████████████████████████████████████████████████████▍                  | 2132/2594 [00:08<00:01, 248.25it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  84%|███████████████████████████████████████████████████████████████████████████████████████▌                | 2184/2594 [00:08<00:01, 251.20it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  86%|█████████████████████████████████████████████████████████████████████████████████████████▋              | 2236/2594 [00:08<00:01, 254.19it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  88%|███████████████████████████████████████████████████████████████████████████████████████████▋            | 2288/2594 [00:08<00:01, 257.09it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▊          | 2340/2594 [00:09<00:00, 259.86it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▉        | 2392/2594 [00:09<00:00, 262.62it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▉      | 2444/2594 [00:09<00:00, 265.34it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 267.95it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2548/2594 [00:09<00:00, 270.52it/s, loss=1.39e-06, v_num=32, train_loss=0.000835, test_loss=0.00084]\u001b[A\n",
      "Epoch 28: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.33it/s, loss=1.39e-06, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  50%|████████████████████████████████████████████████████▉                                                     | 1296/2594 [00:06<00:06, 194.95it/s, loss=8.19e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[AAdjusting learning rate of group 0 to 9.2766e-03.\n",
      "Epoch 29:  50%|█████████████████████████████████████████████████████                                                     | 1297/2594 [00:06<00:06, 193.27it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29:  50%|█████████████████████████████████████████████████████                                                     | 1300/2594 [00:06<00:06, 187.70it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  52%|███████████████████████████████████████████████████████▏                                                  | 1352/2594 [00:07<00:06, 191.92it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  54%|█████████████████████████████████████████████████████████▎                                                | 1404/2594 [00:07<00:06, 196.31it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  56%|███████████████████████████████████████████████████████████▍                                              | 1456/2594 [00:07<00:05, 200.66it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  58%|█████████████████████████████████████████████████████████████▌                                            | 1508/2594 [00:07<00:05, 204.90it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  60%|███████████████████████████████████████████████████████████████▋                                          | 1560/2594 [00:07<00:04, 209.05it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  62%|█████████████████████████████████████████████████████████████████▊                                        | 1612/2594 [00:07<00:04, 213.03it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  64%|███████████████████████████████████████████████████████████████████▉                                      | 1664/2594 [00:07<00:04, 216.92it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  66%|██████████████████████████████████████████████████████████████████████                                    | 1716/2594 [00:07<00:03, 220.73it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  68%|████████████████████████████████████████████████████████████████████████▏                                 | 1768/2594 [00:07<00:03, 224.34it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  70%|██████████████████████████████████████████████████████████████████████████▎                               | 1820/2594 [00:07<00:03, 227.89it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29:  72%|████████████████████████████████████████████████████████████████████████████▍                             | 1872/2594 [00:08<00:03, 231.42it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  74%|██████████████████████████████████████████████████████████████████████████████▌                           | 1924/2594 [00:08<00:02, 234.78it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  76%|████████████████████████████████████████████████████████████████████████████████▋                         | 1976/2594 [00:08<00:02, 238.06it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  78%|██████████████████████████████████████████████████████████████████████████████████▊                       | 2028/2594 [00:08<00:02, 241.24it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  80%|████████████████████████████████████████████████████████████████████████████████████▉                     | 2080/2594 [00:08<00:02, 244.39it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  82%|███████████████████████████████████████████████████████████████████████████████████████                   | 2132/2594 [00:08<00:01, 247.47it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  84%|█████████████████████████████████████████████████████████████████████████████████████████▏                | 2184/2594 [00:08<00:01, 250.42it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  86%|███████████████████████████████████████████████████████████████████████████████████████████▎              | 2236/2594 [00:08<00:01, 253.34it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  88%|█████████████████████████████████████████████████████████████████████████████████████████████▍            | 2288/2594 [00:08<00:01, 256.24it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  90%|███████████████████████████████████████████████████████████████████████████████████████████████▌          | 2340/2594 [00:09<00:00, 259.08it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████▋        | 2392/2594 [00:09<00:00, 261.84it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2444/2594 [00:09<00:00, 264.44it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2496/2594 [00:09<00:00, 266.99it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████  | 2548/2594 [00:09<00:00, 269.39it/s, loss=8.66e-07, v_num=32, train_loss=0.00168, test_loss=0.0017]\u001b[A\n",
      "Epoch 29: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.17it/s, loss=8.66e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 195.00it/s, loss=7.12e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[AAdjusting learning rate of group 0 to 9.2534e-03.\n",
      "Epoch 30:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 193.34it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30:  50%|████████████████████████████████████████████████████                                                    | 1300/2594 [00:06<00:06, 187.23it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  52%|██████████████████████████████████████████████████████▏                                                 | 1352/2594 [00:07<00:06, 191.71it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  54%|████████████████████████████████████████████████████████▎                                               | 1404/2594 [00:07<00:06, 196.16it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  56%|██████████████████████████████████████████████████████████▎                                             | 1456/2594 [00:07<00:05, 200.52it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  58%|████████████████████████████████████████████████████████████▍                                           | 1508/2594 [00:07<00:05, 204.72it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  60%|██████████████████████████████████████████████████████████████▌                                         | 1560/2594 [00:07<00:04, 208.85it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  62%|████████████████████████████████████████████████████████████████▋                                       | 1612/2594 [00:07<00:04, 212.90it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  64%|██████████████████████████████████████████████████████████████████▋                                     | 1664/2594 [00:07<00:04, 216.82it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  66%|████████████████████████████████████████████████████████████████████▊                                   | 1716/2594 [00:07<00:03, 220.63it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  68%|██████████████████████████████████████████████████████████████████████▉                                 | 1768/2594 [00:07<00:03, 224.10it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  70%|████████████████████████████████████████████████████████████████████████▉                               | 1820/2594 [00:07<00:03, 227.67it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  72%|███████████████████████████████████████████████████████████████████████████                             | 1872/2594 [00:08<00:03, 231.22it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  74%|█████████████████████████████████████████████████████████████████████████████▏                          | 1924/2594 [00:08<00:02, 234.65it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  76%|███████████████████████████████████████████████████████████████████████████████▏                        | 1976/2594 [00:08<00:02, 238.01it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  78%|█████████████████████████████████████████████████████████████████████████████████▎                      | 2028/2594 [00:08<00:02, 241.25it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  80%|███████████████████████████████████████████████████████████████████████████████████▍                    | 2080/2594 [00:08<00:02, 244.41it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  82%|█████████████████████████████████████████████████████████████████████████████████████▍                  | 2132/2594 [00:08<00:01, 247.56it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  84%|███████████████████████████████████████████████████████████████████████████████████████▌                | 2184/2594 [00:08<00:01, 250.59it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30:  86%|█████████████████████████████████████████████████████████████████████████████████████████▋              | 2236/2594 [00:08<00:01, 253.54it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  88%|███████████████████████████████████████████████████████████████████████████████████████████▋            | 2288/2594 [00:08<00:01, 256.41it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▊          | 2340/2594 [00:09<00:00, 259.22it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▉        | 2392/2594 [00:09<00:00, 261.91it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▉      | 2444/2594 [00:09<00:00, 264.62it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 267.25it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2548/2594 [00:09<00:00, 269.90it/s, loss=7.65e-07, v_num=32, train_loss=0.000863, test_loss=0.00087]\u001b[A\n",
      "Epoch 30: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.75it/s, loss=7.65e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 194.46it/s, loss=5.36e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[AAdjusting learning rate of group 0 to 9.2302e-03.\n",
      "Epoch 31:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 192.05it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31:  50%|████████████████████████████████████████████████████▌                                                    | 1300/2594 [00:06<00:06, 186.48it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  52%|██████████████████████████████████████████████████████▋                                                  | 1352/2594 [00:07<00:06, 190.88it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  54%|████████████████████████████████████████████████████████▊                                                | 1404/2594 [00:07<00:06, 195.29it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  56%|██████████████████████████████████████████████████████████▉                                              | 1456/2594 [00:07<00:05, 199.56it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  58%|█████████████████████████████████████████████████████████████                                            | 1508/2594 [00:07<00:05, 203.77it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  60%|███████████████████████████████████████████████████████████████▏                                         | 1560/2594 [00:07<00:04, 207.88it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  62%|█████████████████████████████████████████████████████████████████▎                                       | 1612/2594 [00:07<00:04, 211.80it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  64%|███████████████████████████████████████████████████████████████████▎                                     | 1664/2594 [00:07<00:04, 215.62it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  66%|█████████████████████████████████████████████████████████████████████▍                                   | 1716/2594 [00:07<00:04, 219.34it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  68%|███████████████████████████████████████████████████████████████████████▌                                 | 1768/2594 [00:07<00:03, 222.96it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  70%|█████████████████████████████████████████████████████████████████████████▋                               | 1820/2594 [00:08<00:03, 226.56it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  72%|███████████████████████████████████████████████████████████████████████████▊                             | 1872/2594 [00:08<00:03, 230.04it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  74%|█████████████████████████████████████████████████████████████████████████████▉                           | 1924/2594 [00:08<00:02, 233.42it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  76%|███████████████████████████████████████████████████████████████████████████████▉                         | 1976/2594 [00:08<00:02, 236.73it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  78%|██████████████████████████████████████████████████████████████████████████████████                       | 2028/2594 [00:08<00:02, 239.95it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  80%|████████████████████████████████████████████████████████████████████████████████████▏                    | 2080/2594 [00:08<00:02, 243.09it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  82%|██████████████████████████████████████████████████████████████████████████████████████▎                  | 2132/2594 [00:08<00:01, 246.24it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  84%|████████████████████████████████████████████████████████████████████████████████████████▍                | 2184/2594 [00:08<00:01, 249.26it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  86%|██████████████████████████████████████████████████████████████████████████████████████████▌              | 2236/2594 [00:08<00:01, 252.15it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  88%|████████████████████████████████████████████████████████████████████████████████████████████▌            | 2288/2594 [00:08<00:01, 254.93it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▋          | 2340/2594 [00:09<00:00, 257.71it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▊        | 2392/2594 [00:09<00:00, 260.44it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▉      | 2444/2594 [00:09<00:00, 263.08it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 265.64it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n",
      "Epoch 31:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2548/2594 [00:09<00:00, 268.20it/s, loss=5.22e-07, v_num=32, train_loss=0.00109, test_loss=0.00113]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.01it/s, loss=5.22e-07, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.94it/s, loss=1.99e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[AAdjusting learning rate of group 0 to 9.2072e-03.\n",
      "Epoch 32:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 194.30it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32:  50%|███████████████████████████████████████████████████▌                                                   | 1300/2594 [00:06<00:06, 188.20it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  52%|█████████████████████████████████████████████████████▋                                                 | 1352/2594 [00:07<00:06, 192.51it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  54%|███████████████████████████████████████████████████████▋                                               | 1404/2594 [00:07<00:06, 197.00it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  56%|█████████████████████████████████████████████████████████▊                                             | 1456/2594 [00:07<00:05, 201.44it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  58%|███████████████████████████████████████████████████████████▉                                           | 1508/2594 [00:07<00:05, 205.70it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  60%|█████████████████████████████████████████████████████████████▉                                         | 1560/2594 [00:07<00:04, 209.80it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  62%|████████████████████████████████████████████████████████████████                                       | 1612/2594 [00:07<00:04, 213.88it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  64%|██████████████████████████████████████████████████████████████████                                     | 1664/2594 [00:07<00:04, 217.81it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  66%|████████████████████████████████████████████████████████████████████▏                                  | 1716/2594 [00:07<00:03, 221.63it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  68%|██████████████████████████████████████████████████████████████████████▏                                | 1768/2594 [00:07<00:03, 225.35it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  70%|████████████████████████████████████████████████████████████████████████▎                              | 1820/2594 [00:07<00:03, 228.99it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  72%|██████████████████████████████████████████████████████████████████████████▎                            | 1872/2594 [00:08<00:03, 232.56it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  74%|████████████████████████████████████████████████████████████████████████████▍                          | 1924/2594 [00:08<00:02, 236.00it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  76%|██████████████████████████████████████████████████████████████████████████████▍                        | 1976/2594 [00:08<00:02, 239.34it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  78%|████████████████████████████████████████████████████████████████████████████████▌                      | 2028/2594 [00:08<00:02, 242.63it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  80%|██████████████████████████████████████████████████████████████████████████████████▌                    | 2080/2594 [00:08<00:02, 245.82it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  82%|████████████████████████████████████████████████████████████████████████████████████▋                  | 2132/2594 [00:08<00:01, 248.96it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  84%|██████████████████████████████████████████████████████████████████████████████████████▋                | 2184/2594 [00:08<00:01, 251.92it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  86%|████████████████████████████████████████████████████████████████████████████████████████▊              | 2236/2594 [00:08<00:01, 254.86it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  88%|██████████████████████████████████████████████████████████████████████████████████████████▊            | 2288/2594 [00:08<00:01, 257.80it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  90%|████████████████████████████████████████████████████████████████████████████████████████████▉          | 2340/2594 [00:08<00:00, 260.67it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▉        | 2392/2594 [00:09<00:00, 263.44it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████      | 2444/2594 [00:09<00:00, 266.11it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████    | 2496/2594 [00:09<00:00, 268.70it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 2548/2594 [00:09<00:00, 271.31it/s, loss=1.72e-06, v_num=32, train_loss=0.000631, test_loss=0.000636]\u001b[A\n",
      "Epoch 32: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 272.15it/s, loss=1.72e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.03it/s, loss=1.39e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[AAdjusting learning rate of group 0 to 9.1841e-03.\n",
      "Epoch 33:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.34it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33:  50%|████████████████████████████████████████████████████▌                                                    | 1300/2594 [00:06<00:06, 187.41it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  52%|██████████████████████████████████████████████████████▋                                                  | 1352/2594 [00:07<00:06, 191.79it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33:  54%|████████████████████████████████████████████████████████▊                                                | 1404/2594 [00:07<00:06, 196.19it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  56%|██████████████████████████████████████████████████████████▉                                              | 1456/2594 [00:07<00:05, 200.53it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  58%|█████████████████████████████████████████████████████████████                                            | 1508/2594 [00:07<00:05, 204.74it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  60%|███████████████████████████████████████████████████████████████▏                                         | 1560/2594 [00:07<00:04, 208.83it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  62%|█████████████████████████████████████████████████████████████████▎                                       | 1612/2594 [00:07<00:04, 212.88it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  64%|███████████████████████████████████████████████████████████████████▎                                     | 1664/2594 [00:07<00:04, 216.82it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  66%|█████████████████████████████████████████████████████████████████████▍                                   | 1716/2594 [00:07<00:03, 220.59it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  68%|███████████████████████████████████████████████████████████████████████▌                                 | 1769/2594 [00:07<00:03, 224.47it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  70%|█████████████████████████████████████████████████████████████████████████▊                               | 1822/2594 [00:07<00:03, 228.20it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  72%|███████████████████████████████████████████████████████████████████████████▉                             | 1875/2594 [00:08<00:03, 231.80it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  74%|██████████████████████████████████████████████████████████████████████████████                           | 1928/2594 [00:08<00:02, 235.29it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  76%|████████████████████████████████████████████████████████████████████████████████▏                        | 1981/2594 [00:08<00:02, 238.65it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  78%|██████████████████████████████████████████████████████████████████████████████████▎                      | 2034/2594 [00:08<00:02, 241.98it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  80%|████████████████████████████████████████████████████████████████████████████████████▍                    | 2087/2594 [00:08<00:02, 245.17it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  82%|██████████████████████████████████████████████████████████████████████████████████████▌                  | 2140/2594 [00:08<00:01, 248.28it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  85%|████████████████████████████████████████████████████████████████████████████████████████▊                | 2193/2594 [00:08<00:01, 251.27it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  87%|██████████████████████████████████████████████████████████████████████████████████████████▉              | 2246/2594 [00:08<00:01, 254.17it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  89%|█████████████████████████████████████████████████████████████████████████████████████████████            | 2299/2594 [00:08<00:01, 257.13it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  91%|███████████████████████████████████████████████████████████████████████████████████████████████▏         | 2352/2594 [00:09<00:00, 260.10it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  93%|█████████████████████████████████████████████████████████████████████████████████████████████████▎       | 2405/2594 [00:09<00:00, 262.86it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33:  95%|███████████████████████████████████████████████████████████████████████████████████████████████████▍     | 2458/2594 [00:09<00:00, 265.55it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Validating:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                 | 1161/1297 [00:02<00:00, 500.50it/s]\u001b[A\n",
      "Epoch 33:  97%|█████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 2511/2594 [00:09<00:00, 268.12it/s, loss=1.41e-06, v_num=32, train_loss=0.00209, test_loss=0.00211]\u001b[A\n",
      "Epoch 33: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.62it/s, loss=1.41e-06, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.36it/s, loss=4.83e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[AAdjusting learning rate of group 0 to 9.1612e-03.\n",
      "Epoch 34:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.62it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 34:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:06<00:06, 189.60it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 194.11it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 198.43it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 202.71it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Validating:  14%|████████████████████████▏                                                                                                                                               | 187/1297 [00:00<00:02, 391.33it/s]\u001b[A\n",
      "Epoch 34:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 206.91it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 211.01it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 215.06it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 218.90it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 222.64it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 226.33it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 229.85it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 233.31it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 236.72it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 240.08it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 243.36it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 246.61it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 249.65it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 252.65it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 255.59it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 258.46it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 261.28it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 263.97it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Epoch 34:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.64it/s, loss=4.53e-07, v_num=32, train_loss=0.00219, test_loss=0.00222]\u001b[A\n",
      "Validating:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉             | 1196/1297 [00:02<00:00, 498.44it/s]\u001b[A\n",
      "Epoch 34: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.27it/s, loss=4.53e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 194.25it/s, loss=8.82e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[AAdjusting learning rate of group 0 to 9.1383e-03.\n",
      "Epoch 35:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 191.59it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 35:  51%|█████████████████████████████████████████████████████                                                   | 1325/2594 [00:07<00:06, 187.99it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  53%|███████████████████████████████████████████████████████▏                                                | 1378/2594 [00:07<00:06, 192.58it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  55%|█████████████████████████████████████████████████████████▎                                              | 1431/2594 [00:07<00:05, 197.05it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  57%|███████████████████████████████████████████████████████████▍                                            | 1484/2594 [00:07<00:05, 201.49it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  59%|█████████████████████████████████████████████████████████████▌                                          | 1537/2594 [00:07<00:05, 205.72it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  61%|███████████████████████████████████████████████████████████████▋                                        | 1590/2594 [00:07<00:04, 209.83it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  63%|█████████████████████████████████████████████████████████████████▊                                      | 1643/2594 [00:07<00:04, 213.92it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  65%|███████████████████████████████████████████████████████████████████▉                                    | 1696/2594 [00:07<00:04, 217.87it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Validating:  31%|███████████████████████████████████████████████████▋                                                                                                                    | 399/1297 [00:01<00:01, 486.50it/s]\u001b[A\n",
      "Epoch 35:  67%|██████████████████████████████████████████████████████████████████████                                  | 1749/2594 [00:07<00:03, 221.69it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  69%|████████████████████████████████████████████████████████████████████████▏                               | 1802/2594 [00:07<00:03, 225.46it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  72%|██████████████████████████████████████████████████████████████████████████▎                             | 1855/2594 [00:08<00:03, 229.13it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  74%|████████████████████████████████████████████████████████████████████████████▍                           | 1908/2594 [00:08<00:02, 232.66it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  76%|██████████████████████████████████████████████████████████████████████████████▌                         | 1961/2594 [00:08<00:02, 236.14it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  78%|████████████████████████████████████████████████████████████████████████████████▋                       | 2014/2594 [00:08<00:02, 239.53it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  80%|██████████████████████████████████████████████████████████████████████████████████▊                     | 2067/2594 [00:08<00:02, 242.81it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35:  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 2120/2594 [00:08<00:01, 246.01it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  84%|███████████████████████████████████████████████████████████████████████████████████████                 | 2173/2594 [00:08<00:01, 249.03it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  86%|█████████████████████████████████████████████████████████████████████████████████████████▏              | 2226/2594 [00:08<00:01, 252.04it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  88%|███████████████████████████████████████████████████████████████████████████████████████████▎            | 2279/2594 [00:08<00:01, 255.04it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 257.97it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 260.74it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 263.46it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.16it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 268.80it/s, loss=8.24e-07, v_num=32, train_loss=0.000609, test_loss=0.00062]\u001b[A\n",
      "Epoch 35: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.18it/s, loss=8.24e-07, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 194.78it/s, loss=1.33e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[AAdjusting learning rate of group 0 to 9.1154e-03.\n",
      "Epoch 36:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 192.49it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 36:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:07<00:06, 188.93it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 193.52it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 197.99it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 202.39it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 206.57it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 210.63it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Validating:  23%|██████████████████████████████████████                                                                                                                                  | 294/1297 [00:00<00:02, 453.76it/s]\u001b[A\n",
      "Epoch 36:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 214.64it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 218.58it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 222.37it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 226.06it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 229.67it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 233.14it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 236.57it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 239.91it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 243.18it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 246.42it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 249.46it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 252.52it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 255.56it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 258.49it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 261.29it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 264.00it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.71it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.29it/s, loss=1.32e-06, v_num=32, train_loss=0.00126, test_loss=0.00129]\u001b[A\n",
      "Epoch 36: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.67it/s, loss=1.32e-06, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.34it/s, loss=5.87e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[AAdjusting learning rate of group 0 to 9.0926e-03.\n",
      "Epoch 37:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.69it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 37:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 189.93it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.45it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.94it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 203.36it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 207.68it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 211.85it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 215.87it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 219.77it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Validating:  31%|███████████████████████████████████████████████████▋                                                                                                                    | 399/1297 [00:01<00:01, 483.07it/s]\u001b[A\n",
      "Epoch 37:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 223.53it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 227.26it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 230.89it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 234.36it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 237.82it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 241.13it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 244.37it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 247.55it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 250.57it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 253.53it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 256.55it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:08<00:01, 259.43it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 262.20it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.99it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 267.67it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 270.34it/s, loss=5.78e-07, v_num=32, train_loss=0.000872, test_loss=0.000876]\u001b[A\n",
      "Epoch 37: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.21it/s, loss=5.78e-07, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.20it/s, loss=1.09e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[AAdjusting learning rate of group 0 to 9.0699e-03.\n",
      "Epoch 38:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 193.57it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38:  51%|█████████████████████████████████████████████████████                                                   | 1325/2594 [00:06<00:06, 189.78it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  53%|███████████████████████████████████████████████████████▏                                                | 1378/2594 [00:07<00:06, 194.42it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  55%|█████████████████████████████████████████████████████████▎                                              | 1431/2594 [00:07<00:05, 198.91it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  57%|███████████████████████████████████████████████████████████▍                                            | 1484/2594 [00:07<00:05, 203.27it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  59%|█████████████████████████████████████████████████████████████▌                                          | 1537/2594 [00:07<00:05, 207.58it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  61%|███████████████████████████████████████████████████████████████▋                                        | 1590/2594 [00:07<00:04, 211.71it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  63%|█████████████████████████████████████████████████████████████████▊                                      | 1643/2594 [00:07<00:04, 215.72it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  65%|███████████████████████████████████████████████████████████████████▉                                    | 1696/2594 [00:07<00:04, 219.61it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Validating:  31%|███████████████████████████████████████████████████▊                                                                                                                    | 400/1297 [00:01<00:01, 480.45it/s]\u001b[A\n",
      "Epoch 38:  67%|██████████████████████████████████████████████████████████████████████                                  | 1749/2594 [00:07<00:03, 223.37it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  69%|████████████████████████████████████████████████████████████████████████▏                               | 1802/2594 [00:07<00:03, 227.10it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  72%|██████████████████████████████████████████████████████████████████████████▎                             | 1855/2594 [00:08<00:03, 230.71it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  74%|████████████████████████████████████████████████████████████████████████████▍                           | 1908/2594 [00:08<00:02, 234.12it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  76%|██████████████████████████████████████████████████████████████████████████████▌                         | 1961/2594 [00:08<00:02, 237.50it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  78%|████████████████████████████████████████████████████████████████████████████████▋                       | 2014/2594 [00:08<00:02, 240.83it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  80%|██████████████████████████████████████████████████████████████████████████████████▊                     | 2067/2594 [00:08<00:02, 244.10it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 2120/2594 [00:08<00:01, 247.29it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  84%|███████████████████████████████████████████████████████████████████████████████████████                 | 2173/2594 [00:08<00:01, 250.40it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  86%|█████████████████████████████████████████████████████████████████████████████████████████▏              | 2226/2594 [00:08<00:01, 253.41it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  88%|███████████████████████████████████████████████████████████████████████████████████████████▎            | 2279/2594 [00:08<00:01, 256.33it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 259.20it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 261.95it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 264.72it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 267.42it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 270.01it/s, loss=1.1e-06, v_num=32, train_loss=0.000603, test_loss=0.000614]\u001b[A\n",
      "Epoch 38: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.20it/s, loss=1.1e-06, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 194.66it/s, loss=9.86e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[AAdjusting learning rate of group 0 to 9.0472e-03.\n",
      "Epoch 39:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 192.13it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 39:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 188.33it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 192.89it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 197.34it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 201.61it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 205.84it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 209.95it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 213.97it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:  27%|████████████████████████████████████████████▉                                                                                                                           | 347/1297 [00:00<00:02, 468.53it/s]\u001b[A\n",
      "Epoch 39:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 217.86it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 221.67it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 225.41it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.04it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 232.52it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 235.91it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 239.27it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 242.51it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 245.63it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 248.76it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 251.74it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 254.62it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 257.46it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 260.26it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 263.10it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 265.84it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 268.51it/s, loss=9.81e-07, v_num=32, train_loss=0.000826, test_loss=0.000839]\u001b[A\n",
      "Epoch 39: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.54it/s, loss=9.81e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 194.41it/s, loss=7.65e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[AAdjusting learning rate of group 0 to 9.0246e-03.\n",
      "Epoch 40:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 192.75it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 40:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:07<00:06, 188.89it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 193.42it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 197.80it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 202.16it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 206.36it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Validating:  19%|███████████████████████████████▍                                                                                                                                        | 243/1297 [00:00<00:02, 431.82it/s]\u001b[A\n",
      "Epoch 40:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 210.34it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 214.34it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 218.22it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 222.00it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 225.65it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 229.19it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 232.55it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 235.98it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 239.27it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 242.45it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 245.53it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 248.61it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 251.60it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 254.58it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 257.44it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 260.26it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 263.03it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 265.61it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 268.22it/s, loss=7.51e-07, v_num=32, train_loss=0.00115, test_loss=0.00118]\u001b[A\n",
      "Epoch 40: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.48it/s, loss=7.51e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 195.72it/s, loss=8.1e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[AAdjusting learning rate of group 0 to 9.0021e-03.\n",
      "Epoch 41:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.81it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 41:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 189.83it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.48it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.96it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 203.16it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 207.33it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 211.34it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 215.37it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▊                                                                                                                           | 346/1297 [00:00<00:02, 462.32it/s]\u001b[A\n",
      "Epoch 41:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 219.10it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.85it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.51it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 230.12it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.59it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.95it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 240.29it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.48it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.61it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.71it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.68it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.60it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.52it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.26it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 263.97it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.67it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 269.27it/s, loss=7.09e-07, v_num=32, train_loss=0.000753, test_loss=0.000753]\u001b[A\n",
      "Epoch 41: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.34it/s, loss=7.09e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.85it/s, loss=9.52e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[AAdjusting learning rate of group 0 to 8.9796e-03.\n",
      "Epoch 42:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 194.27it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 42:  51%|█████████████████████████████████████████████████████                                                   | 1325/2594 [00:06<00:06, 190.51it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  53%|███████████████████████████████████████████████████████▏                                                | 1378/2594 [00:07<00:06, 195.08it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  55%|█████████████████████████████████████████████████████████▎                                              | 1431/2594 [00:07<00:05, 199.60it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  57%|███████████████████████████████████████████████████████████▍                                            | 1484/2594 [00:07<00:05, 204.03it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  59%|█████████████████████████████████████████████████████████████▌                                          | 1537/2594 [00:07<00:05, 208.30it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  61%|███████████████████████████████████████████████████████████████▋                                        | 1590/2594 [00:07<00:04, 212.38it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  63%|█████████████████████████████████████████████████████████████████▊                                      | 1643/2594 [00:07<00:04, 216.35it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  65%|███████████████████████████████████████████████████████████████████▉                                    | 1696/2594 [00:07<00:04, 220.28it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  67%|██████████████████████████████████████████████████████████████████████                                  | 1749/2594 [00:07<00:03, 224.04it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Validating:  35%|██████████████████████████████████████████████████████████▌                                                                                                             | 452/1297 [00:01<00:01, 484.31it/s]\u001b[A\n",
      "Epoch 42:  69%|████████████████████████████████████████████████████████████████████████▏                               | 1802/2594 [00:07<00:03, 227.71it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  72%|██████████████████████████████████████████████████████████████████████████▎                             | 1855/2594 [00:08<00:03, 231.34it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  74%|████████████████████████████████████████████████████████████████████████████▍                           | 1908/2594 [00:08<00:02, 234.79it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  76%|██████████████████████████████████████████████████████████████████████████████▌                         | 1961/2594 [00:08<00:02, 238.20it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  78%|████████████████████████████████████████████████████████████████████████████████▋                       | 2014/2594 [00:08<00:02, 241.55it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  80%|██████████████████████████████████████████████████████████████████████████████████▊                     | 2067/2594 [00:08<00:02, 244.80it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 2120/2594 [00:08<00:01, 247.97it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  84%|███████████████████████████████████████████████████████████████████████████████████████                 | 2173/2594 [00:08<00:01, 251.12it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  86%|█████████████████████████████████████████████████████████████████████████████████████████▏              | 2226/2594 [00:08<00:01, 254.08it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  88%|███████████████████████████████████████████████████████████████████████████████████████████▎            | 2279/2594 [00:08<00:01, 257.00it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:08<00:01, 259.83it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 262.63it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 265.32it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 268.03it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 270.50it/s, loss=8.5e-07, v_num=32, train_loss=0.000887, test_loss=0.000909]\u001b[A\n",
      "Epoch 42: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.51it/s, loss=8.5e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.66it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[AAdjusting learning rate of group 0 to 8.9571e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.37it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 43:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:06<00:06, 189.73it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 194.27it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 198.52it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 202.74it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Validating:  15%|████████████████████████▍                                                                                                                                               | 189/1297 [00:00<00:02, 391.61it/s]\u001b[A\n",
      "Epoch 43:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 206.85it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 210.93it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 214.82it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 218.58it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 222.26it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 225.87it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 229.46it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 232.91it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 236.14it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 239.21it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 242.47it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 245.60it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 248.69it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 251.66it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Validating:  72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                               | 930/1297 [00:02<00:00, 491.02it/s]\u001b[A\n",
      "Epoch 43:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 254.54it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 257.39it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 260.16it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 262.87it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 265.53it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 268.18it/s, loss=4.06e-07, v_num=32, train_loss=0.00105, test_loss=0.00106]\u001b[A\n",
      "Epoch 43: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.10it/s, loss=4.06e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 194.84it/s, loss=5.63e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[AAdjusting learning rate of group 0 to 8.9347e-03.\n",
      "Epoch 44:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 193.20it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 44:  51%|█████████████████████████████████████████████████████                                                   | 1325/2594 [00:07<00:06, 189.37it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  53%|███████████████████████████████████████████████████████▏                                                | 1378/2594 [00:07<00:06, 194.04it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  55%|█████████████████████████████████████████████████████████▎                                              | 1431/2594 [00:07<00:05, 198.51it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44:  57%|███████████████████████████████████████████████████████████▍                                            | 1484/2594 [00:07<00:05, 202.75it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  59%|█████████████████████████████████████████████████████████████▌                                          | 1537/2594 [00:07<00:05, 206.94it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  61%|███████████████████████████████████████████████████████████████▋                                        | 1590/2594 [00:07<00:04, 211.11it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  63%|█████████████████████████████████████████████████████████████████▊                                      | 1643/2594 [00:07<00:04, 215.14it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  65%|███████████████████████████████████████████████████████████████████▉                                    | 1696/2594 [00:07<00:04, 219.02it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Validating:  31%|███████████████████████████████████████████████████▋                                                                                                                    | 399/1297 [00:01<00:01, 477.35it/s]\u001b[A\n",
      "Epoch 44:  67%|██████████████████████████████████████████████████████████████████████                                  | 1749/2594 [00:07<00:03, 222.59it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  69%|████████████████████████████████████████████████████████████████████████▏                               | 1802/2594 [00:07<00:03, 226.27it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  72%|██████████████████████████████████████████████████████████████████████████▎                             | 1855/2594 [00:08<00:03, 229.87it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  74%|████████████████████████████████████████████████████████████████████████████▍                           | 1908/2594 [00:08<00:02, 233.40it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  76%|██████████████████████████████████████████████████████████████████████████████▌                         | 1961/2594 [00:08<00:02, 236.83it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  78%|████████████████████████████████████████████████████████████████████████████████▋                       | 2014/2594 [00:08<00:02, 240.13it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  80%|██████████████████████████████████████████████████████████████████████████████████▊                     | 2067/2594 [00:08<00:02, 243.47it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 2120/2594 [00:08<00:01, 246.63it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  84%|███████████████████████████████████████████████████████████████████████████████████████                 | 2173/2594 [00:08<00:01, 249.68it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  86%|█████████████████████████████████████████████████████████████████████████████████████████▏              | 2226/2594 [00:08<00:01, 252.68it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  88%|███████████████████████████████████████████████████████████████████████████████████████████▎            | 2279/2594 [00:08<00:01, 255.57it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 258.49it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 261.26it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 263.93it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.56it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.11it/s, loss=5.56e-07, v_num=32, train_loss=0.00053, test_loss=0.000537]\u001b[A\n",
      "Epoch 44: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.33it/s, loss=5.56e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 196.01it/s, loss=5.71e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[AAdjusting learning rate of group 0 to 8.9124e-03.\n",
      "Epoch 45:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 194.19it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 45:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 190.29it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.87it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 199.23it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 203.61it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 207.80it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 211.91it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Validating:  23%|█████████████████████████████████████▉                                                                                                                                  | 293/1297 [00:00<00:02, 451.52it/s]\u001b[A\n",
      "Epoch 45:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 215.84it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 219.73it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 223.56it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 227.23it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 230.82it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 234.36it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 237.69it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 241.01it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 244.32it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 247.53it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 250.62it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 253.67it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 256.61it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:08<00:01, 259.45it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 262.29it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.92it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 267.69it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 270.25it/s, loss=5.62e-07, v_num=32, train_loss=0.000718, test_loss=0.000721]\u001b[A\n",
      "Epoch 45: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.30it/s, loss=5.62e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.10it/s, loss=8.37e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[AAdjusting learning rate of group 0 to 8.8901e-03.\n",
      "Epoch 46:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.42it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 46:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 189.64it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 193.94it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.46it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.71it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Validating:  14%|████████████████████████▏                                                                                                                                               | 187/1297 [00:00<00:02, 396.19it/s]\u001b[A\n",
      "Epoch 46:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.80it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.94it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 214.96it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.85it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.69it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.35it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.87it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.40it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.80it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 240.03it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.24it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.39it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.44it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.43it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.36it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.19it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.02it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 263.72it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Epoch 46:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.29it/s, loss=8.71e-07, v_num=32, train_loss=0.000547, test_loss=0.000559]\u001b[A\n",
      "Validating:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 1195/1297 [00:02<00:00, 491.47it/s]\u001b[A\n",
      "Epoch 46: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.88it/s, loss=8.71e-07, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 196.01it/s, loss=1.07e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[AAdjusting learning rate of group 0 to 8.8679e-03.\n",
      "Epoch 47:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 194.31it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 47:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:06<00:06, 190.52it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 195.11it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 199.61it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 203.95it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 208.14it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 212.27it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 216.26it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▉                                                                                                                           | 347/1297 [00:00<00:02, 468.04it/s]\u001b[A\n",
      "Epoch 47:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 220.16it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 223.95it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 227.70it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 231.26it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 234.76it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 238.10it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 241.34it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 244.58it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 247.76it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 250.83it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 253.89it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 256.78it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:08<00:01, 259.67it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 262.51it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 265.28it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 267.95it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 270.55it/s, loss=1.05e-06, v_num=32, train_loss=0.00127, test_loss=0.00128]\u001b[A\n",
      "Epoch 47: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.45it/s, loss=1.05e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.02it/s, loss=1.25e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[AAdjusting learning rate of group 0 to 8.8457e-03.\n",
      "Epoch 48:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.35it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 48:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:06<00:06, 189.47it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 194.12it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 198.57it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 202.94it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 207.16it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 211.27it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 215.33it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 219.22it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Validating:  31%|███████████████████████████████████████████████████▋                                                                                                                    | 399/1297 [00:01<00:01, 479.41it/s]\u001b[A\n",
      "Epoch 48:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 222.94it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 226.64it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 230.20it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 233.58it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 236.92it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 240.20it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 243.48it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 246.68it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 249.74it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 252.68it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 255.60it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 258.46it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 261.28it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 263.97it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.55it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.09it/s, loss=1.16e-06, v_num=32, train_loss=0.00143, test_loss=0.00143]\u001b[A\n",
      "Epoch 48: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.16it/s, loss=1.16e-06, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  50%|████████████████████████████████████████████████████▉                                                     | 1296/2594 [00:06<00:06, 195.10it/s, loss=3.79e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[AAdjusting learning rate of group 0 to 8.8236e-03.\n",
      "Epoch 49:  50%|█████████████████████████████████████████████████████                                                     | 1297/2594 [00:06<00:06, 193.02it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49:  51%|██████████████████████████████████████████████████████▏                                                   | 1325/2594 [00:07<00:06, 189.23it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49:  53%|████████████████████████████████████████████████████████▎                                                 | 1378/2594 [00:07<00:06, 193.79it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  55%|██████████████████████████████████████████████████████████▍                                               | 1431/2594 [00:07<00:05, 198.26it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  57%|████████████████████████████████████████████████████████████▋                                             | 1484/2594 [00:07<00:05, 202.69it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  59%|██████████████████████████████████████████████████████████████▊                                           | 1537/2594 [00:07<00:05, 206.89it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  61%|████████████████████████████████████████████████████████████████▉                                         | 1590/2594 [00:07<00:04, 210.96it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  63%|███████████████████████████████████████████████████████████████████▏                                      | 1643/2594 [00:07<00:04, 215.02it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▉                                                                                                                           | 347/1297 [00:00<00:02, 471.03it/s]\u001b[A\n",
      "Epoch 49:  65%|█████████████████████████████████████████████████████████████████████▎                                    | 1696/2594 [00:07<00:04, 218.88it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  67%|███████████████████████████████████████████████████████████████████████▍                                  | 1749/2594 [00:07<00:03, 222.69it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  69%|█████████████████████████████████████████████████████████████████████████▋                                | 1802/2594 [00:07<00:03, 226.40it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  72%|███████████████████████████████████████████████████████████████████████████▊                              | 1855/2594 [00:08<00:03, 229.96it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  74%|█████████████████████████████████████████████████████████████████████████████▉                            | 1908/2594 [00:08<00:02, 233.46it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  76%|████████████████████████████████████████████████████████████████████████████████▏                         | 1961/2594 [00:08<00:02, 236.93it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  78%|██████████████████████████████████████████████████████████████████████████████████▎                       | 2014/2594 [00:08<00:02, 240.26it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  80%|████████████████████████████████████████████████████████████████████████████████████▍                     | 2067/2594 [00:08<00:02, 243.59it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  82%|██████████████████████████████████████████████████████████████████████████████████████▋                   | 2120/2594 [00:08<00:01, 246.79it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  84%|████████████████████████████████████████████████████████████████████████████████████████▊                 | 2173/2594 [00:08<00:01, 249.85it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  86%|██████████████████████████████████████████████████████████████████████████████████████████▉               | 2226/2594 [00:08<00:01, 252.83it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  88%|█████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 255.79it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  90%|███████████████████████████████████████████████████████████████████████████████████████████████▎          | 2332/2594 [00:09<00:01, 258.66it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████▍        | 2385/2594 [00:09<00:00, 261.46it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 264.17it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.82it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.46it/s, loss=4.03e-07, v_num=32, train_loss=0.00169, test_loss=0.0017]\u001b[A\n",
      "Epoch 49: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.35it/s, loss=4.03e-07, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.52it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[AAdjusting learning rate of group 0 to 8.8015e-03.\n",
      "Epoch 50:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.10it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 50:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 189.43it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.00it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.52it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.83it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 207.05it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 211.16it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 215.18it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▊                                                                                                                           | 346/1297 [00:00<00:02, 469.39it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 219.02it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.88it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.59it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 230.18it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.71it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 237.07it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 240.33it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.58it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.76it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.84it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.90it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.81it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.64it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.43it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.13it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.81it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 269.43it/s, loss=1.91e-06, v_num=32, train_loss=0.000555, test_loss=0.000566]\u001b[A\n",
      "Epoch 50: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.28it/s, loss=1.91e-06, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.89it/s, loss=4.43e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[AAdjusting learning rate of group 0 to 8.7795e-03.\n",
      "Epoch 51:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 194.20it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 51:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:06<00:06, 190.03it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 194.49it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 198.85it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 203.23it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 207.48it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 211.61it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Validating:  23%|█████████████████████████████████████▉                                                                                                                                  | 293/1297 [00:00<00:02, 452.53it/s]\u001b[A\n",
      "Epoch 51:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 215.64it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 219.54it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 223.26it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 226.93it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 230.55it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 234.05it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 237.52it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 240.86it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 244.12it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 247.37it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 250.42it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 253.45it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 256.42it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:08<00:01, 259.29it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 262.09it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 264.81it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 267.44it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.99it/s, loss=4.29e-07, v_num=32, train_loss=0.00135, test_loss=0.00136]\u001b[A\n",
      "Epoch 51: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.02it/s, loss=4.29e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 194.85it/s, loss=4.42e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[AAdjusting learning rate of group 0 to 8.7576e-03.\n",
      "Epoch 52:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 192.32it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 52:  51%|█████████████████████████████████████████████████████                                                   | 1325/2594 [00:07<00:06, 188.61it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  53%|███████████████████████████████████████████████████████▏                                                | 1378/2594 [00:07<00:06, 193.10it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  55%|█████████████████████████████████████████████████████████▎                                              | 1431/2594 [00:07<00:05, 197.57it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  57%|███████████████████████████████████████████████████████████▍                                            | 1484/2594 [00:07<00:05, 201.82it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Validating:  15%|████████████████████████▍                                                                                                                                               | 189/1297 [00:00<00:02, 396.51it/s]\u001b[A\n",
      "Epoch 52:  59%|█████████████████████████████████████████████████████████████▌                                          | 1537/2594 [00:07<00:05, 205.93it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  61%|███████████████████████████████████████████████████████████████▋                                        | 1590/2594 [00:07<00:04, 209.97it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  63%|█████████████████████████████████████████████████████████████████▊                                      | 1643/2594 [00:07<00:04, 213.93it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  65%|███████████████████████████████████████████████████████████████████▉                                    | 1696/2594 [00:07<00:04, 217.79it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  67%|██████████████████████████████████████████████████████████████████████                                  | 1749/2594 [00:07<00:03, 221.56it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  69%|████████████████████████████████████████████████████████████████████████▏                               | 1802/2594 [00:08<00:03, 225.30it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  72%|██████████████████████████████████████████████████████████████████████████▎                             | 1855/2594 [00:08<00:03, 228.83it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  74%|████████████████████████████████████████████████████████████████████████████▍                           | 1908/2594 [00:08<00:02, 232.32it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  76%|██████████████████████████████████████████████████████████████████████████████▌                         | 1961/2594 [00:08<00:02, 235.72it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  78%|████████████████████████████████████████████████████████████████████████████████▋                       | 2014/2594 [00:08<00:02, 239.01it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  80%|██████████████████████████████████████████████████████████████████████████████████▊                     | 2067/2594 [00:08<00:02, 242.26it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 2120/2594 [00:08<00:01, 245.43it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  84%|███████████████████████████████████████████████████████████████████████████████████████                 | 2173/2594 [00:08<00:01, 248.47it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  86%|█████████████████████████████████████████████████████████████████████████████████████████▏              | 2226/2594 [00:08<00:01, 251.48it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  88%|███████████████████████████████████████████████████████████████████████████████████████████▎            | 2279/2594 [00:08<00:01, 254.41it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 257.27it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 260.05it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 262.81it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Epoch 52:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 265.52it/s, loss=4.31e-07, v_num=32, train_loss=0.00049, test_loss=0.000499]\u001b[A\n",
      "Validating:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 1195/1297 [00:02<00:00, 500.21it/s]\u001b[A\n",
      "Epoch 52: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.51it/s, loss=4.31e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.32it/s, loss=3.29e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[AAdjusting learning rate of group 0 to 8.7357e-03.\n",
      "Epoch 53:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.59it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 53:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 189.57it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.15it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.64it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 203.01it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 207.27it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 211.31it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Validating:  23%|██████████████████████████████████████                                                                                                                                  | 294/1297 [00:00<00:02, 451.31it/s]\u001b[A\n",
      "Epoch 53:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 215.24it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 219.13it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.95it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.66it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 230.23it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.68it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 237.08it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 240.38it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.66it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.79it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.87it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.81it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.76it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.61it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.36it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.18it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.88it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 269.54it/s, loss=3.55e-07, v_num=32, train_loss=0.000567, test_loss=0.000574]\u001b[A\n",
      "Epoch 53: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.82it/s, loss=3.55e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 196.25it/s, loss=5.18e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[AAdjusting learning rate of group 0 to 8.7138e-03.\n",
      "Epoch 54:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 194.56it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 54:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 190.75it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 195.33it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 199.80it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 204.15it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 208.38it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 212.52it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 216.51it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▉                                                                                                                           | 347/1297 [00:00<00:02, 468.00it/s]\u001b[A\n",
      "Epoch 54:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 220.23it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 224.02it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 227.70it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 231.16it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 234.56it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 237.96it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 241.26it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 244.49it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 247.63it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 250.66it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 253.61it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 256.60it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:08<00:01, 259.45it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 262.17it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.94it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 267.57it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 270.22it/s, loss=5.14e-07, v_num=32, train_loss=0.000895, test_loss=0.000915]\u001b[A\n",
      "Epoch 54: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.12it/s, loss=5.14e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 194.45it/s, loss=3.97e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[AAdjusting learning rate of group 0 to 8.6921e-03.\n",
      "Epoch 55:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 192.80it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 55:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 188.89it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 193.52it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 197.98it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.34it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.58it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.65it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 214.61it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▉                                                                                                                           | 347/1297 [00:00<00:02, 465.63it/s]\u001b[A\n",
      "Epoch 55:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.51it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.36it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.06it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.63it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.13it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.56it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 239.91it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.10it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.20it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.31it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.29it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.29it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.12it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 260.92it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 263.72it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.42it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 268.92it/s, loss=4.13e-07, v_num=32, train_loss=0.000708, test_loss=0.000716]\u001b[A\n",
      "Epoch 55: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.94it/s, loss=4.13e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.21it/s, loss=6.94e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[AAdjusting learning rate of group 0 to 8.6703e-03.\n",
      "Epoch 56:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.14it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 56:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 189.38it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 193.96it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.40it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.70it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.92it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.97it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Validating:  23%|██████████████████████████████████████                                                                                                                                  | 294/1297 [00:00<00:02, 450.42it/s]\u001b[A\n",
      "Epoch 56:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 214.97it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.91it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.68it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.35it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.97it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.49it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.87it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 240.20it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.45it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.61it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.78it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.76it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.61it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.52it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.31it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.07it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.81it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 269.45it/s, loss=6.71e-07, v_num=32, train_loss=0.000629, test_loss=0.000638]\u001b[A\n",
      "Epoch 56: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.51it/s, loss=6.71e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 194.92it/s, loss=6.66e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[AAdjusting learning rate of group 0 to 8.6487e-03.\n",
      "Epoch 57:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.25it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 57:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 189.18it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 193.81it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.29it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.64it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.89it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.98it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 214.97it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.92it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Validating:  31%|███████████████████████████████████████████████████▋                                                                                                                    | 399/1297 [00:01<00:01, 480.29it/s]\u001b[A\n",
      "Epoch 57:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.73it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.41it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 230.04it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.56it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.99it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 240.37it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.62it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.82it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.91it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.88it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.84it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.77it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.55it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.32it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.98it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 269.62it/s, loss=6.33e-07, v_num=32, train_loss=0.000695, test_loss=0.000706]\u001b[A\n",
      "Epoch 57: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.68it/s, loss=6.33e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 196.00it/s, loss=4.37e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[AAdjusting learning rate of group 0 to 8.6270e-03.\n",
      "Epoch 58:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.40it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 58:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 189.75it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.33it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.82it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 203.13it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 207.36it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 211.48it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 215.43it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▊                                                                                                                           | 346/1297 [00:00<00:02, 466.25it/s]\u001b[A\n",
      "Epoch 58:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 219.33it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 223.03it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.68it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 230.36it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.84it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 237.26it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 240.66it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.90it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 247.06it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 250.14it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 253.18it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 256.02it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.98it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.77it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.48it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 267.18it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 269.80it/s, loss=4.34e-07, v_num=32, train_loss=0.000829, test_loss=0.000842]\u001b[A\n",
      "Epoch 58: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.82it/s, loss=4.34e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.35it/s, loss=3.53e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[AAdjusting learning rate of group 0 to 8.6055e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.67it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 59:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 189.56it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.13it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.68it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 203.06it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 207.31it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 211.45it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 215.45it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▉                                                                                                                           | 347/1297 [00:00<00:02, 469.36it/s]\u001b[A\n",
      "Epoch 59:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 219.26it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 223.08it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.75it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 230.36it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.81it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 237.22it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 240.61it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.84it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.99it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 250.11it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 253.10it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 256.01it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.91it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.69it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.46it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 267.19it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 269.75it/s, loss=3.42e-07, v_num=32, train_loss=0.000575, test_loss=0.000599]\u001b[A\n",
      "Epoch 59: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.14it/s, loss=3.42e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 195.65it/s, loss=7.5e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[AAdjusting learning rate of group 0 to 8.5839e-03.\n",
      "Epoch 60:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.07it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 60:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 189.42it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.04it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.48it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.73it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.90it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.96it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Validating:  23%|██████████████████████████████████████▏                                                                                                                                 | 295/1297 [00:00<00:02, 448.53it/s]\u001b[A\n",
      "Epoch 60:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 214.87it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.77it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.58it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.22it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.81it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.30it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.69it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 239.95it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.23it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.39it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.52it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.53it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.45it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.40it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.26it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.02it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.74it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 269.42it/s, loss=7.17e-07, v_num=32, train_loss=0.000436, test_loss=0.000442]\u001b[A\n",
      "Epoch 60: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:10<00:00, 257.00it/s, loss=7.17e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  50%|████████████████████████████████████████████████████▉                                                     | 1296/2594 [00:07<00:07, 179.40it/s, loss=9.8e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[AAdjusting learning rate of group 0 to 8.5625e-03.\n",
      "Epoch 61:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:07<00:07, 177.57it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 61:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:07<00:07, 174.74it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 179.16it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:06, 183.50it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:08<00:06, 181.60it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Validating:  14%|████████████████████████▎                                                                                                                                               | 188/1297 [00:00<00:05, 209.86it/s]\u001b[A\n",
      "Epoch 61:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:08<00:05, 185.62it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:08<00:05, 181.23it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:08<00:05, 185.00it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:08<00:04, 188.71it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:09<00:04, 192.29it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:09<00:04, 195.82it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:09<00:03, 199.25it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:09<00:03, 202.67it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:09<00:03, 205.99it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:09<00:02, 209.20it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:09<00:02, 212.39it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:09<00:02, 215.49it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:09<00:01, 218.48it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Validating:  68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                      | 877/1297 [00:02<00:00, 485.72it/s]\u001b[A\n",
      "Epoch 61:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:10<00:01, 221.45it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:10<00:01, 219.29it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:10<00:01, 222.10it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:10<00:00, 224.89it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:10<00:00, 227.59it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:10<00:00, 230.26it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:10<00:00, 232.80it/s, loss=9.45e-07, v_num=32, train_loss=0.00182, test_loss=0.00183]\u001b[A\n",
      "Epoch 61: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:11<00:00, 233.95it/s, loss=9.45e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 194.45it/s, loss=4.07e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[AAdjusting learning rate of group 0 to 8.5411e-03.\n",
      "Epoch 62:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 192.86it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 62:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:07<00:06, 188.85it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 193.39it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 197.79it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 202.04it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 206.21it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 210.26it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Validating:  23%|█████████████████████████████████████▉                                                                                                                                  | 293/1297 [00:00<00:02, 444.68it/s]\u001b[A\n",
      "Epoch 62:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 214.19it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 218.11it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 221.96it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 225.63it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 229.26it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 232.83it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 236.27it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 239.57it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 242.89it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 246.04it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 249.15it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 252.22it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 255.09it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 258.00it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 260.82it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 263.58it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.30it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.02it/s, loss=4.25e-07, v_num=32, train_loss=0.00117, test_loss=0.00118]\u001b[A\n",
      "Epoch 62: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.11it/s, loss=4.25e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 194.83it/s, loss=3.02e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[AAdjusting learning rate of group 0 to 8.5197e-03.\n",
      "Epoch 63:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.15it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 63:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:07<00:06, 189.16it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 193.67it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 198.09it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 202.36it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 206.60it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 210.77it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 214.82it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▊                                                                                                                           | 346/1297 [00:00<00:02, 471.70it/s]\u001b[A\n",
      "Epoch 63:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 218.61it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 222.39it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 226.09it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 229.70it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 233.25it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 236.64it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 239.98it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 243.27it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 246.44it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 249.45it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 252.45it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 255.35it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 258.21it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 261.02it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 263.73it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.37it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.03it/s, loss=3.04e-07, v_num=32, train_loss=0.00119, test_loss=0.00121]\u001b[A\n",
      "Epoch 63: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.07it/s, loss=3.04e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 194.78it/s, loss=5.71e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[AAdjusting learning rate of group 0 to 8.4984e-03.\n",
      "Epoch 64:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.10it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 64:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 189.31it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 193.84it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.32it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.76it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 207.05it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 211.20it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 215.28it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▉                                                                                                                           | 347/1297 [00:00<00:01, 475.34it/s]\u001b[A\n",
      "Epoch 64:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.95it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.74it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.42it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.97it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.48it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.86it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 240.20it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.50it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.66it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.74it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.75it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.69it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.62it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.52it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.25it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.89it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 269.54it/s, loss=5.52e-07, v_num=32, train_loss=0.000589, test_loss=0.000598]\u001b[A\n",
      "Epoch 64: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.56it/s, loss=5.52e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.61it/s, loss=5.61e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[AAdjusting learning rate of group 0 to 8.4772e-03.\n",
      "Epoch 65:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.94it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 65:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 190.11it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.66it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 199.09it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 203.45it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 207.71it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 211.84it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Validating:  23%|█████████████████████████████████████▉                                                                                                                                  | 293/1297 [00:00<00:02, 455.66it/s]\u001b[A\n",
      "Epoch 65:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 215.81it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 219.66it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 223.35it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.99it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 230.59it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 234.07it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 237.48it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 240.65it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.92it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 247.10it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 250.19it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 253.15it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.99it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.91it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.76it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.54it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 267.19it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 269.81it/s, loss=5.46e-07, v_num=32, train_loss=0.000454, test_loss=0.000462]\u001b[A\n",
      "Epoch 65: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.80it/s, loss=5.46e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.00it/s, loss=3.84e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[AAdjusting learning rate of group 0 to 8.4560e-03.\n",
      "Epoch 66:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 192.46it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 66:  51%|█████████████████████████████████████████████████████                                                   | 1325/2594 [00:07<00:06, 188.83it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  53%|███████████████████████████████████████████████████████▏                                                | 1378/2594 [00:07<00:06, 193.42it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  55%|█████████████████████████████████████████████████████████▎                                              | 1431/2594 [00:07<00:05, 197.83it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  57%|███████████████████████████████████████████████████████████▍                                            | 1484/2594 [00:07<00:05, 202.16it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  59%|█████████████████████████████████████████████████████████████▌                                          | 1537/2594 [00:07<00:05, 206.34it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Validating:  19%|███████████████████████████████▎                                                                                                                                        | 242/1297 [00:00<00:02, 431.81it/s]\u001b[A\n",
      "Epoch 66:  61%|███████████████████████████████████████████████████████████████▋                                        | 1590/2594 [00:07<00:04, 210.33it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  63%|█████████████████████████████████████████████████████████████████▊                                      | 1643/2594 [00:07<00:04, 214.24it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66:  65%|███████████████████████████████████████████████████████████████████▉                                    | 1696/2594 [00:07<00:04, 218.06it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  67%|██████████████████████████████████████████████████████████████████████                                  | 1749/2594 [00:07<00:03, 221.81it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  69%|████████████████████████████████████████████████████████████████████████▏                               | 1802/2594 [00:07<00:03, 225.52it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  72%|██████████████████████████████████████████████████████████████████████████▎                             | 1855/2594 [00:08<00:03, 229.08it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  74%|████████████████████████████████████████████████████████████████████████████▍                           | 1908/2594 [00:08<00:02, 232.60it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  76%|██████████████████████████████████████████████████████████████████████████████▌                         | 1961/2594 [00:08<00:02, 236.07it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  78%|████████████████████████████████████████████████████████████████████████████████▋                       | 2014/2594 [00:08<00:02, 239.42it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  80%|██████████████████████████████████████████████████████████████████████████████████▊                     | 2067/2594 [00:08<00:02, 242.66it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 2120/2594 [00:08<00:01, 245.85it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  84%|███████████████████████████████████████████████████████████████████████████████████████                 | 2173/2594 [00:08<00:01, 248.87it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  86%|█████████████████████████████████████████████████████████████████████████████████████████▏              | 2226/2594 [00:08<00:01, 251.80it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  88%|███████████████████████████████████████████████████████████████████████████████████████████▎            | 2279/2594 [00:08<00:01, 254.81it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 257.67it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 260.50it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 263.18it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 265.85it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 268.52it/s, loss=3.7e-07, v_num=32, train_loss=0.000846, test_loss=0.000853]\u001b[A\n",
      "Epoch 66: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.60it/s, loss=3.7e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.81it/s, loss=4.99e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[AAdjusting learning rate of group 0 to 8.4349e-03.\n",
      "Epoch 67:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 194.10it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 67:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 190.31it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.85it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 199.32it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 203.70it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 207.95it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 212.03it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 216.04it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▉                                                                                                                           | 347/1297 [00:00<00:02, 467.48it/s]\u001b[A\n",
      "Epoch 67:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 219.87it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 223.64it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 227.39it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 231.03it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 234.50it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 237.96it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 241.31it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 244.40it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 247.60it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 250.64it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 253.74it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 256.70it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:08<00:01, 259.64it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 262.44it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 265.22it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 267.89it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 270.51it/s, loss=4.91e-07, v_num=32, train_loss=0.000426, test_loss=0.000431]\u001b[A\n",
      "Epoch 67: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.58it/s, loss=4.91e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 195.18it/s, loss=5.55e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[AAdjusting learning rate of group 0 to 8.4138e-03.\n",
      "Epoch 68:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 192.62it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 68:  51%|█████████████████████████████████████████████████████                                                   | 1325/2594 [00:07<00:06, 188.97it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  53%|███████████████████████████████████████████████████████▏                                                | 1378/2594 [00:07<00:06, 193.51it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  55%|█████████████████████████████████████████████████████████▎                                              | 1431/2594 [00:07<00:05, 197.92it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  57%|███████████████████████████████████████████████████████████▍                                            | 1484/2594 [00:07<00:05, 202.34it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  59%|█████████████████████████████████████████████████████████████▌                                          | 1537/2594 [00:07<00:05, 206.55it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  61%|███████████████████████████████████████████████████████████████▋                                        | 1590/2594 [00:07<00:04, 210.77it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  63%|█████████████████████████████████████████████████████████████████▊                                      | 1643/2594 [00:07<00:04, 214.85it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  65%|███████████████████████████████████████████████████████████████████▉                                    | 1696/2594 [00:07<00:04, 218.74it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Validating:  31%|███████████████████████████████████████████████████▋                                                                                                                    | 399/1297 [00:01<00:01, 485.53it/s]\u001b[A\n",
      "Epoch 68:  67%|██████████████████████████████████████████████████████████████████████                                  | 1749/2594 [00:07<00:03, 222.57it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  69%|████████████████████████████████████████████████████████████████████████▏                               | 1802/2594 [00:07<00:03, 226.29it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  72%|██████████████████████████████████████████████████████████████████████████▎                             | 1855/2594 [00:08<00:03, 229.82it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  74%|████████████████████████████████████████████████████████████████████████████▍                           | 1908/2594 [00:08<00:02, 233.27it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  76%|██████████████████████████████████████████████████████████████████████████████▌                         | 1961/2594 [00:08<00:02, 236.65it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  78%|████████████████████████████████████████████████████████████████████████████████▋                       | 2014/2594 [00:08<00:02, 239.97it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  80%|██████████████████████████████████████████████████████████████████████████████████▊                     | 2067/2594 [00:08<00:02, 243.24it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 2120/2594 [00:08<00:01, 246.38it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  84%|███████████████████████████████████████████████████████████████████████████████████████                 | 2173/2594 [00:08<00:01, 249.45it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  86%|█████████████████████████████████████████████████████████████████████████████████████████▏              | 2226/2594 [00:08<00:01, 252.51it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  88%|███████████████████████████████████████████████████████████████████████████████████████████▎            | 2279/2594 [00:08<00:01, 255.54it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 258.45it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 261.22it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 263.94it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.64it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.20it/s, loss=5.21e-07, v_num=32, train_loss=0.000587, test_loss=0.00059]\u001b[A\n",
      "Epoch 68: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.25it/s, loss=5.21e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.33it/s, loss=3.52e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[AAdjusting learning rate of group 0 to 8.3927e-03.\n",
      "Epoch 69:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 192.59it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 69:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 188.92it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 193.54it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 197.99it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.26it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.49it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.58it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 214.62it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▊                                                                                                                           | 346/1297 [00:00<00:02, 466.32it/s]\u001b[A\n",
      "Epoch 69:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.49it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.30it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.03it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.61it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.08it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.58it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 239.93it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.21it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.42it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.47it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.49it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.46it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.33it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.16it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 263.99it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.70it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 269.32it/s, loss=3.42e-07, v_num=32, train_loss=0.000883, test_loss=0.000888]\u001b[A\n",
      "Epoch 69: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.69it/s, loss=3.42e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 194.56it/s, loss=4.29e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[AAdjusting learning rate of group 0 to 8.3717e-03.\n",
      "Epoch 70:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 192.90it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 70:  51%|█████████████████████████████████████████████████████                                                   | 1325/2594 [00:07<00:06, 189.32it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  53%|███████████████████████████████████████████████████████▏                                                | 1378/2594 [00:07<00:06, 193.96it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  55%|█████████████████████████████████████████████████████████▎                                              | 1431/2594 [00:07<00:05, 198.38it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  57%|███████████████████████████████████████████████████████████▍                                            | 1484/2594 [00:07<00:05, 202.75it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  59%|█████████████████████████████████████████████████████████████▌                                          | 1537/2594 [00:07<00:05, 206.96it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  61%|███████████████████████████████████████████████████████████████▋                                        | 1590/2594 [00:07<00:04, 211.10it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  63%|█████████████████████████████████████████████████████████████████▊                                      | 1643/2594 [00:07<00:04, 215.12it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Validating:  27%|█████████████████████████████████████████████                                                                                                                           | 348/1297 [00:00<00:02, 471.85it/s]\u001b[A\n",
      "Epoch 70:  65%|███████████████████████████████████████████████████████████████████▉                                    | 1696/2594 [00:07<00:04, 218.94it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  67%|██████████████████████████████████████████████████████████████████████                                  | 1749/2594 [00:07<00:03, 222.67it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  69%|████████████████████████████████████████████████████████████████████████▏                               | 1802/2594 [00:07<00:03, 226.31it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  72%|██████████████████████████████████████████████████████████████████████████▎                             | 1855/2594 [00:08<00:03, 229.76it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  74%|████████████████████████████████████████████████████████████████████████████▍                           | 1908/2594 [00:08<00:02, 233.21it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  76%|██████████████████████████████████████████████████████████████████████████████▌                         | 1961/2594 [00:08<00:02, 236.61it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  78%|████████████████████████████████████████████████████████████████████████████████▋                       | 2014/2594 [00:08<00:02, 239.90it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  80%|██████████████████████████████████████████████████████████████████████████████████▊                     | 2067/2594 [00:08<00:02, 243.09it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 2120/2594 [00:08<00:01, 246.20it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  84%|███████████████████████████████████████████████████████████████████████████████████████                 | 2173/2594 [00:08<00:01, 249.30it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  86%|█████████████████████████████████████████████████████████████████████████████████████████▏              | 2226/2594 [00:08<00:01, 252.29it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  88%|███████████████████████████████████████████████████████████████████████████████████████████▎            | 2279/2594 [00:08<00:01, 255.27it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 258.13it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 260.95it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 263.68it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.35it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 268.96it/s, loss=4.2e-07, v_num=32, train_loss=0.000404, test_loss=0.000412]\u001b[A\n",
      "Epoch 70: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.17it/s, loss=4.2e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.94it/s, loss=2.91e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[AAdjusting learning rate of group 0 to 8.3508e-03.\n",
      "Epoch 71:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 194.25it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 71:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 190.34it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.87it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 199.38it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 203.74it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 208.02it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Validating:  19%|███████████████████████████████▍                                                                                                                                        | 243/1297 [00:00<00:02, 435.80it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 212.00it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 216.00it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 219.87it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 223.66it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 227.24it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 230.82it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 234.25it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 237.65it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 241.00it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 244.26it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 247.38it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 250.48it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 253.40it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 256.35it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 259.18it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.95it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.77it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 267.47it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 270.14it/s, loss=2.94e-07, v_num=32, train_loss=0.000419, test_loss=0.000423]\u001b[A\n",
      "Epoch 71: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.45it/s, loss=2.94e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.47it/s, loss=5.19e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[AAdjusting learning rate of group 0 to 8.3299e-03.\n",
      "Epoch 72:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.42it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 72:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 189.59it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.07it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.41it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.74it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.95it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Validating:  19%|███████████████████████████████▎                                                                                                                                        | 242/1297 [00:00<00:02, 429.98it/s]\u001b[A\n",
      "Epoch 72:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.98it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 215.00it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.88it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.65it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.35it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.94it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.41it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.83it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 240.11it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.31it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.48it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.53it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.50it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.43it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.29it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.08it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 263.82it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.45it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 269.07it/s, loss=5.12e-07, v_num=32, train_loss=0.000554, test_loss=0.000562]\u001b[A\n",
      "Epoch 72: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.04it/s, loss=5.12e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.48it/s, loss=4.33e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[AAdjusting learning rate of group 0 to 8.3091e-03.\n",
      "Epoch 73:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.80it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 73:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:06<00:06, 190.05it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 194.66it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 199.11it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 203.48it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 207.74it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 211.80it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Validating:  23%|██████████████████████████████████████▎                                                                                                                                 | 296/1297 [00:00<00:02, 452.96it/s]\u001b[A\n",
      "Epoch 73:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 215.68it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 219.67it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 223.49it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 227.20it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 230.82it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 234.33it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 237.72it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 241.06it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 244.31it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 247.45it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 250.61it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 253.60it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 256.55it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:08<00:01, 259.42it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 262.22it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 264.98it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 267.68it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 270.30it/s, loss=4.41e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 73: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.40it/s, loss=4.41e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 194.50it/s, loss=7.38e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[AAdjusting learning rate of group 0 to 8.2883e-03.\n",
      "Epoch 74:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 192.81it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 74:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 189.00it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 193.58it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.02it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.36it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.50it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.52it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Validating:  23%|█████████████████████████████████████▉                                                                                                                                  | 293/1297 [00:00<00:02, 445.13it/s]\u001b[A\n",
      "Epoch 74:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 214.41it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.38it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.08it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 225.69it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.24it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 232.64it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 235.99it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 239.30it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 242.49it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 245.62it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 248.70it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 251.72it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 254.63it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 257.47it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 260.25it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 263.07it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Epoch 74:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 265.72it/s, loss=7.65e-07, v_num=32, train_loss=0.000383, test_loss=0.000389]\u001b[A\n",
      "Validating:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋             | 1194/1297 [00:02<00:00, 498.98it/s]\u001b[A\n",
      "Epoch 74: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.34it/s, loss=7.65e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.40it/s, loss=2.93e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[AAdjusting learning rate of group 0 to 8.2676e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.72it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 75:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:06<00:06, 190.04it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 194.64it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 199.12it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 203.45it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 207.60it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 211.75it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Validating:  23%|██████████████████████████████████████▎                                                                                                                                 | 296/1297 [00:00<00:02, 455.41it/s]\u001b[A\n",
      "Epoch 75:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 215.61it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 219.47it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 223.37it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 227.11it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 230.74it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 234.25it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 237.70it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 241.00it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 244.22it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 247.37it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 250.52it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 253.50it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 256.44it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:08<00:01, 259.34it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 262.14it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 264.88it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 267.58it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 270.16it/s, loss=2.83e-07, v_num=32, train_loss=0.00107, test_loss=0.00108]\u001b[A\n",
      "Epoch 75: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.18it/s, loss=2.83e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 194.45it/s, loss=4.05e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[AAdjusting learning rate of group 0 to 8.2470e-03.\n",
      "Epoch 76:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 192.82it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 76:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 189.11it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 193.65it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.00it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.27it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.45it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Validating:  19%|███████████████████████████████                                                                                                                                         | 240/1297 [00:00<00:02, 426.65it/s]\u001b[A\n",
      "Epoch 76:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.47it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 214.44it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.32it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.09it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 225.76it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.36it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 232.85it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.26it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 239.53it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 242.82it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 245.95it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.08it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.06it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 254.97it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 257.94it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 260.83it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 263.51it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.11it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 268.70it/s, loss=4.06e-07, v_num=32, train_loss=0.000378, test_loss=0.000386]\u001b[A\n",
      "Epoch 76: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.12it/s, loss=4.06e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 194.73it/s, loss=3.47e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[AAdjusting learning rate of group 0 to 8.2263e-03.\n",
      "Epoch 77:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 192.23it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 77:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 188.51it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 193.03it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 197.53it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 201.87it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.08it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.26it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 214.31it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▊                                                                                                                           | 346/1297 [00:00<00:02, 474.18it/s]\u001b[A\n",
      "Epoch 77:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.16it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 221.97it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 225.59it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.14it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 232.68it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.06it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 239.23it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 242.48it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 245.65it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 248.63it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 251.68it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 254.61it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 257.47it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 260.27it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 263.03it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 265.73it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 268.33it/s, loss=3.41e-07, v_num=32, train_loss=0.000669, test_loss=0.000674]\u001b[A\n",
      "Epoch 77: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.35it/s, loss=3.41e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.60it/s, loss=3.17e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[AAdjusting learning rate of group 0 to 8.2058e-03.\n",
      "Epoch 78:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.93it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 78:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 190.14it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.67it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 199.10it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 203.37it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 207.59it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Validating:  19%|███████████████████████████████                                                                                                                                         | 240/1297 [00:00<00:02, 429.83it/s]\u001b[A\n",
      "Epoch 78:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 211.67it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 215.55it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 219.45it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 223.14it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.78it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 230.38it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.96it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 237.39it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 240.67it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.85it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 247.00it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 250.09it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 253.17it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 256.11it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 259.01it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.79it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.50it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 267.13it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 269.73it/s, loss=3.16e-07, v_num=32, train_loss=0.000437, test_loss=0.000443]\u001b[A\n",
      "Epoch 78: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.83it/s, loss=3.16e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.94it/s, loss=2.46e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[AAdjusting learning rate of group 0 to 8.1853e-03.\n",
      "Epoch 79:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.61it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 79:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 189.91it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.36it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.74it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 203.00it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Validating:  15%|████████████████████████▍                                                                                                                                               | 189/1297 [00:00<00:02, 395.66it/s]\u001b[A\n",
      "Epoch 79:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 207.13it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 211.20it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 215.20it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 219.02it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.83it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.57it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 230.17it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.70it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 237.07it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 240.35it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.62it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.74it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.82it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.85it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.80it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.63it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.47it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.20it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.80it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n",
      "Epoch 79:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 269.45it/s, loss=2.55e-07, v_num=32, train_loss=0.000568, test_loss=0.000577]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.65it/s, loss=2.55e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 194.98it/s, loss=3.51e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[AAdjusting learning rate of group 0 to 8.1648e-03.\n",
      "Epoch 80:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 192.45it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 80:  51%|█████████████████████████████████████████████████████                                                   | 1325/2594 [00:07<00:06, 188.73it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  53%|███████████████████████████████████████████████████████▏                                                | 1378/2594 [00:07<00:06, 193.31it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  55%|█████████████████████████████████████████████████████████▎                                              | 1431/2594 [00:07<00:05, 197.79it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  57%|███████████████████████████████████████████████████████████▍                                            | 1484/2594 [00:07<00:05, 202.17it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  59%|█████████████████████████████████████████████████████████████▌                                          | 1537/2594 [00:07<00:05, 206.39it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  61%|███████████████████████████████████████████████████████████████▋                                        | 1590/2594 [00:07<00:04, 210.49it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Validating:  23%|██████████████████████████████████████                                                                                                                                  | 294/1297 [00:00<00:02, 455.76it/s]\u001b[A\n",
      "Epoch 80:  63%|█████████████████████████████████████████████████████████████████▊                                      | 1643/2594 [00:07<00:04, 214.46it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  65%|███████████████████████████████████████████████████████████████████▉                                    | 1696/2594 [00:07<00:04, 218.37it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  67%|██████████████████████████████████████████████████████████████████████                                  | 1749/2594 [00:07<00:03, 222.11it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  69%|████████████████████████████████████████████████████████████████████████▏                               | 1802/2594 [00:07<00:03, 225.81it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  72%|██████████████████████████████████████████████████████████████████████████▎                             | 1855/2594 [00:08<00:03, 229.33it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  74%|████████████████████████████████████████████████████████████████████████████▍                           | 1908/2594 [00:08<00:02, 232.84it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  76%|██████████████████████████████████████████████████████████████████████████████▌                         | 1961/2594 [00:08<00:02, 236.25it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  78%|████████████████████████████████████████████████████████████████████████████████▋                       | 2014/2594 [00:08<00:02, 239.62it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  80%|██████████████████████████████████████████████████████████████████████████████████▊                     | 2067/2594 [00:08<00:02, 242.92it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 2120/2594 [00:08<00:01, 246.03it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  84%|███████████████████████████████████████████████████████████████████████████████████████                 | 2173/2594 [00:08<00:01, 249.12it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  86%|█████████████████████████████████████████████████████████████████████████████████████████▏              | 2226/2594 [00:08<00:01, 252.21it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  88%|███████████████████████████████████████████████████████████████████████████████████████████▎            | 2279/2594 [00:08<00:01, 255.19it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 258.09it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 260.97it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 263.73it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.39it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.00it/s, loss=3.81e-07, v_num=32, train_loss=0.00031, test_loss=0.000314]\u001b[A\n",
      "Epoch 80: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.26it/s, loss=3.81e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 195.25it/s, loss=6.49e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[AAdjusting learning rate of group 0 to 8.1444e-03.\n",
      "Epoch 81:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 193.58it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 81:  51%|█████████████████████████████████████████████████████                                                   | 1325/2594 [00:06<00:06, 189.79it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  53%|███████████████████████████████████████████████████████▏                                                | 1378/2594 [00:07<00:06, 194.39it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81:  55%|█████████████████████████████████████████████████████████▎                                              | 1431/2594 [00:07<00:05, 198.77it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  57%|███████████████████████████████████████████████████████████▍                                            | 1484/2594 [00:07<00:05, 203.15it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  59%|█████████████████████████████████████████████████████████████▌                                          | 1537/2594 [00:07<00:05, 207.30it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  61%|███████████████████████████████████████████████████████████████▋                                        | 1590/2594 [00:07<00:04, 211.38it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  63%|█████████████████████████████████████████████████████████████████▊                                      | 1643/2594 [00:07<00:04, 215.40it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Validating:  27%|█████████████████████████████████████████████                                                                                                                           | 348/1297 [00:00<00:02, 466.32it/s]\u001b[A\n",
      "Epoch 81:  65%|███████████████████████████████████████████████████████████████████▉                                    | 1696/2594 [00:07<00:04, 219.12it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  67%|██████████████████████████████████████████████████████████████████████                                  | 1749/2594 [00:07<00:03, 222.86it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  69%|████████████████████████████████████████████████████████████████████████▏                               | 1802/2594 [00:07<00:03, 226.48it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  72%|██████████████████████████████████████████████████████████████████████████▎                             | 1855/2594 [00:08<00:03, 230.09it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  74%|████████████████████████████████████████████████████████████████████████████▍                           | 1908/2594 [00:08<00:02, 233.63it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  76%|██████████████████████████████████████████████████████████████████████████████▌                         | 1961/2594 [00:08<00:02, 237.06it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  78%|████████████████████████████████████████████████████████████████████████████████▋                       | 2014/2594 [00:08<00:02, 240.35it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  80%|██████████████████████████████████████████████████████████████████████████████████▊                     | 2067/2594 [00:08<00:02, 243.60it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 2120/2594 [00:08<00:01, 246.73it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  84%|███████████████████████████████████████████████████████████████████████████████████████                 | 2173/2594 [00:08<00:01, 249.83it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  86%|█████████████████████████████████████████████████████████████████████████████████████████▏              | 2226/2594 [00:08<00:01, 252.87it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  88%|███████████████████████████████████████████████████████████████████████████████████████████▎            | 2279/2594 [00:08<00:01, 255.78it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 258.62it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 261.44it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 264.20it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.90it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.57it/s, loss=7.21e-07, v_num=32, train_loss=0.000432, test_loss=0.00044]\u001b[A\n",
      "Epoch 81: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.64it/s, loss=7.21e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 194.80it/s, loss=3.79e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[AAdjusting learning rate of group 0 to 8.1240e-03.\n",
      "Epoch 82:  50%|█████████████████████████████████████████████████████                                                     | 1297/2594 [00:06<00:06, 193.20it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 82:  51%|██████████████████████████████████████████████████████▏                                                   | 1325/2594 [00:06<00:06, 189.43it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  53%|████████████████████████████████████████████████████████▎                                                 | 1378/2594 [00:07<00:06, 194.03it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  55%|██████████████████████████████████████████████████████████▍                                               | 1431/2594 [00:07<00:05, 198.38it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  57%|████████████████████████████████████████████████████████████▋                                             | 1484/2594 [00:07<00:05, 202.73it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  59%|██████████████████████████████████████████████████████████████▊                                           | 1537/2594 [00:07<00:05, 206.94it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Validating:  19%|███████████████████████████████▏                                                                                                                                        | 241/1297 [00:00<00:02, 430.15it/s]\u001b[A\n",
      "Epoch 82:  61%|████████████████████████████████████████████████████████████████▉                                         | 1590/2594 [00:07<00:04, 210.88it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  63%|███████████████████████████████████████████████████████████████████▏                                      | 1643/2594 [00:07<00:04, 214.89it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  65%|█████████████████████████████████████████████████████████████████████▎                                    | 1696/2594 [00:07<00:04, 218.78it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82:  67%|███████████████████████████████████████████████████████████████████████▍                                  | 1749/2594 [00:07<00:03, 222.62it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  69%|█████████████████████████████████████████████████████████████████████████▋                                | 1802/2594 [00:07<00:03, 226.36it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  72%|███████████████████████████████████████████████████████████████████████████▊                              | 1855/2594 [00:08<00:03, 229.95it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  74%|█████████████████████████████████████████████████████████████████████████████▉                            | 1908/2594 [00:08<00:02, 233.39it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  76%|████████████████████████████████████████████████████████████████████████████████▏                         | 1961/2594 [00:08<00:02, 236.81it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  78%|██████████████████████████████████████████████████████████████████████████████████▎                       | 2014/2594 [00:08<00:02, 240.09it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  80%|████████████████████████████████████████████████████████████████████████████████████▍                     | 2067/2594 [00:08<00:02, 243.36it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  82%|██████████████████████████████████████████████████████████████████████████████████████▋                   | 2120/2594 [00:08<00:01, 246.60it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  84%|████████████████████████████████████████████████████████████████████████████████████████▊                 | 2173/2594 [00:08<00:01, 249.65it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  86%|██████████████████████████████████████████████████████████████████████████████████████████▉               | 2226/2594 [00:08<00:01, 252.62it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  88%|█████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 255.55it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  90%|███████████████████████████████████████████████████████████████████████████████████████████████▎          | 2332/2594 [00:09<00:01, 258.45it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████▍        | 2385/2594 [00:09<00:00, 261.28it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 264.01it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.66it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.29it/s, loss=3.8e-07, v_num=32, train_loss=0.00349, test_loss=0.00358]\u001b[A\n",
      "Epoch 82: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.35it/s, loss=3.8e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 194.98it/s, loss=4.11e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[AAdjusting learning rate of group 0 to 8.1037e-03.\n",
      "Epoch 83:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 192.99it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 83:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 189.17it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 193.77it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.17it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.50it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.77it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.88it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Validating:  23%|█████████████████████████████████████▉                                                                                                                                  | 293/1297 [00:00<00:02, 454.44it/s]\u001b[A\n",
      "Epoch 83:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 214.85it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.73it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.50it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.16it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.75it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.23it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.68it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 239.90it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.13it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.28it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.30it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.32it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.25it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.12it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 260.93it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 263.64it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.32it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 268.81it/s, loss=3.86e-07, v_num=32, train_loss=0.000597, test_loss=0.000601]\u001b[A\n",
      "Epoch 83: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.12it/s, loss=3.86e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.87it/s, loss=9.93e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[AAdjusting learning rate of group 0 to 8.0835e-03.\n",
      "Epoch 84:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 194.19it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 84:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 190.41it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.99it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 199.37it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 203.74it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 207.98it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Validating:  19%|███████████████████████████████▏                                                                                                                                        | 241/1297 [00:00<00:02, 432.66it/s]\u001b[A\n",
      "Epoch 84:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 211.94it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 216.01it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 219.94it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 223.78it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 227.54it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 231.18it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 234.68it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 238.16it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 241.52it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 244.79it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 248.04it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 251.15it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 254.17it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 257.17it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:08<00:01, 260.07it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 262.93it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 265.73it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 268.49it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 271.10it/s, loss=9.11e-07, v_num=32, train_loss=0.000801, test_loss=0.000807]\u001b[A\n",
      "Epoch 84: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 272.08it/s, loss=9.11e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.60it/s, loss=4.53e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[AAdjusting learning rate of group 0 to 8.0632e-03.\n",
      "Epoch 85:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.58it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 85:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:06<00:06, 189.83it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 194.43it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 198.82it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 203.16it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 207.44it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 211.59it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Validating:  23%|█████████████████████████████████████▉                                                                                                                                  | 293/1297 [00:00<00:02, 456.94it/s]\u001b[A\n",
      "Epoch 85:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 215.54it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 219.39it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 223.19it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 226.81it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 230.40it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 233.89it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 237.36it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 240.65it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 243.94it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 247.21it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 250.25it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 253.28it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 256.23it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 259.13it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 261.96it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 264.79it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 267.50it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 270.16it/s, loss=4.59e-07, v_num=32, train_loss=0.00101, test_loss=0.00103]\u001b[A\n",
      "Epoch 85: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.17it/s, loss=4.59e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 195.38it/s, loss=4.24e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[AAdjusting learning rate of group 0 to 8.0431e-03.\n",
      "Epoch 86:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 193.73it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86:  51%|█████████████████████████████████████████████████████                                                   | 1325/2594 [00:06<00:06, 189.97it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  53%|███████████████████████████████████████████████████████▏                                                | 1378/2594 [00:07<00:06, 194.36it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  55%|█████████████████████████████████████████████████████████▎                                              | 1431/2594 [00:07<00:05, 198.85it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  57%|███████████████████████████████████████████████████████████▍                                            | 1484/2594 [00:07<00:05, 203.26it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  59%|█████████████████████████████████████████████████████████████▌                                          | 1537/2594 [00:07<00:05, 207.56it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  61%|███████████████████████████████████████████████████████████████▋                                        | 1590/2594 [00:07<00:04, 211.69it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Validating:  23%|██████████████████████████████████████                                                                                                                                  | 294/1297 [00:00<00:02, 457.94it/s]\u001b[A\n",
      "Epoch 86:  63%|█████████████████████████████████████████████████████████████████▊                                      | 1643/2594 [00:07<00:04, 215.58it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  65%|███████████████████████████████████████████████████████████████████▉                                    | 1696/2594 [00:07<00:04, 219.45it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  67%|██████████████████████████████████████████████████████████████████████                                  | 1749/2594 [00:07<00:03, 223.27it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  69%|████████████████████████████████████████████████████████████████████████▏                               | 1802/2594 [00:07<00:03, 227.00it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  72%|██████████████████████████████████████████████████████████████████████████▎                             | 1855/2594 [00:08<00:03, 230.57it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  74%|████████████████████████████████████████████████████████████████████████████▍                           | 1908/2594 [00:08<00:02, 234.06it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  76%|██████████████████████████████████████████████████████████████████████████████▌                         | 1961/2594 [00:08<00:02, 237.58it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  78%|████████████████████████████████████████████████████████████████████████████████▋                       | 2014/2594 [00:08<00:02, 240.95it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  80%|██████████████████████████████████████████████████████████████████████████████████▊                     | 2067/2594 [00:08<00:02, 244.15it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 2120/2594 [00:08<00:01, 247.33it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  84%|███████████████████████████████████████████████████████████████████████████████████████                 | 2173/2594 [00:08<00:01, 250.44it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  86%|█████████████████████████████████████████████████████████████████████████████████████████▏              | 2226/2594 [00:08<00:01, 253.46it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  88%|███████████████████████████████████████████████████████████████████████████████████████████▎            | 2279/2594 [00:08<00:01, 256.45it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:08<00:01, 259.29it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 262.10it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 264.89it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 267.50it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 270.11it/s, loss=4.32e-07, v_num=32, train_loss=0.000564, test_loss=0.00057]\u001b[A\n",
      "Epoch 86: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.48it/s, loss=4.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.60it/s, loss=3.28e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[AAdjusting learning rate of group 0 to 8.0230e-03.\n",
      "Epoch 87:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 193.25it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 87:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:06<00:06, 189.51it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 194.12it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.59it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.99it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 207.26it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 211.31it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 215.27it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:  27%|████████████████████████████████████████████▉                                                                                                                           | 347/1297 [00:00<00:02, 466.16it/s]\u001b[A\n",
      "Epoch 87:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 219.17it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.99it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 226.75it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 230.36it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.91it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 237.30it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 240.65it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 243.88it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.98it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 250.17it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 253.18it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 256.18it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 259.07it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 261.85it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 264.63it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 267.26it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 269.88it/s, loss=3.32e-07, v_num=32, train_loss=0.000427, test_loss=0.000435]\u001b[A\n",
      "Epoch 87: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.96it/s, loss=3.32e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 195.42it/s, loss=3.91e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[AAdjusting learning rate of group 0 to 8.0029e-03.\n",
      "Epoch 88:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.79it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 88:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:06<00:06, 189.96it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 194.50it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 198.89it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 203.21it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 207.48it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Validating:  19%|███████████████████████████████                                                                                                                                         | 240/1297 [00:00<00:02, 432.74it/s]\u001b[A\n",
      "Epoch 88:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 211.52it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 215.51it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 219.46it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 223.24it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 226.91it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 230.47it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 233.97it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 237.30it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 240.69it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 243.95it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 247.06it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 250.14it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 253.17it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 256.16it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 259.06it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 261.88it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 264.62it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 267.32it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.98it/s, loss=3.8e-07, v_num=32, train_loss=0.000365, test_loss=0.00037]\u001b[A\n",
      "Epoch 88: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.99it/s, loss=3.8e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 194.82it/s, loss=2.56e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[AAdjusting learning rate of group 0 to 7.9829e-03.\n",
      "Epoch 89:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.11it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 89:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:07<00:06, 189.29it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 193.92it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 198.38it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 202.63it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 206.92it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 211.02it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 215.04it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▊                                                                                                                           | 346/1297 [00:00<00:02, 469.22it/s]\u001b[A\n",
      "Epoch 89:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 218.92it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 222.74it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 226.39it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 230.02it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 233.51it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 236.91it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 240.24it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 243.38it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 246.50it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 249.61it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 252.57it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 255.49it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 258.35it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 261.16it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 263.94it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.70it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.35it/s, loss=2.67e-07, v_num=32, train_loss=0.0004, test_loss=0.000405]\u001b[A\n",
      "Epoch 89: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.49it/s, loss=2.67e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.32it/s, loss=7.76e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[AAdjusting learning rate of group 0 to 7.9630e-03.\n",
      "Epoch 90:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 193.62it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 90:  51%|█████████████████████████████████████████████████████                                                   | 1325/2594 [00:06<00:06, 189.80it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  53%|███████████████████████████████████████████████████████▏                                                | 1378/2594 [00:07<00:06, 194.35it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  55%|█████████████████████████████████████████████████████████▎                                              | 1431/2594 [00:07<00:05, 198.88it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  57%|███████████████████████████████████████████████████████████▍                                            | 1484/2594 [00:07<00:05, 203.14it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  59%|█████████████████████████████████████████████████████████████▌                                          | 1537/2594 [00:07<00:05, 207.29it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Validating:  19%|███████████████████████████████▏                                                                                                                                        | 241/1297 [00:00<00:02, 425.99it/s]\u001b[A\n",
      "Epoch 90:  61%|███████████████████████████████████████████████████████████████▋                                        | 1590/2594 [00:07<00:04, 211.27it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  63%|█████████████████████████████████████████████████████████████████▊                                      | 1643/2594 [00:07<00:04, 215.26it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  65%|███████████████████████████████████████████████████████████████████▉                                    | 1696/2594 [00:07<00:04, 219.19it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  67%|██████████████████████████████████████████████████████████████████████                                  | 1749/2594 [00:07<00:03, 222.98it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  69%|████████████████████████████████████████████████████████████████████████▏                               | 1802/2594 [00:07<00:03, 226.61it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  72%|██████████████████████████████████████████████████████████████████████████▎                             | 1855/2594 [00:08<00:03, 230.29it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  74%|████████████████████████████████████████████████████████████████████████████▍                           | 1908/2594 [00:08<00:02, 233.81it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  76%|██████████████████████████████████████████████████████████████████████████████▌                         | 1961/2594 [00:08<00:02, 237.26it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  78%|████████████████████████████████████████████████████████████████████████████████▋                       | 2014/2594 [00:08<00:02, 240.60it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  80%|██████████████████████████████████████████████████████████████████████████████████▊                     | 2067/2594 [00:08<00:02, 243.80it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 2120/2594 [00:08<00:01, 246.93it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  84%|███████████████████████████████████████████████████████████████████████████████████████                 | 2173/2594 [00:08<00:01, 250.09it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  86%|█████████████████████████████████████████████████████████████████████████████████████████▏              | 2226/2594 [00:08<00:01, 253.01it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  88%|███████████████████████████████████████████████████████████████████████████████████████████▎            | 2279/2594 [00:08<00:01, 255.91it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 258.76it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 261.46it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 264.16it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.82it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.44it/s, loss=7.8e-07, v_num=32, train_loss=0.000342, test_loss=0.000347]\u001b[A\n",
      "Epoch 90: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.41it/s, loss=7.8e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  50%|████████████████████████████████████████████████████▍                                                    | 1296/2594 [00:06<00:06, 195.66it/s, loss=4.72e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[AAdjusting learning rate of group 0 to 7.9430e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 193.96it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 91:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:06<00:06, 189.97it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 194.47it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 198.97it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 203.36it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 207.62it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 211.78it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 215.76it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Validating:  27%|█████████████████████████████████████████████▏                                                                                                                          | 349/1297 [00:00<00:02, 469.32it/s]\u001b[A\n",
      "Epoch 91:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 219.53it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 223.37it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 227.05it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 230.67it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 234.18it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 237.60it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 240.94it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 244.16it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 247.34it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 250.41it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 253.45it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 256.34it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:08<00:01, 259.25it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 262.03it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 264.79it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 267.32it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.95it/s, loss=4.64e-07, v_num=32, train_loss=0.00121, test_loss=0.00121]\u001b[A\n",
      "Epoch 91: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.32it/s, loss=4.64e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.31it/s, loss=5.94e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[AAdjusting learning rate of group 0 to 7.9232e-03.\n",
      "Epoch 92:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 192.83it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 92:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 189.15it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 193.67it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.13it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.46it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.56it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Validating:  19%|███████████████████████████████▏                                                                                                                                        | 241/1297 [00:00<00:02, 426.51it/s]\u001b[A\n",
      "Epoch 92:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.63it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 214.60it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.49it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.34it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 225.97it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.56it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.09it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.51it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 239.76it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 242.97it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.13it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.25it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.23it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.15it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.05it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 260.85it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 263.60it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.27it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 268.87it/s, loss=6.05e-07, v_num=32, train_loss=0.000503, test_loss=0.000506]\u001b[A\n",
      "Epoch 92: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.88it/s, loss=6.05e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 194.34it/s, loss=2.78e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[AAdjusting learning rate of group 0 to 7.9034e-03.\n",
      "Epoch 93:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 192.66it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 93:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 189.05it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 193.64it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 198.09it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.35it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.53it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Validating:  19%|███████████████████████████████▏                                                                                                                                        | 241/1297 [00:00<00:02, 429.44it/s]\u001b[A\n",
      "Epoch 93:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.53it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 214.49it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.30it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.05it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 225.78it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.44it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 233.00it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.43it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 239.67it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 242.96it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 246.13it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 249.21it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 252.27it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 255.21it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 258.07it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 260.88it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 263.59it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 266.32it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 268.91it/s, loss=2.87e-07, v_num=32, train_loss=0.000624, test_loss=0.000631]\u001b[A\n",
      "Epoch 93: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.97it/s, loss=2.87e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 194.66it/s, loss=7.32e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[AAdjusting learning rate of group 0 to 7.8836e-03.\n",
      "Epoch 94:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 192.96it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 94:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 188.92it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 193.46it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 197.93it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.23it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.39it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.53it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Validating:  23%|█████████████████████████████████████▉                                                                                                                                  | 293/1297 [00:00<00:02, 451.86it/s]\u001b[A\n",
      "Epoch 94:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 214.48it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.35it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.16it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 225.84it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.38it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 232.81it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.10it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 239.34it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 242.60it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 245.76it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 248.78it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 251.86it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 254.78it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 257.62it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 260.47it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 263.17it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 265.82it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 268.47it/s, loss=7.46e-07, v_num=32, train_loss=0.000453, test_loss=0.000458]\u001b[A\n",
      "Epoch 94: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.51it/s, loss=7.46e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 195.50it/s, loss=2.44e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[AAdjusting learning rate of group 0 to 7.8639e-03.\n",
      "Epoch 95:  50%|████████████████████████████████████████████████████                                                    | 1297/2594 [00:06<00:06, 193.79it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 95:  51%|█████████████████████████████████████████████████████                                                   | 1325/2594 [00:06<00:06, 190.16it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  53%|███████████████████████████████████████████████████████▏                                                | 1378/2594 [00:07<00:06, 194.78it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  55%|█████████████████████████████████████████████████████████▎                                              | 1431/2594 [00:07<00:05, 199.25it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  57%|███████████████████████████████████████████████████████████▍                                            | 1484/2594 [00:07<00:05, 203.58it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  59%|█████████████████████████████████████████████████████████████▌                                          | 1537/2594 [00:07<00:05, 207.82it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  61%|███████████████████████████████████████████████████████████████▋                                        | 1590/2594 [00:07<00:04, 211.89it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  63%|█████████████████████████████████████████████████████████████████▊                                      | 1643/2594 [00:07<00:04, 215.90it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▉                                                                                                                           | 347/1297 [00:00<00:02, 467.56it/s]\u001b[A\n",
      "Epoch 95:  65%|███████████████████████████████████████████████████████████████████▉                                    | 1696/2594 [00:07<00:04, 219.68it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  67%|██████████████████████████████████████████████████████████████████████                                  | 1749/2594 [00:07<00:03, 223.43it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  69%|████████████████████████████████████████████████████████████████████████▏                               | 1802/2594 [00:07<00:03, 227.09it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  72%|██████████████████████████████████████████████████████████████████████████▎                             | 1855/2594 [00:08<00:03, 230.65it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  74%|████████████████████████████████████████████████████████████████████████████▍                           | 1908/2594 [00:08<00:02, 234.16it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  76%|██████████████████████████████████████████████████████████████████████████████▌                         | 1961/2594 [00:08<00:02, 237.52it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  78%|████████████████████████████████████████████████████████████████████████████████▋                       | 2014/2594 [00:08<00:02, 240.79it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  80%|██████████████████████████████████████████████████████████████████████████████████▊                     | 2067/2594 [00:08<00:02, 244.00it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  82%|████████████████████████████████████████████████████████████████████████████████████▉                   | 2120/2594 [00:08<00:01, 247.12it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  84%|███████████████████████████████████████████████████████████████████████████████████████                 | 2173/2594 [00:08<00:01, 250.16it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  86%|█████████████████████████████████████████████████████████████████████████████████████████▏              | 2226/2594 [00:08<00:01, 253.11it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  88%|███████████████████████████████████████████████████████████████████████████████████████████▎            | 2279/2594 [00:08<00:01, 255.99it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  90%|█████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:09<00:01, 258.91it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  92%|███████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 261.58it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 264.26it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 266.95it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n",
      "Epoch 95:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 269.59it/s, loss=2.33e-07, v_num=32, train_loss=0.00067, test_loss=0.000683]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.52it/s, loss=2.33e-07, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 194.12it/s, loss=2.03e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[AAdjusting learning rate of group 0 to 7.8443e-03.\n",
      "Epoch 96:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 192.35it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 96:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 188.57it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 193.11it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 197.56it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 201.86it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.07it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Validating:  19%|███████████████████████████████                                                                                                                                         | 240/1297 [00:00<00:02, 430.43it/s]\u001b[A\n",
      "Epoch 96:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.12it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 214.22it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.08it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 221.89it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 225.58it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.17it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 232.65it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.08it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 239.34it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 242.66it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 245.88it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 248.97it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 251.96it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 254.74it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 257.65it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 260.51it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 263.29it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 265.97it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████  | 2544/2594 [00:09<00:00, 268.59it/s, loss=2.09e-06, v_num=32, train_loss=0.000378, test_loss=0.000382]\u001b[A\n",
      "Epoch 96: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.01it/s, loss=2.09e-06, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  50%|████████████████████████████████████████████████████▉                                                     | 1296/2594 [00:06<00:06, 196.30it/s, loss=6.6e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[AAdjusting learning rate of group 0 to 7.8246e-03.\n",
      "Epoch 97:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 194.56it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 97:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:06<00:06, 190.66it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 195.19it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 199.63it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 203.93it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 208.20it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Validating:  19%|███████████████████████████████                                                                                                                                         | 240/1297 [00:00<00:02, 431.87it/s]\u001b[A\n",
      "Epoch 97:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 212.31it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 216.35it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 220.21it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 224.02it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 227.74it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 231.37it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 234.87it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 238.30it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 241.58it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 244.82it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 247.97it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 251.06it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 253.99it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 256.86it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:08<00:01, 259.71it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 262.48it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 265.15it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 267.78it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 270.46it/s, loss=6.52e-07, v_num=32, train_loss=0.00295, test_loss=0.00294]\u001b[A\n",
      "Epoch 97: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.51it/s, loss=6.52e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  50%|███████████████████████████████████████████████████▍                                                   | 1296/2594 [00:06<00:06, 195.50it/s, loss=6.63e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[AAdjusting learning rate of group 0 to 7.8051e-03.\n",
      "Epoch 98:  50%|███████████████████████████████████████████████████▌                                                   | 1297/2594 [00:06<00:06, 192.73it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 98:  51%|████████████████████████████████████████████████████▌                                                  | 1325/2594 [00:07<00:06, 189.03it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  53%|██████████████████████████████████████████████████████▋                                                | 1378/2594 [00:07<00:06, 193.51it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  55%|████████████████████████████████████████████████████████▊                                              | 1431/2594 [00:07<00:05, 197.89it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  57%|██████████████████████████████████████████████████████████▉                                            | 1484/2594 [00:07<00:05, 202.18it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Validating:  14%|████████████████████████▏                                                                                                                                               | 187/1297 [00:00<00:02, 395.59it/s]\u001b[A\n",
      "Epoch 98:  59%|█████████████████████████████████████████████████████████████                                          | 1537/2594 [00:07<00:05, 206.35it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  61%|███████████████████████████████████████████████████████████████▏                                       | 1590/2594 [00:07<00:04, 210.45it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  63%|█████████████████████████████████████████████████████████████████▏                                     | 1643/2594 [00:07<00:04, 214.45it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  65%|███████████████████████████████████████████████████████████████████▎                                   | 1696/2594 [00:07<00:04, 218.24it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98:  67%|█████████████████████████████████████████████████████████████████████▍                                 | 1749/2594 [00:07<00:03, 222.00it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  69%|███████████████████████████████████████████████████████████████████████▌                               | 1802/2594 [00:07<00:03, 225.72it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  72%|█████████████████████████████████████████████████████████████████████████▋                             | 1855/2594 [00:08<00:03, 229.32it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  74%|███████████████████████████████████████████████████████████████████████████▊                           | 1908/2594 [00:08<00:02, 232.75it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  76%|█████████████████████████████████████████████████████████████████████████████▊                         | 1961/2594 [00:08<00:02, 236.15it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  78%|███████████████████████████████████████████████████████████████████████████████▉                       | 2014/2594 [00:08<00:02, 239.46it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  80%|██████████████████████████████████████████████████████████████████████████████████                     | 2067/2594 [00:08<00:02, 242.68it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 2120/2594 [00:08<00:01, 245.86it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  84%|██████████████████████████████████████████████████████████████████████████████████████▎                | 2173/2594 [00:08<00:01, 248.85it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  86%|████████████████████████████████████████████████████████████████████████████████████████▍              | 2226/2594 [00:08<00:01, 251.86it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  88%|██████████████████████████████████████████████████████████████████████████████████████████▍            | 2279/2594 [00:08<00:01, 254.79it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  90%|████████████████████████████████████████████████████████████████████████████████████████████▌          | 2332/2594 [00:09<00:01, 257.70it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  92%|██████████████████████████████████████████████████████████████████████████████████████████████▋        | 2385/2594 [00:09<00:00, 260.50it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98:  94%|████████████████████████████████████████████████████████████████████████████████████████████████▊      | 2438/2594 [00:09<00:00, 263.21it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Validating:  88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                    | 1142/1297 [00:02<00:00, 496.16it/s]\u001b[A\n",
      "Epoch 98:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████▉    | 2491/2594 [00:09<00:00, 265.83it/s, loss=6.42e-07, v_num=32, train_loss=0.000936, test_loss=0.000945]\u001b[A\n",
      "Epoch 98: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 269.46it/s, loss=6.42e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  50%|███████████████████████████████████████████████████▉                                                    | 1296/2594 [00:06<00:06, 195.91it/s, loss=2.82e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[AAdjusting learning rate of group 0 to 7.7856e-03.\n",
      "Epoch 99:  50%|████████████████████████████████████████████████████▌                                                    | 1297/2594 [00:06<00:06, 194.11it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                                                                                                                                   | 0/1297 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 99:  51%|█████████████████████████████████████████████████████▋                                                   | 1325/2594 [00:06<00:06, 190.18it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  53%|███████████████████████████████████████████████████████▊                                                 | 1378/2594 [00:07<00:06, 194.76it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  55%|█████████████████████████████████████████████████████████▉                                               | 1431/2594 [00:07<00:05, 199.23it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  57%|████████████████████████████████████████████████████████████                                             | 1484/2594 [00:07<00:05, 203.55it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  59%|██████████████████████████████████████████████████████████████▏                                          | 1537/2594 [00:07<00:05, 207.83it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  61%|████████████████████████████████████████████████████████████████▎                                        | 1590/2594 [00:07<00:04, 211.87it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  63%|██████████████████████████████████████████████████████████████████▌                                      | 1643/2594 [00:07<00:04, 215.81it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Validating:  27%|████████████████████████████████████████████▊                                                                                                                           | 346/1297 [00:00<00:02, 462.65it/s]\u001b[A\n",
      "Epoch 99:  65%|████████████████████████████████████████████████████████████████████▋                                    | 1696/2594 [00:07<00:04, 219.65it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  67%|██████████████████████████████████████████████████████████████████████▊                                  | 1749/2594 [00:07<00:03, 223.47it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  69%|████████████████████████████████████████████████████████████████████████▉                                | 1802/2594 [00:07<00:03, 227.23it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  72%|███████████████████████████████████████████████████████████████████████████                              | 1855/2594 [00:08<00:03, 230.83it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  74%|█████████████████████████████████████████████████████████████████████████████▏                           | 1908/2594 [00:08<00:02, 234.32it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  76%|███████████████████████████████████████████████████████████████████████████████▍                         | 1961/2594 [00:08<00:02, 237.75it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  78%|█████████████████████████████████████████████████████████████████████████████████▌                       | 2014/2594 [00:08<00:02, 241.03it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 2067/2594 [00:08<00:02, 244.23it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99:  82%|█████████████████████████████████████████████████████████████████████████████████████▊                   | 2120/2594 [00:08<00:01, 247.45it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  84%|███████████████████████████████████████████████████████████████████████████████████████▉                 | 2173/2594 [00:08<00:01, 250.48it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  86%|██████████████████████████████████████████████████████████████████████████████████████████               | 2226/2594 [00:08<00:01, 253.52it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  88%|████████████████████████████████████████████████████████████████████████████████████████████▏            | 2279/2594 [00:08<00:01, 256.48it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  90%|██████████████████████████████████████████████████████████████████████████████████████████████▍          | 2332/2594 [00:08<00:01, 259.37it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  92%|████████████████████████████████████████████████████████████████████████████████████████████████▌        | 2385/2594 [00:09<00:00, 262.15it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████▋      | 2438/2594 [00:09<00:00, 264.89it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 2491/2594 [00:09<00:00, 267.62it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 2544/2594 [00:09<00:00, 270.21it/s, loss=2.8e-07, v_num=32, train_loss=0.000882, test_loss=0.00089]\u001b[A\n",
      "Epoch 99: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 271.22it/s, loss=2.8e-07, v_num=32, train_loss=0.000361, test_loss=0.000367]\u001b[A\n",
      "Epoch 99: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2594/2594 [00:09<00:00, 270.89it/s, loss=2.8e-07, v_num=32, train_loss=0.000361, test_loss=0.000367]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "    print(f\"Training model {model_index}\")\n",
    "    omegas = torch.Tensor(dirichlet.rvs(np.ones(n_samples))).T\n",
    "    omegas = omegas.type(torch.FloatTensor)\n",
    "    omegas_0 = torch.ones_like(omegas) / len(omegas)\n",
    "    area = torch.ones_like(omegas)\n",
    "    train_size = 1.0\n",
    "    num_workers = 8\n",
    "    hparams = {\"n_layers\": 5, \"n_hidden\": 128, \"batch_size\": 128, \"learning_rate\": 0.01}\n",
    "    \n",
    "    if train_size == 1.0:\n",
    "        data_loader = PDDDataModule(X_train_norm, Y_train, omegas, omegas_0, num_workers=num_workers)\n",
    "    else:\n",
    "        data_loader = PDDDataModule(\n",
    "            X_train_norm, Y_train, omegas, omegas_0, train_size=train_size, num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    data_loader.setup()\n",
    "    e = PDDEmulator(\n",
    "        n_parameters,\n",
    "        n_outputs,\n",
    "        hparams,\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        auto_lr_find=True,\n",
    "        max_epochs=100,\n",
    "        gpus=1,\n",
    "#        deterministic=True,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    if train_size == 1.0:\n",
    "        train_loader = data_loader.train_all_loader\n",
    "        val_loader = data_loader.val_all_loader\n",
    "    else:\n",
    "        train_loader = data_loader.train_loader\n",
    "        val_loader = data_loader.val_loader\n",
    "\n",
    "        \n",
    "    # lr_finder = trainer.tuner.lr_find(e, train_loader, val_loader)\n",
    "    # fig = lr_finder.plot(suggest=True) # Plot\n",
    "    # fig.show()\n",
    "    trainer.fit(e, train_loader, val_loader)\n",
    "    torch.save(e.state_dict(), f\"{emulator_dir}/emulator/emulator_{model_index}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20de572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.sample import saltelli\n",
    "from pyDOE import lhs\n",
    "from scipy.stats.distributions import truncnorm, gamma, uniform, randint\n",
    "method = \"lhs\"\n",
    "n_prior_samples = 200\n",
    "np.random.seed(3)\n",
    "\n",
    "distributions = {\n",
    "    \"f_snow\": uniform(\n",
    "        loc=2.0, scale=4.0\n",
    "    ),  # uniform between 2 and 6\n",
    "    \"f_ice\": uniform(\n",
    "        loc=3.0, scale=9\n",
    "    ),  # uniform between 3 and 12\n",
    "    \"refreeze\": uniform(loc=0, scale=1.0),  # uniform between 0 and 1\n",
    "}\n",
    "# Names of all the variables\n",
    "keys = [x for x in distributions.keys()]\n",
    "\n",
    "# Describe the Problem\n",
    "problem = {\"num_vars\": len(keys), \"names\": keys, \"bounds\": [[0, 1]] * len(keys)}\n",
    "\n",
    "# Generate uniform samples (i.e. one unit hypercube)\n",
    "if method == \"saltelli\":\n",
    "    unif_sample = saltelli.sample(problem, n_prior_samples, calc_second_order=False)\n",
    "elif method == \"lhs\":\n",
    "    unif_sample = lhs(len(keys), n_prior_samples)\n",
    "else:\n",
    "    print(f\"Method {method} not available\")\n",
    "\n",
    "# To hold the transformed variables\n",
    "dist_sample = np.zeros_like(unif_sample)\n",
    "\n",
    "# Now transform the unit hypercube to the prescribed distributions\n",
    "# For each variable, transform with the inverse of the CDF (inv(CDF)=ppf)\n",
    "for i, key in enumerate(keys):\n",
    "    dist_sample[:, i] = distributions[key].ppf(unif_sample[:, i])\n",
    "\n",
    "# Save to CSV file using Pandas DataFrame and to_csv method\n",
    "header = keys\n",
    "# Convert to Pandas dataframe, append column headers, output as csv\n",
    "df = pd.DataFrame(data=dist_sample, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b8761667",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X = []\n",
    "    Y = []\n",
    "    for k, row in df.iterrows():   \n",
    "        m_f_snow = row[\"f_snow\"]\n",
    "        m_f_ice = row[\"f_ice\"]\n",
    "        m_refreeze = row[\"refreeze\"]\n",
    "\n",
    "        pdd = TorchPDDModel(\n",
    "            pdd_factor_snow=m_f_snow,\n",
    "            pdd_factor_ice=m_f_ice,\n",
    "            refreeze_snow=m_refreeze,\n",
    "            refreeze_ice=m_refreeze,\n",
    "        )\n",
    "        result = pdd(temp, precip, std_dev)\n",
    "\n",
    "        M_val = result[\"melt\"]\n",
    "        A_val = result[\"accu\"]\n",
    "        R_val = result[\"refreeze\"]\n",
    "        m_Y = torch.vstack((M_val, A_val, R_val,)).T\n",
    "        Y.append(m_Y)\n",
    "        X.append(torch.from_numpy(np.hstack((temp.T, precip.T, np.tile(m_f_snow, (temp.shape[1], 1)), np.tile(m_f_ice, (temp.shape[1], 1)), np.tile(m_refreeze, (temp.shape[1], 1))))))\n",
    "\n",
    "    X_val = torch.vstack(X).type(torch.FloatTensor)\n",
    "    Y_val = torch.vstack(Y).type(torch.FloatTensor)\n",
    "\n",
    "    X_val_mean = X_val.mean(axis=0)\n",
    "    X_val_std = X_val.std(axis=0)\n",
    "    X_val_norm = (X_val - X_val_mean) / X_val_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3f097205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([166000, 27])\n",
      "[0.01323718 0.00180899 0.0137568 ]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_norm.shape)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "device = \"cuda\"\n",
    "e.to(device)\n",
    "X_val_norm = X_val_norm.to(device)\n",
    "e.eval()\n",
    "Y_pred = e(X_val_norm).detach().cpu()\n",
    "rmse = [np.sqrt(mean_squared_error(Y_pred.detach().cpu().numpy()[:,i], Y_val.detach().cpu().numpy()[:,i])) for i in range(Y_val.shape[1])]\n",
    "print(np.array(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6249f798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1506e+00, 3.1322e-03, 4.4661e-01],\n",
       "        [4.6769e-01, 1.6635e-03, 2.2976e-01],\n",
       "        [4.8440e-01, 1.1212e-03, 2.3460e-01],\n",
       "        ...,\n",
       "        [2.6348e-01, 1.3041e-03, 1.0303e-01],\n",
       "        [3.3485e-01, 4.9844e-04, 1.2985e-01],\n",
       "        [2.8387e-01, 6.4072e-04, 1.1276e-01]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9d1a2de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1278, -0.0090, -0.0386],\n",
       "        [-0.0117, -0.0031,  0.0023],\n",
       "        [-0.0352, -0.0002, -0.0119],\n",
       "        ...,\n",
       "        [-0.0180, -0.0003,  0.0202],\n",
       "        [-0.0174, -0.0007,  0.0262],\n",
       "        [-0.0282, -0.0004,  0.0209]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred - Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "24e61c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MALASampler(object):\n",
    "    \"\"\"\n",
    "    MALA Sampler\n",
    "\n",
    "    Author: Douglas C Brinkerhoff, University of Montana\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self, model, alpha_b=3.0, beta_b=3.0, alpha=0.01, emulator_dir=\"./emulator\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model.eval()\n",
    "        self.alpha = alpha\n",
    "        self.alpha_b = alpha_b\n",
    "        self.beta_b = beta_b\n",
    "        self.emulator_dir = emulator_dir\n",
    "\n",
    "    def find_MAP(self, X, X_I, Y_target, X_min, X_max, n_iters=50, print_interval=10):\n",
    "        print(\"***********************************************\")\n",
    "        print(\"***********************************************\")\n",
    "        print(\"Finding MAP point\")\n",
    "        print(\"***********************************************\")\n",
    "        print(\"***********************************************\")\n",
    "        # Line search distances\n",
    "        alphas = np.logspace(-4, 0, 11)\n",
    "        # Find MAP point\n",
    "        for i in range(n_iters):\n",
    "            log_pi, g, _, Hinv, log_det_Hinv = self.get_log_like_gradient_and_hessian(\n",
    "                X, X_I, Y_target, X_min, X_max, compute_hessian=True\n",
    "            )\n",
    "            p = Hinv @ -g\n",
    "            alpha_index = np.nanargmin(\n",
    "                [\n",
    "                    self.get_log_like_gradient_and_hessian(\n",
    "                        X + alpha * p, X_I, Y_target, X_min, X_max, compute_hessian=False\n",
    "                    )\n",
    "                    .detach()\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                    for alpha in alphas\n",
    "                ]\n",
    "            )\n",
    "            mu = X + alphas[alpha_index] * p\n",
    "            X.data = mu.data\n",
    "            if i % print_interval == 0:\n",
    "                print(\"===============================================\")\n",
    "                print(f\"iter: {i:d}, log(P): {log_pi:.1f}\\n\")\n",
    "                print(\n",
    "                    \"\".join(\n",
    "                        [\n",
    "                            f\"{key}: {(val * std + mean):.3f}\\n\"\n",
    "                            for key, val, std, mean in zip(\n",
    "                                X_P_keys,\n",
    "                                X.data.cpu().numpy(),\n",
    "                                X_P_std,\n",
    "                                X_P_mean,\n",
    "                            )\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "                print(\"===============================================\")\n",
    "        return X\n",
    "\n",
    "    def V(self, X, X_I, Y_target, X_bar):\n",
    "        # model result is in log space\n",
    "        X_IP = torch.hstack((X, X_I))\n",
    "        Y_pred = self.model(X_IP)\n",
    "        r = Y_pred - Y_target\n",
    "        L1 = torch.sum(\n",
    "            np.log(gamma((nu + 1) / 2.0))\n",
    "            - np.log(gamma(nu / 2.0))\n",
    "            - np.log(np.sqrt(np.pi * nu) * sigma_hat)\n",
    "            - (nu + 1) / 2.0 * torch.log(1 + 1.0 / nu * (r / sigma_hat) ** 2)\n",
    "        )\n",
    "        L2 = torch.sum(\n",
    "            (self.alpha_b - 1) * torch.log(X_bar)\n",
    "            + (self.beta_b - 1) * torch.log(1 - X_bar)\n",
    "        )\n",
    "\n",
    "        return -(self.alpha * L1 + L2)\n",
    "\n",
    "    def get_log_like_gradient_and_hessian(\n",
    "        self, X, X_I, Y_target, X_min, X_max, eps=1e-2, compute_hessian=False\n",
    "    ):\n",
    "\n",
    "        X_bar = (X - X_min) / (X_max - X_min)\n",
    "        log_pi = self.V(X, X_I, Y_target, X_bar)\n",
    "        if compute_hessian:\n",
    "            g = torch.autograd.grad(log_pi, X, retain_graph=True, create_graph=True)[0]\n",
    "            H = torch.stack(\n",
    "                [torch.autograd.grad(e, X, retain_graph=True)[0] for e in g]\n",
    "            )\n",
    "            lamda, Q = torch.linalg.eig(H)\n",
    "            lamda, Q = lamda.type(torch.float), Q.type(torch.float)\n",
    "            lamda_prime = torch.sqrt(lamda ** 2 + eps)\n",
    "            lamda_prime_inv = 1.0 / torch.sqrt(lamda ** 2 + eps)\n",
    "            H = Q @ torch.diag(lamda_prime) @ Q.T\n",
    "            Hinv = Q @ torch.diag(lamda_prime_inv) @ Q.T\n",
    "            log_det_Hinv = torch.sum(torch.log(lamda_prime_inv))\n",
    "            return log_pi, g, H, Hinv, log_det_Hinv\n",
    "        else:\n",
    "            return log_pi\n",
    "\n",
    "    def draw_sample(self, mu, cov, eps=1e-10):\n",
    "        L = torch.linalg.cholesky(cov + eps * torch.eye(cov.shape[0], device=device))\n",
    "        return mu + L @ torch.randn(L.shape[0], device=device)\n",
    "\n",
    "    def get_proposal_likelihood(self, Y, mu, inverse_cov, log_det_cov):\n",
    "        return -0.5 * log_det_cov - 0.5 * (Y - mu) @ inverse_cov @ (Y - mu)\n",
    "\n",
    "    def MALA_step(self, X, X_I, Y_target, X_min, X_max, h, local_data=None):\n",
    "        if local_data is not None:\n",
    "            pass\n",
    "        else:\n",
    "            local_data = self.get_log_like_gradient_and_hessian(\n",
    "                X, X_I, Y_target, X_min, X_max, compute_hessian=True\n",
    "            )\n",
    "\n",
    "        log_pi, _, H, Hinv, log_det_Hinv = local_data\n",
    "\n",
    "        X_ = self.draw_sample(X, 2 * h * Hinv).detach()\n",
    "        X_.requires_grad = True\n",
    "\n",
    "        log_pi_ = self.get_log_like_gradient_and_hessian(\n",
    "            X_, X_I, Y_target, X_min, X_max, compute_hessian=False\n",
    "        )\n",
    "\n",
    "        logq = self.get_proposal_likelihood(X_, X, H / (2 * h), log_det_Hinv)\n",
    "        logq_ = self.get_proposal_likelihood(X, X_, H / (2 * h), log_det_Hinv)\n",
    "\n",
    "        log_alpha = -log_pi_ + logq_ + log_pi - logq\n",
    "        alpha = torch.exp(min(log_alpha, torch.tensor([0.0], device=device)))\n",
    "        u = torch.rand(1, device=device)\n",
    "        if u <= alpha and log_alpha != np.inf:\n",
    "            X.data = X_.data\n",
    "            local_data = self.get_log_like_gradient_and_hessian(\n",
    "                X, X_I, Y_target, X_min, X_max, compute_hessian=True\n",
    "            )\n",
    "            s = 1\n",
    "        else:\n",
    "            s = 0\n",
    "        return X, local_data, s\n",
    "\n",
    "    def MALA(\n",
    "        self,\n",
    "        X,\n",
    "        X_I,\n",
    "        X_min,\n",
    "        X_max,\n",
    "        Y_target,\n",
    "        n_iters=10001,\n",
    "        h=0.1,\n",
    "        h_max=1.0,\n",
    "        acc_target=0.25,\n",
    "        k=0.01,\n",
    "        beta=0.99,\n",
    "        model_index=0,\n",
    "        save_interval=1000,\n",
    "        print_interval=50,\n",
    "    ):\n",
    "        print(\"***********************************************\")\n",
    "        print(\"***********************************************\")\n",
    "        print(\n",
    "            \"Running Metropolis-Adjusted Langevin Algorithm for model index {0}\".format(\n",
    "                model_index\n",
    "            )\n",
    "        )\n",
    "        print(\"***********************************************\")\n",
    "        print(\"***********************************************\")\n",
    "\n",
    "        posterior_dir = f\"{self.emulator_dir}/posterior_samples/\"\n",
    "        if not os.path.isdir(posterior_dir):\n",
    "            os.makedirs(posterior_dir)\n",
    "\n",
    "        local_data = None\n",
    "        m_vars = []\n",
    "        acc = acc_target\n",
    "        print(n_iters)\n",
    "        for i in range(n_iters):\n",
    "            X, local_data, s = self.MALA_step(\n",
    "                X, X_I, Y_target, X_min, X_max, h, local_data=local_data\n",
    "            )\n",
    "            m_vars.append(X.detach())\n",
    "            acc = beta * acc + (1 - beta) * s\n",
    "            h = min(h * (1 + k * np.sign(acc - acc_target)), h_max)\n",
    "            if i % print_interval == 0:\n",
    "                print(\"===============================================\")\n",
    "                print(\n",
    "                    \"sample: {0:d}, acc. rate: {1:4.2f}, log(P): {2:6.1f}\".format(\n",
    "                        i, acc, local_data[0].item()\n",
    "                    )\n",
    "                )\n",
    "                print(\n",
    "                    \" \".join(\n",
    "                        [\n",
    "                            f\"{key}: {(val * std + mean):.3f}\\n\"\n",
    "                            for key, val, std, mean in zip(\n",
    "                                X_P_keys,\n",
    "                                X.data.cpu().numpy(),\n",
    "                                X_P_std,\n",
    "                                X_P_mean,\n",
    "                            )\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "                print(\"===============================================\")\n",
    "\n",
    "            if i % save_interval == 0:\n",
    "                print(\"///////////////////////////////////////////////\")\n",
    "                print(\"Saving samples for model {0}\".format(model_index))\n",
    "                print(\"///////////////////////////////////////////////\")\n",
    "                X_posterior = torch.stack(m_vars).cpu().numpy()\n",
    "                df = pd.DataFrame(\n",
    "                    data=X_posterior.astype(\"float32\") * X_P_std.cpu().numpy()\n",
    "                    + X_P_mean.cpu().numpy(),\n",
    "                    columns=X_P_keys,\n",
    "                )\n",
    "                df.to_csv(\n",
    "                    posterior_dir + \"X_posterior_model_{0}.csv.gz\".format(model_index),\n",
    "                    compression=\"infer\",\n",
    "                )\n",
    "        X_posterior = torch.stack(m_vars).cpu().numpy()\n",
    "        return X_posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18975c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.sample import saltelli\n",
    "from pyDOE import lhs\n",
    "from scipy.stats.distributions import truncnorm, gamma, uniform, randint\n",
    "method = \"lhs\"\n",
    "n_prior_samples = 500\n",
    "np.random.seed(3)\n",
    "\n",
    "distributions = {\n",
    "    \"f_snow\": uniform(\n",
    "        loc=2.0, scale=4.0\n",
    "    ),  # uniform between 2 and 6\n",
    "    \"f_ice\": uniform(\n",
    "        loc=3.0, scale=9\n",
    "    ),  # uniform between 3 and 12\n",
    "    \"refreeze\": uniform(loc=0, scale=1.0),  # uniform between 0 and 1\n",
    "}\n",
    "# Names of all the variables\n",
    "keys = [x for x in distributions.keys()]\n",
    "\n",
    "# Describe the Problem\n",
    "problem = {\"num_vars\": len(keys), \"names\": keys, \"bounds\": [[0, 1]] * len(keys)}\n",
    "\n",
    "# Generate uniform samples (i.e. one unit hypercube)\n",
    "if method == \"saltelli\":\n",
    "    unif_sample = saltelli.sample(problem, n_prior_samples, calc_second_order=False)\n",
    "elif method == \"lhs\":\n",
    "    unif_sample = lhs(len(keys), n_prior_samples)\n",
    "else:\n",
    "    print(f\"Method {method} not available\")\n",
    "\n",
    "# To hold the transformed variables\n",
    "dist_sample = np.zeros_like(unif_sample)\n",
    "\n",
    "# Now transform the unit hypercube to the prescribed distributions\n",
    "# For each variable, transform with the inverse of the CDF (inv(CDF)=ppf)\n",
    "for i, key in enumerate(keys):\n",
    "    dist_sample[:, i] = distributions[key].ppf(unif_sample[:, i])\n",
    "\n",
    "# Save to CSV file using Pandas DataFrame and to_csv method\n",
    "header = keys\n",
    "# Convert to Pandas dataframe, append column headers, output as csv\n",
    "df = pd.DataFrame(data=dist_sample, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a069a21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21437/2542389321.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_P_0 = torch.tensor(X_P_prior.mean(axis=0),\n",
      "/tmp/ipykernel_21437/2542389321.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_I_0 = torch.tensor(X_I_prior.mean(axis=0),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "***********************************************\n",
      "Finding MAP point\n",
      "***********************************************\n",
      "***********************************************\n",
      "===============================================\n",
      "iter: 0, log(P): 498.8\n",
      "\n",
      "f_snow: 3.975\n",
      "f_ice: 9.907\n",
      "refreeze: 0.621\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "iter: 10, log(P): 493.1\n",
      "\n",
      "f_snow: 4.228\n",
      "f_ice: 10.888\n",
      "refreeze: 0.758\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "iter: 20, log(P): 493.1\n",
      "\n",
      "f_snow: 4.228\n",
      "f_ice: 10.888\n",
      "refreeze: 0.758\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "iter: 30, log(P): 493.1\n",
      "\n",
      "f_snow: 4.228\n",
      "f_ice: 10.888\n",
      "refreeze: 0.758\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "iter: 40, log(P): 493.1\n",
      "\n",
      "f_snow: 4.228\n",
      "f_ice: 10.888\n",
      "refreeze: 0.758\n",
      "\n",
      "===============================================\n",
      "***********************************************\n",
      "***********************************************\n",
      "Running Metropolis-Adjusted Langevin Algorithm for model index 0\n",
      "***********************************************\n",
      "***********************************************\n",
      "100000\n",
      "===============================================\n",
      "sample: 0, acc. rate: 0.25, log(P):  493.1\n",
      "f_snow: 4.228\n",
      " f_ice: 10.888\n",
      " refreeze: 0.758\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 100, acc. rate: 0.52, log(P):  498.7\n",
      "f_snow: 3.296\n",
      " f_ice: 11.667\n",
      " refreeze: 0.981\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 200, acc. rate: 0.47, log(P):  493.7\n",
      "f_snow: 4.119\n",
      " f_ice: 11.384\n",
      " refreeze: 0.626\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 300, acc. rate: 0.31, log(P):  493.8\n",
      "f_snow: 5.448\n",
      " f_ice: 11.281\n",
      " refreeze: 0.285\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 400, acc. rate: 0.29, log(P):  494.6\n",
      "f_snow: 5.190\n",
      " f_ice: 9.314\n",
      " refreeze: 0.350\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 500, acc. rate: 0.30, log(P):  494.4\n",
      "f_snow: 3.199\n",
      " f_ice: 11.458\n",
      " refreeze: 0.686\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 600, acc. rate: 0.34, log(P):  493.6\n",
      "f_snow: 5.504\n",
      " f_ice: 10.037\n",
      " refreeze: 0.362\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 700, acc. rate: 0.33, log(P):  493.1\n",
      "f_snow: 5.297\n",
      " f_ice: 10.480\n",
      " refreeze: 0.717\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 800, acc. rate: 0.24, log(P):  495.3\n",
      "f_snow: 5.618\n",
      " f_ice: 10.295\n",
      " refreeze: 0.149\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 900, acc. rate: 0.29, log(P):  493.8\n",
      "f_snow: 5.665\n",
      " f_ice: 11.440\n",
      " refreeze: 0.685\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 1000, acc. rate: 0.30, log(P):  493.4\n",
      "f_snow: 4.160\n",
      " f_ice: 10.751\n",
      " refreeze: 0.634\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 1100, acc. rate: 0.25, log(P):  495.1\n",
      "f_snow: 3.099\n",
      " f_ice: 9.668\n",
      " refreeze: 0.795\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 1200, acc. rate: 0.28, log(P):  493.6\n",
      "f_snow: 5.768\n",
      " f_ice: 10.896\n",
      " refreeze: 0.582\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 1300, acc. rate: 0.31, log(P):  494.8\n",
      "f_snow: 5.884\n",
      " f_ice: 10.490\n",
      " refreeze: 0.667\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 1400, acc. rate: 0.29, log(P):  494.4\n",
      "f_snow: 5.322\n",
      " f_ice: 10.832\n",
      " refreeze: 0.191\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 1500, acc. rate: 0.26, log(P):  492.9\n",
      "f_snow: 5.264\n",
      " f_ice: 11.025\n",
      " refreeze: 0.632\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 1600, acc. rate: 0.27, log(P):  493.5\n",
      "f_snow: 5.591\n",
      " f_ice: 11.129\n",
      " refreeze: 0.354\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 1700, acc. rate: 0.28, log(P):  494.8\n",
      "f_snow: 4.273\n",
      " f_ice: 11.046\n",
      " refreeze: 0.945\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 1800, acc. rate: 0.33, log(P):  495.6\n",
      "f_snow: 3.791\n",
      " f_ice: 8.714\n",
      " refreeze: 0.754\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 1900, acc. rate: 0.26, log(P):  493.3\n",
      "f_snow: 5.395\n",
      " f_ice: 11.105\n",
      " refreeze: 0.752\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 2000, acc. rate: 0.28, log(P):  493.5\n",
      "f_snow: 5.471\n",
      " f_ice: 11.431\n",
      " refreeze: 0.673\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 2100, acc. rate: 0.28, log(P):  495.8\n",
      "f_snow: 5.409\n",
      " f_ice: 8.105\n",
      " refreeze: 0.495\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 2200, acc. rate: 0.25, log(P):  492.9\n",
      "f_snow: 5.371\n",
      " f_ice: 10.765\n",
      " refreeze: 0.682\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 2300, acc. rate: 0.29, log(P):  495.7\n",
      "f_snow: 4.586\n",
      " f_ice: 9.135\n",
      " refreeze: 0.468\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 2400, acc. rate: 0.30, log(P):  495.8\n",
      "f_snow: 3.177\n",
      " f_ice: 11.375\n",
      " refreeze: 0.946\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 2500, acc. rate: 0.30, log(P):  492.8\n",
      "f_snow: 5.437\n",
      " f_ice: 10.827\n",
      " refreeze: 0.606\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 2600, acc. rate: 0.23, log(P):  495.6\n",
      "f_snow: 2.859\n",
      " f_ice: 10.428\n",
      " refreeze: 0.911\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 2700, acc. rate: 0.29, log(P):  493.2\n",
      "f_snow: 5.545\n",
      " f_ice: 10.207\n",
      " refreeze: 0.687\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 2800, acc. rate: 0.34, log(P):  493.5\n",
      "f_snow: 5.680\n",
      " f_ice: 11.289\n",
      " refreeze: 0.562\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 2900, acc. rate: 0.26, log(P):  493.5\n",
      "f_snow: 4.421\n",
      " f_ice: 11.068\n",
      " refreeze: 0.603\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 3000, acc. rate: 0.26, log(P):  496.0\n",
      "f_snow: 4.287\n",
      " f_ice: 11.042\n",
      " refreeze: 0.233\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 3100, acc. rate: 0.27, log(P):  493.3\n",
      "f_snow: 4.486\n",
      " f_ice: 10.783\n",
      " refreeze: 0.680\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 3200, acc. rate: 0.29, log(P):  495.5\n",
      "f_snow: 5.396\n",
      " f_ice: 10.297\n",
      " refreeze: 0.935\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 3300, acc. rate: 0.29, log(P):  494.4\n",
      "f_snow: 3.673\n",
      " f_ice: 11.571\n",
      " refreeze: 0.863\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 3400, acc. rate: 0.31, log(P):  492.9\n",
      "f_snow: 5.423\n",
      " f_ice: 10.588\n",
      " refreeze: 0.688\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 3500, acc. rate: 0.31, log(P):  496.4\n",
      "f_snow: 2.435\n",
      " f_ice: 10.452\n",
      " refreeze: 0.671\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 3600, acc. rate: 0.30, log(P):  494.4\n",
      "f_snow: 2.929\n",
      " f_ice: 10.984\n",
      " refreeze: 0.702\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 3700, acc. rate: 0.30, log(P):  493.2\n",
      "f_snow: 5.383\n",
      " f_ice: 9.999\n",
      " refreeze: 0.513\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 3800, acc. rate: 0.23, log(P):  494.2\n",
      "f_snow: 3.979\n",
      " f_ice: 10.342\n",
      " refreeze: 0.528\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 3900, acc. rate: 0.32, log(P):  496.1\n",
      "f_snow: 5.296\n",
      " f_ice: 11.880\n",
      " refreeze: 0.613\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 4000, acc. rate: 0.30, log(P):  494.5\n",
      "f_snow: 4.925\n",
      " f_ice: 9.361\n",
      " refreeze: 0.529\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 4100, acc. rate: 0.30, log(P):  495.2\n",
      "f_snow: 2.694\n",
      " f_ice: 10.621\n",
      " refreeze: 0.735\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 4200, acc. rate: 0.34, log(P):  494.2\n",
      "f_snow: 5.683\n",
      " f_ice: 11.577\n",
      " refreeze: 0.633\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 4300, acc. rate: 0.29, log(P):  493.7\n",
      "f_snow: 5.597\n",
      " f_ice: 11.013\n",
      " refreeze: 0.806\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 4400, acc. rate: 0.27, log(P):  493.5\n",
      "f_snow: 5.200\n",
      " f_ice: 11.466\n",
      " refreeze: 0.462\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 4500, acc. rate: 0.28, log(P):  493.4\n",
      "f_snow: 5.026\n",
      " f_ice: 10.246\n",
      " refreeze: 0.601\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 4600, acc. rate: 0.29, log(P):  494.9\n",
      "f_snow: 3.118\n",
      " f_ice: 9.951\n",
      " refreeze: 0.845\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 4700, acc. rate: 0.30, log(P):  493.7\n",
      "f_snow: 4.460\n",
      " f_ice: 10.899\n",
      " refreeze: 0.888\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 4800, acc. rate: 0.25, log(P):  493.8\n",
      "f_snow: 5.644\n",
      " f_ice: 10.786\n",
      " refreeze: 0.813\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 4900, acc. rate: 0.28, log(P):  495.3\n",
      "f_snow: 3.375\n",
      " f_ice: 11.598\n",
      " refreeze: 0.913\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 5000, acc. rate: 0.27, log(P):  493.7\n",
      "f_snow: 5.498\n",
      " f_ice: 10.071\n",
      " refreeze: 0.344\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 5100, acc. rate: 0.30, log(P):  494.0\n",
      "f_snow: 5.583\n",
      " f_ice: 9.463\n",
      " refreeze: 0.730\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 5200, acc. rate: 0.34, log(P):  494.4\n",
      "f_snow: 5.699\n",
      " f_ice: 9.903\n",
      " refreeze: 0.822\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 5300, acc. rate: 0.31, log(P):  493.4\n",
      "f_snow: 4.480\n",
      " f_ice: 10.444\n",
      " refreeze: 0.830\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 5400, acc. rate: 0.28, log(P):  492.8\n",
      "f_snow: 5.267\n",
      " f_ice: 10.808\n",
      " refreeze: 0.555\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 5500, acc. rate: 0.25, log(P):  494.5\n",
      "f_snow: 3.077\n",
      " f_ice: 11.395\n",
      " refreeze: 0.809\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 5600, acc. rate: 0.30, log(P):  494.3\n",
      "f_snow: 4.550\n",
      " f_ice: 9.394\n",
      " refreeze: 0.755\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 5700, acc. rate: 0.30, log(P):  493.0\n",
      "f_snow: 5.461\n",
      " f_ice: 10.339\n",
      " refreeze: 0.513\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 5800, acc. rate: 0.32, log(P):  494.8\n",
      "f_snow: 5.283\n",
      " f_ice: 11.590\n",
      " refreeze: 0.248\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 5900, acc. rate: 0.25, log(P):  493.8\n",
      "f_snow: 5.541\n",
      " f_ice: 11.271\n",
      " refreeze: 0.305\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 6000, acc. rate: 0.36, log(P):  493.3\n",
      "f_snow: 5.026\n",
      " f_ice: 11.148\n",
      " refreeze: 0.477\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 6100, acc. rate: 0.32, log(P):  494.2\n",
      "f_snow: 5.784\n",
      " f_ice: 11.392\n",
      " refreeze: 0.678\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 6200, acc. rate: 0.31, log(P):  496.1\n",
      "f_snow: 5.058\n",
      " f_ice: 8.284\n",
      " refreeze: 0.772\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 6300, acc. rate: 0.24, log(P):  494.5\n",
      "f_snow: 5.308\n",
      " f_ice: 11.699\n",
      " refreeze: 0.410\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 6400, acc. rate: 0.27, log(P):  493.3\n",
      "f_snow: 5.701\n",
      " f_ice: 10.333\n",
      " refreeze: 0.577\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 6500, acc. rate: 0.36, log(P):  494.9\n",
      "f_snow: 5.204\n",
      " f_ice: 11.002\n",
      " refreeze: 0.916\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 6600, acc. rate: 0.28, log(P):  494.6\n",
      "f_snow: 3.851\n",
      " f_ice: 9.513\n",
      " refreeze: 0.866\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 6700, acc. rate: 0.27, log(P):  493.6\n",
      "f_snow: 4.247\n",
      " f_ice: 10.200\n",
      " refreeze: 0.826\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 6800, acc. rate: 0.28, log(P):  493.5\n",
      "f_snow: 4.982\n",
      " f_ice: 11.190\n",
      " refreeze: 0.444\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 6900, acc. rate: 0.25, log(P):  493.8\n",
      "f_snow: 5.518\n",
      " f_ice: 10.307\n",
      " refreeze: 0.296\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 7000, acc. rate: 0.28, log(P):  494.3\n",
      "f_snow: 5.828\n",
      " f_ice: 10.059\n",
      " refreeze: 0.626\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 7100, acc. rate: 0.26, log(P):  495.8\n",
      "f_snow: 5.598\n",
      " f_ice: 10.504\n",
      " refreeze: 0.944\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 7200, acc. rate: 0.28, log(P):  496.9\n",
      "f_snow: 5.866\n",
      " f_ice: 10.154\n",
      " refreeze: 0.137\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 7300, acc. rate: 0.28, log(P):  493.5\n",
      "f_snow: 5.688\n",
      " f_ice: 10.934\n",
      " refreeze: 0.398\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 7400, acc. rate: 0.22, log(P):  493.4\n",
      "f_snow: 5.370\n",
      " f_ice: 9.964\n",
      " refreeze: 0.710\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 7500, acc. rate: 0.25, log(P):  493.9\n",
      "f_snow: 3.937\n",
      " f_ice: 10.018\n",
      " refreeze: 0.848\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 7600, acc. rate: 0.29, log(P):  493.2\n",
      "f_snow: 4.312\n",
      " f_ice: 10.631\n",
      " refreeze: 0.757\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 7700, acc. rate: 0.27, log(P):  494.8\n",
      "f_snow: 4.982\n",
      " f_ice: 10.250\n",
      " refreeze: 0.215\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 7800, acc. rate: 0.31, log(P):  495.4\n",
      "f_snow: 4.592\n",
      " f_ice: 11.532\n",
      " refreeze: 0.940\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 7900, acc. rate: 0.29, log(P):  494.8\n",
      "f_snow: 5.166\n",
      " f_ice: 11.210\n",
      " refreeze: 0.908\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 8000, acc. rate: 0.30, log(P):  492.9\n",
      "f_snow: 5.493\n",
      " f_ice: 10.963\n",
      " refreeze: 0.654\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 8100, acc. rate: 0.30, log(P):  495.5\n",
      "f_snow: 2.896\n",
      " f_ice: 10.204\n",
      " refreeze: 0.895\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 8200, acc. rate: 0.26, log(P):  493.4\n",
      "f_snow: 5.166\n",
      " f_ice: 10.042\n",
      " refreeze: 0.480\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 8300, acc. rate: 0.32, log(P):  493.3\n",
      "f_snow: 5.481\n",
      " f_ice: 10.949\n",
      " refreeze: 0.344\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 8400, acc. rate: 0.32, log(P):  495.1\n",
      "f_snow: 4.497\n",
      " f_ice: 8.987\n",
      " refreeze: 0.675\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 8500, acc. rate: 0.31, log(P):  495.3\n",
      "f_snow: 5.585\n",
      " f_ice: 9.212\n",
      " refreeze: 0.874\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 8600, acc. rate: 0.27, log(P):  496.6\n",
      "f_snow: 3.856\n",
      " f_ice: 11.468\n",
      " refreeze: 0.251\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 8700, acc. rate: 0.30, log(P):  494.5\n",
      "f_snow: 5.632\n",
      " f_ice: 9.286\n",
      " refreeze: 0.773\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 8800, acc. rate: 0.32, log(P):  494.2\n",
      "f_snow: 5.142\n",
      " f_ice: 10.893\n",
      " refreeze: 0.226\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 8900, acc. rate: 0.30, log(P):  493.4\n",
      "f_snow: 5.355\n",
      " f_ice: 9.777\n",
      " refreeze: 0.662\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 9000, acc. rate: 0.32, log(P):  493.4\n",
      "f_snow: 4.803\n",
      " f_ice: 10.962\n",
      " refreeze: 0.802\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 9100, acc. rate: 0.36, log(P):  493.3\n",
      "f_snow: 5.331\n",
      " f_ice: 11.284\n",
      " refreeze: 0.424\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 9200, acc. rate: 0.25, log(P):  495.9\n",
      "f_snow: 4.557\n",
      " f_ice: 8.540\n",
      " refreeze: 0.630\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 9300, acc. rate: 0.27, log(P):  494.0\n",
      "f_snow: 4.815\n",
      " f_ice: 10.031\n",
      " refreeze: 0.651\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 9400, acc. rate: 0.28, log(P):  493.5\n",
      "f_snow: 4.787\n",
      " f_ice: 10.568\n",
      " refreeze: 0.709\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 9500, acc. rate: 0.23, log(P):  493.4\n",
      "f_snow: 3.859\n",
      " f_ice: 10.786\n",
      " refreeze: 0.827\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 9600, acc. rate: 0.31, log(P):  494.6\n",
      "f_snow: 3.366\n",
      " f_ice: 11.023\n",
      " refreeze: 0.482\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 9700, acc. rate: 0.22, log(P):  493.8\n",
      "f_snow: 4.652\n",
      " f_ice: 11.356\n",
      " refreeze: 0.639\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 9800, acc. rate: 0.26, log(P):  494.0\n",
      "f_snow: 5.642\n",
      " f_ice: 10.476\n",
      " refreeze: 0.833\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 9900, acc. rate: 0.29, log(P):  493.3\n",
      "f_snow: 5.417\n",
      " f_ice: 10.225\n",
      " refreeze: 0.734\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 10000, acc. rate: 0.29, log(P):  494.4\n",
      "f_snow: 3.379\n",
      " f_ice: 11.289\n",
      " refreeze: 0.546\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 10100, acc. rate: 0.29, log(P):  493.9\n",
      "f_snow: 3.758\n",
      " f_ice: 11.159\n",
      " refreeze: 0.889\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 10200, acc. rate: 0.30, log(P):  493.4\n",
      "f_snow: 4.683\n",
      " f_ice: 10.911\n",
      " refreeze: 0.658\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 10300, acc. rate: 0.34, log(P):  495.0\n",
      "f_snow: 5.533\n",
      " f_ice: 9.867\n",
      " refreeze: 0.897\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 10400, acc. rate: 0.27, log(P):  493.0\n",
      "f_snow: 5.400\n",
      " f_ice: 10.248\n",
      " refreeze: 0.612\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 10500, acc. rate: 0.28, log(P):  493.5\n",
      "f_snow: 3.611\n",
      " f_ice: 10.769\n",
      " refreeze: 0.805\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 10600, acc. rate: 0.26, log(P):  493.8\n",
      "f_snow: 4.322\n",
      " f_ice: 10.499\n",
      " refreeze: 0.889\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 10700, acc. rate: 0.26, log(P):  493.5\n",
      "f_snow: 3.759\n",
      " f_ice: 10.370\n",
      " refreeze: 0.745\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 10800, acc. rate: 0.24, log(P):  493.5\n",
      "f_snow: 3.995\n",
      " f_ice: 11.352\n",
      " refreeze: 0.812\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 10900, acc. rate: 0.30, log(P):  493.7\n",
      "f_snow: 5.602\n",
      " f_ice: 10.583\n",
      " refreeze: 0.812\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 11000, acc. rate: 0.33, log(P):  494.2\n",
      "f_snow: 3.909\n",
      " f_ice: 11.530\n",
      " refreeze: 0.592\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 11100, acc. rate: 0.26, log(P):  494.0\n",
      "f_snow: 5.827\n",
      " f_ice: 10.642\n",
      " refreeze: 0.551\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 11200, acc. rate: 0.27, log(P):  497.0\n",
      "f_snow: 5.073\n",
      " f_ice: 7.910\n",
      " refreeze: 0.384\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 11300, acc. rate: 0.33, log(P):  493.6\n",
      "f_snow: 5.143\n",
      " f_ice: 11.175\n",
      " refreeze: 0.344\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 11400, acc. rate: 0.29, log(P):  496.5\n",
      "f_snow: 4.329\n",
      " f_ice: 8.055\n",
      " refreeze: 0.712\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 11500, acc. rate: 0.29, log(P):  493.6\n",
      "f_snow: 5.643\n",
      " f_ice: 11.463\n",
      " refreeze: 0.560\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 11600, acc. rate: 0.33, log(P):  498.2\n",
      "f_snow: 5.787\n",
      " f_ice: 8.128\n",
      " refreeze: 0.200\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 11700, acc. rate: 0.35, log(P):  493.7\n",
      "f_snow: 5.463\n",
      " f_ice: 11.239\n",
      " refreeze: 0.302\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 11800, acc. rate: 0.32, log(P):  493.6\n",
      "f_snow: 5.613\n",
      " f_ice: 11.472\n",
      " refreeze: 0.591\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 11900, acc. rate: 0.28, log(P):  496.8\n",
      "f_snow: 5.919\n",
      " f_ice: 11.387\n",
      " refreeze: 0.846\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 12000, acc. rate: 0.29, log(P):  495.1\n",
      "f_snow: 5.230\n",
      " f_ice: 11.707\n",
      " refreeze: 0.802\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 12100, acc. rate: 0.27, log(P):  494.2\n",
      "f_snow: 5.801\n",
      " f_ice: 11.184\n",
      " refreeze: 0.418\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 12200, acc. rate: 0.27, log(P):  494.8\n",
      "f_snow: 4.090\n",
      " f_ice: 10.481\n",
      " refreeze: 0.414\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 12300, acc. rate: 0.26, log(P):  494.3\n",
      "f_snow: 5.317\n",
      " f_ice: 9.094\n",
      " refreeze: 0.503\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 12400, acc. rate: 0.27, log(P):  496.2\n",
      "f_snow: 4.158\n",
      " f_ice: 11.504\n",
      " refreeze: 0.287\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 12500, acc. rate: 0.30, log(P):  494.4\n",
      "f_snow: 5.546\n",
      " f_ice: 11.700\n",
      " refreeze: 0.574\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 12600, acc. rate: 0.26, log(P):  493.5\n",
      "f_snow: 5.339\n",
      " f_ice: 10.998\n",
      " refreeze: 0.795\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 12700, acc. rate: 0.32, log(P):  493.7\n",
      "f_snow: 4.413\n",
      " f_ice: 10.435\n",
      " refreeze: 0.607\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 12800, acc. rate: 0.33, log(P):  493.0\n",
      "f_snow: 5.241\n",
      " f_ice: 10.879\n",
      " refreeze: 0.447\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 12900, acc. rate: 0.29, log(P):  493.7\n",
      "f_snow: 4.468\n",
      " f_ice: 10.190\n",
      " refreeze: 0.849\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 13000, acc. rate: 0.29, log(P):  494.7\n",
      "f_snow: 5.818\n",
      " f_ice: 11.485\n",
      " refreeze: 0.665\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 13100, acc. rate: 0.29, log(P):  493.2\n",
      "f_snow: 5.078\n",
      " f_ice: 10.467\n",
      " refreeze: 0.478\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 13200, acc. rate: 0.30, log(P):  494.2\n",
      "f_snow: 5.836\n",
      " f_ice: 10.498\n",
      " refreeze: 0.680\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 13300, acc. rate: 0.31, log(P):  493.8\n",
      "f_snow: 3.684\n",
      " f_ice: 10.966\n",
      " refreeze: 0.881\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 13400, acc. rate: 0.33, log(P):  495.9\n",
      "f_snow: 3.421\n",
      " f_ice: 11.789\n",
      " refreeze: 0.574\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 13500, acc. rate: 0.32, log(P):  494.1\n",
      "f_snow: 5.346\n",
      " f_ice: 10.313\n",
      " refreeze: 0.249\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 13600, acc. rate: 0.30, log(P):  494.4\n",
      "f_snow: 3.197\n",
      " f_ice: 10.951\n",
      " refreeze: 0.883\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 13700, acc. rate: 0.28, log(P):  494.3\n",
      "f_snow: 5.279\n",
      " f_ice: 9.100\n",
      " refreeze: 0.657\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 13800, acc. rate: 0.34, log(P):  493.6\n",
      "f_snow: 4.878\n",
      " f_ice: 10.329\n",
      " refreeze: 0.625\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 13900, acc. rate: 0.32, log(P):  494.5\n",
      "f_snow: 3.417\n",
      " f_ice: 11.461\n",
      " refreeze: 0.561\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 14000, acc. rate: 0.27, log(P):  495.0\n",
      "f_snow: 5.274\n",
      " f_ice: 10.059\n",
      " refreeze: 0.904\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 14100, acc. rate: 0.30, log(P):  493.5\n",
      "f_snow: 5.586\n",
      " f_ice: 10.695\n",
      " refreeze: 0.790\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 14200, acc. rate: 0.32, log(P):  495.9\n",
      "f_snow: 5.571\n",
      " f_ice: 10.547\n",
      " refreeze: 0.098\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 14300, acc. rate: 0.31, log(P):  494.4\n",
      "f_snow: 3.231\n",
      " f_ice: 10.943\n",
      " refreeze: 0.559\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 14400, acc. rate: 0.31, log(P):  493.8\n",
      "f_snow: 4.751\n",
      " f_ice: 11.103\n",
      " refreeze: 0.458\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 14500, acc. rate: 0.29, log(P):  494.2\n",
      "f_snow: 5.782\n",
      " f_ice: 9.816\n",
      " refreeze: 0.672\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 14600, acc. rate: 0.25, log(P):  493.9\n",
      "f_snow: 4.079\n",
      " f_ice: 11.507\n",
      " refreeze: 0.847\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 14700, acc. rate: 0.32, log(P):  494.2\n",
      "f_snow: 5.422\n",
      " f_ice: 11.641\n",
      " refreeze: 0.417\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 14800, acc. rate: 0.26, log(P):  493.4\n",
      "f_snow: 5.703\n",
      " f_ice: 11.171\n",
      " refreeze: 0.623\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 14900, acc. rate: 0.27, log(P):  494.0\n",
      "f_snow: 3.556\n",
      " f_ice: 9.984\n",
      " refreeze: 0.770\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 15000, acc. rate: 0.32, log(P):  497.9\n",
      "f_snow: 5.591\n",
      " f_ice: 11.923\n",
      " refreeze: 0.313\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 15100, acc. rate: 0.31, log(P):  494.4\n",
      "f_snow: 4.365\n",
      " f_ice: 11.696\n",
      " refreeze: 0.749\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 15200, acc. rate: 0.25, log(P):  493.4\n",
      "f_snow: 5.232\n",
      " f_ice: 10.571\n",
      " refreeze: 0.774\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 15300, acc. rate: 0.32, log(P):  493.5\n",
      "f_snow: 5.624\n",
      " f_ice: 9.856\n",
      " refreeze: 0.508\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 15400, acc. rate: 0.31, log(P):  493.6\n",
      "f_snow: 4.721\n",
      " f_ice: 10.390\n",
      " refreeze: 0.685\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 15500, acc. rate: 0.29, log(P):  498.2\n",
      "f_snow: 5.742\n",
      " f_ice: 11.824\n",
      " refreeze: 0.124\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 15600, acc. rate: 0.29, log(P):  494.7\n",
      "f_snow: 3.355\n",
      " f_ice: 9.625\n",
      " refreeze: 0.755\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 15700, acc. rate: 0.28, log(P):  494.9\n",
      "f_snow: 4.841\n",
      " f_ice: 11.169\n",
      " refreeze: 0.937\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 15800, acc. rate: 0.31, log(P):  493.3\n",
      "f_snow: 5.326\n",
      " f_ice: 11.400\n",
      " refreeze: 0.596\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 15900, acc. rate: 0.28, log(P):  494.2\n",
      "f_snow: 5.299\n",
      " f_ice: 11.360\n",
      " refreeze: 0.843\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 16000, acc. rate: 0.27, log(P):  493.2\n",
      "f_snow: 5.592\n",
      " f_ice: 10.119\n",
      " refreeze: 0.573\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 16100, acc. rate: 0.27, log(P):  493.1\n",
      "f_snow: 5.250\n",
      " f_ice: 10.281\n",
      " refreeze: 0.662\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 16200, acc. rate: 0.30, log(P):  493.3\n",
      "f_snow: 5.046\n",
      " f_ice: 10.656\n",
      " refreeze: 0.436\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 16300, acc. rate: 0.28, log(P):  495.4\n",
      "f_snow: 5.513\n",
      " f_ice: 9.630\n",
      " refreeze: 0.910\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 16400, acc. rate: 0.30, log(P):  493.2\n",
      "f_snow: 5.022\n",
      " f_ice: 10.756\n",
      " refreeze: 0.680\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 16500, acc. rate: 0.32, log(P):  493.8\n",
      "f_snow: 4.697\n",
      " f_ice: 11.414\n",
      " refreeze: 0.705\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 16600, acc. rate: 0.30, log(P):  494.5\n",
      "f_snow: 5.420\n",
      " f_ice: 9.223\n",
      " refreeze: 0.361\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 16700, acc. rate: 0.28, log(P):  493.0\n",
      "f_snow: 5.335\n",
      " f_ice: 10.210\n",
      " refreeze: 0.611\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 16800, acc. rate: 0.25, log(P):  494.1\n",
      "f_snow: 4.541\n",
      " f_ice: 10.733\n",
      " refreeze: 0.913\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 16900, acc. rate: 0.29, log(P):  499.3\n",
      "f_snow: 5.977\n",
      " f_ice: 9.907\n",
      " refreeze: 0.296\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 17000, acc. rate: 0.28, log(P):  492.9\n",
      "f_snow: 5.426\n",
      " f_ice: 10.897\n",
      " refreeze: 0.688\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 17100, acc. rate: 0.28, log(P):  493.5\n",
      "f_snow: 4.653\n",
      " f_ice: 10.917\n",
      " refreeze: 0.855\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 17200, acc. rate: 0.22, log(P):  493.2\n",
      "f_snow: 5.595\n",
      " f_ice: 11.077\n",
      " refreeze: 0.459\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 17300, acc. rate: 0.28, log(P):  493.1\n",
      "f_snow: 5.395\n",
      " f_ice: 10.488\n",
      " refreeze: 0.400\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 17400, acc. rate: 0.30, log(P):  497.6\n",
      "f_snow: 5.166\n",
      " f_ice: 11.233\n",
      " refreeze: 0.979\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 17500, acc. rate: 0.29, log(P):  493.7\n",
      "f_snow: 4.585\n",
      " f_ice: 10.080\n",
      " refreeze: 0.834\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 17600, acc. rate: 0.26, log(P):  495.2\n",
      "f_snow: 5.712\n",
      " f_ice: 11.438\n",
      " refreeze: 0.205\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 17700, acc. rate: 0.28, log(P):  493.6\n",
      "f_snow: 4.260\n",
      " f_ice: 10.480\n",
      " refreeze: 0.607\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 17800, acc. rate: 0.31, log(P):  493.0\n",
      "f_snow: 5.414\n",
      " f_ice: 10.452\n",
      " refreeze: 0.449\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 17900, acc. rate: 0.25, log(P):  493.8\n",
      "f_snow: 3.855\n",
      " f_ice: 10.087\n",
      " refreeze: 0.668\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 18000, acc. rate: 0.31, log(P):  494.3\n",
      "f_snow: 2.968\n",
      " f_ice: 10.846\n",
      " refreeze: 0.752\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 18100, acc. rate: 0.26, log(P):  493.7\n",
      "f_snow: 5.525\n",
      " f_ice: 11.173\n",
      " refreeze: 0.809\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 18200, acc. rate: 0.33, log(P):  493.1\n",
      "f_snow: 5.399\n",
      " f_ice: 11.274\n",
      " refreeze: 0.631\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 18300, acc. rate: 0.28, log(P):  494.0\n",
      "f_snow: 5.049\n",
      " f_ice: 9.805\n",
      " refreeze: 0.407\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 18400, acc. rate: 0.26, log(P):  496.3\n",
      "f_snow: 5.220\n",
      " f_ice: 10.366\n",
      " refreeze: 0.083\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 18500, acc. rate: 0.27, log(P):  494.1\n",
      "f_snow: 5.156\n",
      " f_ice: 9.482\n",
      " refreeze: 0.737\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 18600, acc. rate: 0.35, log(P):  495.8\n",
      "f_snow: 5.800\n",
      " f_ice: 8.500\n",
      " refreeze: 0.617\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 18700, acc. rate: 0.30, log(P):  494.1\n",
      "f_snow: 5.302\n",
      " f_ice: 11.602\n",
      " refreeze: 0.736\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 18800, acc. rate: 0.33, log(P):  495.4\n",
      "f_snow: 2.597\n",
      " f_ice: 11.002\n",
      " refreeze: 0.794\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 18900, acc. rate: 0.33, log(P):  497.1\n",
      "f_snow: 4.243\n",
      " f_ice: 8.387\n",
      " refreeze: 0.936\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 19000, acc. rate: 0.26, log(P):  493.9\n",
      "f_snow: 4.206\n",
      " f_ice: 11.501\n",
      " refreeze: 0.845\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 19100, acc. rate: 0.28, log(P):  494.6\n",
      "f_snow: 4.588\n",
      " f_ice: 9.837\n",
      " refreeze: 0.909\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 19200, acc. rate: 0.32, log(P):  495.9\n",
      "f_snow: 5.746\n",
      " f_ice: 11.807\n",
      " refreeze: 0.685\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 19300, acc. rate: 0.27, log(P):  494.2\n",
      "f_snow: 5.609\n",
      " f_ice: 11.595\n",
      " refreeze: 0.414\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 19400, acc. rate: 0.24, log(P):  493.5\n",
      "f_snow: 3.848\n",
      " f_ice: 10.873\n",
      " refreeze: 0.624\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 19500, acc. rate: 0.34, log(P):  494.9\n",
      "f_snow: 5.618\n",
      " f_ice: 11.738\n",
      " refreeze: 0.712\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 19600, acc. rate: 0.31, log(P):  495.3\n",
      "f_snow: 5.651\n",
      " f_ice: 9.200\n",
      " refreeze: 0.272\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 19700, acc. rate: 0.34, log(P):  493.6\n",
      "f_snow: 5.669\n",
      " f_ice: 11.259\n",
      " refreeze: 0.697\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 19800, acc. rate: 0.24, log(P):  493.1\n",
      "f_snow: 5.298\n",
      " f_ice: 10.147\n",
      " refreeze: 0.567\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 19900, acc. rate: 0.26, log(P):  493.5\n",
      "f_snow: 4.871\n",
      " f_ice: 11.159\n",
      " refreeze: 0.593\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 20000, acc. rate: 0.29, log(P):  493.3\n",
      "f_snow: 5.529\n",
      " f_ice: 9.934\n",
      " refreeze: 0.542\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 20100, acc. rate: 0.32, log(P):  494.2\n",
      "f_snow: 4.577\n",
      " f_ice: 10.536\n",
      " refreeze: 0.490\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 20200, acc. rate: 0.35, log(P):  494.7\n",
      "f_snow: 4.158\n",
      " f_ice: 9.756\n",
      " refreeze: 0.535\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 20300, acc. rate: 0.38, log(P):  492.9\n",
      "f_snow: 5.441\n",
      " f_ice: 11.055\n",
      " refreeze: 0.539\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 20400, acc. rate: 0.28, log(P):  493.9\n",
      "f_snow: 5.191\n",
      " f_ice: 11.049\n",
      " refreeze: 0.256\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 20500, acc. rate: 0.29, log(P):  494.6\n",
      "f_snow: 3.745\n",
      " f_ice: 10.278\n",
      " refreeze: 0.921\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 20600, acc. rate: 0.29, log(P):  495.0\n",
      "f_snow: 5.681\n",
      " f_ice: 9.596\n",
      " refreeze: 0.865\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 20700, acc. rate: 0.26, log(P):  493.0\n",
      "f_snow: 5.444\n",
      " f_ice: 10.344\n",
      " refreeze: 0.507\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 20800, acc. rate: 0.33, log(P):  493.6\n",
      "f_snow: 3.746\n",
      " f_ice: 10.365\n",
      " refreeze: 0.824\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 20900, acc. rate: 0.35, log(P):  492.8\n",
      "f_snow: 5.464\n",
      " f_ice: 10.948\n",
      " refreeze: 0.584\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 21000, acc. rate: 0.33, log(P):  496.0\n",
      "f_snow: 3.580\n",
      " f_ice: 9.961\n",
      " refreeze: 0.366\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 21100, acc. rate: 0.28, log(P):  493.6\n",
      "f_snow: 5.586\n",
      " f_ice: 11.470\n",
      " refreeze: 0.535\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 21200, acc. rate: 0.32, log(P):  493.9\n",
      "f_snow: 5.043\n",
      " f_ice: 9.637\n",
      " refreeze: 0.659\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 21300, acc. rate: 0.31, log(P):  493.3\n",
      "f_snow: 4.370\n",
      " f_ice: 10.782\n",
      " refreeze: 0.677\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 21400, acc. rate: 0.32, log(P):  494.4\n",
      "f_snow: 4.772\n",
      " f_ice: 10.239\n",
      " refreeze: 0.910\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 21500, acc. rate: 0.26, log(P):  493.3\n",
      "f_snow: 4.476\n",
      " f_ice: 10.933\n",
      " refreeze: 0.836\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 21600, acc. rate: 0.30, log(P):  493.4\n",
      "f_snow: 5.256\n",
      " f_ice: 11.420\n",
      " refreeze: 0.618\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 21700, acc. rate: 0.32, log(P):  493.0\n",
      "f_snow: 5.589\n",
      " f_ice: 10.694\n",
      " refreeze: 0.606\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 21800, acc. rate: 0.30, log(P):  494.5\n",
      "f_snow: 3.934\n",
      " f_ice: 11.432\n",
      " refreeze: 0.913\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 21900, acc. rate: 0.36, log(P):  492.9\n",
      "f_snow: 5.471\n",
      " f_ice: 10.706\n",
      " refreeze: 0.649\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 22000, acc. rate: 0.27, log(P):  494.0\n",
      "f_snow: 5.799\n",
      " f_ice: 10.174\n",
      " refreeze: 0.643\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 22100, acc. rate: 0.30, log(P):  495.5\n",
      "f_snow: 5.834\n",
      " f_ice: 10.596\n",
      " refreeze: 0.201\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 22200, acc. rate: 0.31, log(P):  495.7\n",
      "f_snow: 4.923\n",
      " f_ice: 10.976\n",
      " refreeze: 0.120\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 22300, acc. rate: 0.27, log(P):  493.7\n",
      "f_snow: 3.964\n",
      " f_ice: 10.649\n",
      " refreeze: 0.574\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 22400, acc. rate: 0.22, log(P):  493.5\n",
      "f_snow: 4.642\n",
      " f_ice: 10.948\n",
      " refreeze: 0.648\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 22500, acc. rate: 0.29, log(P):  493.1\n",
      "f_snow: 5.490\n",
      " f_ice: 11.152\n",
      " refreeze: 0.472\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 22600, acc. rate: 0.27, log(P):  496.8\n",
      "f_snow: 2.479\n",
      " f_ice: 11.641\n",
      " refreeze: 0.813\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 22700, acc. rate: 0.28, log(P):  493.3\n",
      "f_snow: 5.156\n",
      " f_ice: 10.213\n",
      " refreeze: 0.672\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 22800, acc. rate: 0.24, log(P):  494.2\n",
      "f_snow: 5.474\n",
      " f_ice: 11.324\n",
      " refreeze: 0.241\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 22900, acc. rate: 0.31, log(P):  496.2\n",
      "f_snow: 5.254\n",
      " f_ice: 9.947\n",
      " refreeze: 0.101\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 23000, acc. rate: 0.27, log(P):  495.1\n",
      "f_snow: 3.869\n",
      " f_ice: 11.525\n",
      " refreeze: 0.432\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 23100, acc. rate: 0.23, log(P):  494.7\n",
      "f_snow: 5.870\n",
      " f_ice: 10.362\n",
      " refreeze: 0.439\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 23200, acc. rate: 0.30, log(P):  493.2\n",
      "f_snow: 4.348\n",
      " f_ice: 10.495\n",
      " refreeze: 0.772\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 23300, acc. rate: 0.30, log(P):  493.5\n",
      "f_snow: 4.973\n",
      " f_ice: 10.877\n",
      " refreeze: 0.719\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 23400, acc. rate: 0.26, log(P):  493.4\n",
      "f_snow: 4.596\n",
      " f_ice: 10.940\n",
      " refreeze: 0.663\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 23500, acc. rate: 0.29, log(P):  495.2\n",
      "f_snow: 5.166\n",
      " f_ice: 8.619\n",
      " refreeze: 0.472\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 23600, acc. rate: 0.31, log(P):  494.6\n",
      "f_snow: 4.430\n",
      " f_ice: 10.062\n",
      " refreeze: 0.488\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 23700, acc. rate: 0.27, log(P):  494.7\n",
      "f_snow: 5.398\n",
      " f_ice: 9.990\n",
      " refreeze: 0.890\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 23800, acc. rate: 0.29, log(P):  493.4\n",
      "f_snow: 5.146\n",
      " f_ice: 10.423\n",
      " refreeze: 0.382\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 23900, acc. rate: 0.30, log(P):  494.3\n",
      "f_snow: 2.968\n",
      " f_ice: 10.986\n",
      " refreeze: 0.742\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 24000, acc. rate: 0.38, log(P):  493.6\n",
      "f_snow: 5.494\n",
      " f_ice: 11.443\n",
      " refreeze: 0.703\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 24100, acc. rate: 0.33, log(P):  493.6\n",
      "f_snow: 5.026\n",
      " f_ice: 11.383\n",
      " refreeze: 0.457\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 24200, acc. rate: 0.23, log(P):  494.7\n",
      "f_snow: 2.893\n",
      " f_ice: 11.189\n",
      " refreeze: 0.841\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 24300, acc. rate: 0.37, log(P):  493.8\n",
      "f_snow: 3.677\n",
      " f_ice: 11.198\n",
      " refreeze: 0.864\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 24400, acc. rate: 0.32, log(P):  494.5\n",
      "f_snow: 3.426\n",
      " f_ice: 9.945\n",
      " refreeze: 0.636\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 24500, acc. rate: 0.28, log(P):  494.3\n",
      "f_snow: 2.992\n",
      " f_ice: 11.026\n",
      " refreeze: 0.774\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 24600, acc. rate: 0.31, log(P):  494.3\n",
      "f_snow: 5.241\n",
      " f_ice: 9.422\n",
      " refreeze: 0.373\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 24700, acc. rate: 0.28, log(P):  493.3\n",
      "f_snow: 5.126\n",
      " f_ice: 10.154\n",
      " refreeze: 0.497\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 24800, acc. rate: 0.32, log(P):  496.7\n",
      "f_snow: 5.326\n",
      " f_ice: 8.043\n",
      " refreeze: 0.851\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 24900, acc. rate: 0.32, log(P):  493.0\n",
      "f_snow: 5.612\n",
      " f_ice: 10.903\n",
      " refreeze: 0.532\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 25000, acc. rate: 0.31, log(P):  494.0\n",
      "f_snow: 5.414\n",
      " f_ice: 10.958\n",
      " refreeze: 0.858\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 25100, acc. rate: 0.26, log(P):  494.3\n",
      "f_snow: 5.444\n",
      " f_ice: 9.042\n",
      " refreeze: 0.681\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 25200, acc. rate: 0.27, log(P):  495.9\n",
      "f_snow: 5.723\n",
      " f_ice: 11.299\n",
      " refreeze: 0.922\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 25300, acc. rate: 0.30, log(P):  493.2\n",
      "f_snow: 5.095\n",
      " f_ice: 10.253\n",
      " refreeze: 0.604\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 25400, acc. rate: 0.29, log(P):  495.1\n",
      "f_snow: 5.835\n",
      " f_ice: 9.225\n",
      " refreeze: 0.627\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 25500, acc. rate: 0.28, log(P):  495.9\n",
      "f_snow: 4.949\n",
      " f_ice: 10.101\n",
      " refreeze: 0.134\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 25600, acc. rate: 0.29, log(P):  494.2\n",
      "f_snow: 5.613\n",
      " f_ice: 11.561\n",
      " refreeze: 0.367\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 25700, acc. rate: 0.26, log(P):  493.4\n",
      "f_snow: 4.181\n",
      " f_ice: 10.965\n",
      " refreeze: 0.629\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 25800, acc. rate: 0.29, log(P):  493.3\n",
      "f_snow: 4.391\n",
      " f_ice: 10.643\n",
      " refreeze: 0.709\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 25900, acc. rate: 0.27, log(P):  494.7\n",
      "f_snow: 4.693\n",
      " f_ice: 11.445\n",
      " refreeze: 0.917\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 26000, acc. rate: 0.26, log(P):  493.9\n",
      "f_snow: 4.952\n",
      " f_ice: 11.499\n",
      " refreeze: 0.523\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 26100, acc. rate: 0.30, log(P):  496.0\n",
      "f_snow: 5.785\n",
      " f_ice: 9.664\n",
      " refreeze: 0.191\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 26200, acc. rate: 0.32, log(P):  493.4\n",
      "f_snow: 5.526\n",
      " f_ice: 10.501\n",
      " refreeze: 0.783\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 26300, acc. rate: 0.33, log(P):  493.3\n",
      "f_snow: 3.806\n",
      " f_ice: 10.855\n",
      " refreeze: 0.805\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 26400, acc. rate: 0.30, log(P):  494.8\n",
      "f_snow: 4.286\n",
      " f_ice: 9.075\n",
      " refreeze: 0.697\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 26500, acc. rate: 0.29, log(P):  493.9\n",
      "f_snow: 3.233\n",
      " f_ice: 10.782\n",
      " refreeze: 0.788\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 26600, acc. rate: 0.32, log(P):  494.7\n",
      "f_snow: 5.673\n",
      " f_ice: 11.550\n",
      " refreeze: 0.810\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 26700, acc. rate: 0.29, log(P):  494.9\n",
      "f_snow: 5.278\n",
      " f_ice: 9.004\n",
      " refreeze: 0.800\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 26800, acc. rate: 0.30, log(P):  495.2\n",
      "f_snow: 3.889\n",
      " f_ice: 11.540\n",
      " refreeze: 0.936\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 26900, acc. rate: 0.30, log(P):  492.9\n",
      "f_snow: 5.372\n",
      " f_ice: 11.053\n",
      " refreeze: 0.511\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 27000, acc. rate: 0.26, log(P):  495.0\n",
      "f_snow: 4.195\n",
      " f_ice: 8.956\n",
      " refreeze: 0.737\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 27100, acc. rate: 0.25, log(P):  494.1\n",
      "f_snow: 4.111\n",
      " f_ice: 10.337\n",
      " refreeze: 0.905\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 27200, acc. rate: 0.35, log(P):  494.2\n",
      "f_snow: 5.722\n",
      " f_ice: 10.162\n",
      " refreeze: 0.818\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 27300, acc. rate: 0.30, log(P):  494.8\n",
      "f_snow: 5.626\n",
      " f_ice: 10.710\n",
      " refreeze: 0.898\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 27400, acc. rate: 0.25, log(P):  493.4\n",
      "f_snow: 5.476\n",
      " f_ice: 9.849\n",
      " refreeze: 0.678\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 27500, acc. rate: 0.27, log(P):  493.3\n",
      "f_snow: 3.916\n",
      " f_ice: 10.744\n",
      " refreeze: 0.790\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 27600, acc. rate: 0.30, log(P):  494.2\n",
      "f_snow: 3.838\n",
      " f_ice: 10.164\n",
      " refreeze: 0.574\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 27700, acc. rate: 0.34, log(P):  496.4\n",
      "f_snow: 3.400\n",
      " f_ice: 11.223\n",
      " refreeze: 0.271\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 27800, acc. rate: 0.30, log(P):  493.3\n",
      "f_snow: 5.169\n",
      " f_ice: 9.996\n",
      " refreeze: 0.595\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 27900, acc. rate: 0.29, log(P):  493.9\n",
      "f_snow: 3.567\n",
      " f_ice: 11.123\n",
      " refreeze: 0.874\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 28000, acc. rate: 0.29, log(P):  495.9\n",
      "f_snow: 2.894\n",
      " f_ice: 11.310\n",
      " refreeze: 0.433\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 28100, acc. rate: 0.26, log(P):  493.4\n",
      "f_snow: 5.671\n",
      " f_ice: 10.211\n",
      " refreeze: 0.504\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 28200, acc. rate: 0.29, log(P):  493.9\n",
      "f_snow: 4.455\n",
      " f_ice: 11.065\n",
      " refreeze: 0.898\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 28300, acc. rate: 0.32, log(P):  494.9\n",
      "f_snow: 5.893\n",
      " f_ice: 10.931\n",
      " refreeze: 0.487\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 28400, acc. rate: 0.25, log(P):  493.6\n",
      "f_snow: 4.681\n",
      " f_ice: 10.154\n",
      " refreeze: 0.747\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 28500, acc. rate: 0.25, log(P):  495.6\n",
      "f_snow: 4.939\n",
      " f_ice: 9.164\n",
      " refreeze: 0.289\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 28600, acc. rate: 0.28, log(P):  494.4\n",
      "f_snow: 4.747\n",
      " f_ice: 11.464\n",
      " refreeze: 0.402\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 28700, acc. rate: 0.31, log(P):  493.0\n",
      "f_snow: 5.408\n",
      " f_ice: 10.345\n",
      " refreeze: 0.642\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 28800, acc. rate: 0.27, log(P):  493.6\n",
      "f_snow: 5.478\n",
      " f_ice: 9.580\n",
      " refreeze: 0.606\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 28900, acc. rate: 0.27, log(P):  495.9\n",
      "f_snow: 5.678\n",
      " f_ice: 11.593\n",
      " refreeze: 0.899\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 29000, acc. rate: 0.32, log(P):  493.2\n",
      "f_snow: 5.511\n",
      " f_ice: 11.287\n",
      " refreeze: 0.485\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 29100, acc. rate: 0.28, log(P):  493.8\n",
      "f_snow: 5.356\n",
      " f_ice: 9.552\n",
      " refreeze: 0.468\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 29200, acc. rate: 0.24, log(P):  493.4\n",
      "f_snow: 4.247\n",
      " f_ice: 10.924\n",
      " refreeze: 0.853\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 29300, acc. rate: 0.32, log(P):  495.0\n",
      "f_snow: 4.667\n",
      " f_ice: 10.269\n",
      " refreeze: 0.273\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 29400, acc. rate: 0.33, log(P):  493.4\n",
      "f_snow: 3.655\n",
      " f_ice: 10.888\n",
      " refreeze: 0.740\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 29500, acc. rate: 0.36, log(P):  494.0\n",
      "f_snow: 4.912\n",
      " f_ice: 9.805\n",
      " refreeze: 0.496\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 29600, acc. rate: 0.31, log(P):  493.9\n",
      "f_snow: 5.776\n",
      " f_ice: 10.534\n",
      " refreeze: 0.726\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 29700, acc. rate: 0.28, log(P):  495.2\n",
      "f_snow: 5.081\n",
      " f_ice: 11.508\n",
      " refreeze: 0.912\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 29800, acc. rate: 0.30, log(P):  494.4\n",
      "f_snow: 5.779\n",
      " f_ice: 9.512\n",
      " refreeze: 0.505\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 29900, acc. rate: 0.33, log(P):  493.4\n",
      "f_snow: 3.755\n",
      " f_ice: 10.945\n",
      " refreeze: 0.700\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 30000, acc. rate: 0.31, log(P):  493.9\n",
      "f_snow: 3.594\n",
      " f_ice: 11.449\n",
      " refreeze: 0.703\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 30100, acc. rate: 0.27, log(P):  494.4\n",
      "f_snow: 5.520\n",
      " f_ice: 10.313\n",
      " refreeze: 0.213\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 30200, acc. rate: 0.23, log(P):  493.2\n",
      "f_snow: 4.478\n",
      " f_ice: 10.714\n",
      " refreeze: 0.712\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 30300, acc. rate: 0.29, log(P):  496.5\n",
      "f_snow: 4.337\n",
      " f_ice: 9.959\n",
      " refreeze: 0.971\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 30400, acc. rate: 0.27, log(P):  495.3\n",
      "f_snow: 3.359\n",
      " f_ice: 9.745\n",
      " refreeze: 0.531\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 30500, acc. rate: 0.33, log(P):  494.7\n",
      "f_snow: 4.984\n",
      " f_ice: 9.247\n",
      " refreeze: 0.719\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 30600, acc. rate: 0.27, log(P):  495.7\n",
      "f_snow: 4.519\n",
      " f_ice: 8.885\n",
      " refreeze: 0.905\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 30700, acc. rate: 0.25, log(P):  494.9\n",
      "f_snow: 5.261\n",
      " f_ice: 11.757\n",
      " refreeze: 0.387\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 30800, acc. rate: 0.29, log(P):  493.5\n",
      "f_snow: 5.051\n",
      " f_ice: 11.425\n",
      " refreeze: 0.543\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 30900, acc. rate: 0.27, log(P):  494.3\n",
      "f_snow: 4.670\n",
      " f_ice: 10.683\n",
      " refreeze: 0.346\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 31000, acc. rate: 0.29, log(P):  495.0\n",
      "f_snow: 4.946\n",
      " f_ice: 10.577\n",
      " refreeze: 0.177\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 31100, acc. rate: 0.27, log(P):  495.4\n",
      "f_snow: 2.778\n",
      " f_ice: 10.700\n",
      " refreeze: 0.893\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 31200, acc. rate: 0.30, log(P):  494.6\n",
      "f_snow: 5.785\n",
      " f_ice: 10.936\n",
      " refreeze: 0.273\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 31300, acc. rate: 0.30, log(P):  493.3\n",
      "f_snow: 3.965\n",
      " f_ice: 10.959\n",
      " refreeze: 0.702\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 31400, acc. rate: 0.27, log(P):  493.7\n",
      "f_snow: 4.833\n",
      " f_ice: 10.306\n",
      " refreeze: 0.693\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 31500, acc. rate: 0.29, log(P):  493.9\n",
      "f_snow: 4.249\n",
      " f_ice: 10.559\n",
      " refreeze: 0.904\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 31600, acc. rate: 0.30, log(P):  493.7\n",
      "f_snow: 5.650\n",
      " f_ice: 11.073\n",
      " refreeze: 0.778\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 31700, acc. rate: 0.31, log(P):  494.2\n",
      "f_snow: 5.439\n",
      " f_ice: 11.159\n",
      " refreeze: 0.217\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 31800, acc. rate: 0.31, log(P):  494.2\n",
      "f_snow: 3.371\n",
      " f_ice: 10.778\n",
      " refreeze: 0.884\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 31900, acc. rate: 0.27, log(P):  493.5\n",
      "f_snow: 3.567\n",
      " f_ice: 10.785\n",
      " refreeze: 0.695\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 32000, acc. rate: 0.29, log(P):  494.9\n",
      "f_snow: 5.539\n",
      " f_ice: 10.489\n",
      " refreeze: 0.156\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 32100, acc. rate: 0.32, log(P):  493.8\n",
      "f_snow: 4.710\n",
      " f_ice: 11.277\n",
      " refreeze: 0.615\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 32200, acc. rate: 0.38, log(P):  493.5\n",
      "f_snow: 3.943\n",
      " f_ice: 11.418\n",
      " refreeze: 0.749\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 32300, acc. rate: 0.29, log(P):  494.3\n",
      "f_snow: 5.558\n",
      " f_ice: 9.909\n",
      " refreeze: 0.844\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 32400, acc. rate: 0.31, log(P):  492.9\n",
      "f_snow: 5.555\n",
      " f_ice: 10.774\n",
      " refreeze: 0.513\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 32500, acc. rate: 0.29, log(P):  494.3\n",
      "f_snow: 5.432\n",
      " f_ice: 10.074\n",
      " refreeze: 0.868\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 32600, acc. rate: 0.30, log(P):  494.8\n",
      "f_snow: 5.260\n",
      " f_ice: 10.979\n",
      " refreeze: 0.906\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 32700, acc. rate: 0.20, log(P):  495.7\n",
      "f_snow: 4.039\n",
      " f_ice: 9.757\n",
      " refreeze: 0.951\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 32800, acc. rate: 0.26, log(P):  494.4\n",
      "f_snow: 4.728\n",
      " f_ice: 11.642\n",
      " refreeze: 0.780\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 32900, acc. rate: 0.34, log(P):  497.5\n",
      "f_snow: 2.719\n",
      " f_ice: 11.783\n",
      " refreeze: 0.489\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 33000, acc. rate: 0.30, log(P):  492.9\n",
      "f_snow: 5.198\n",
      " f_ice: 10.903\n",
      " refreeze: 0.531\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 33100, acc. rate: 0.26, log(P):  493.4\n",
      "f_snow: 5.583\n",
      " f_ice: 10.566\n",
      " refreeze: 0.775\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 33200, acc. rate: 0.30, log(P):  493.7\n",
      "f_snow: 5.102\n",
      " f_ice: 11.424\n",
      " refreeze: 0.707\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 33300, acc. rate: 0.25, log(P):  494.3\n",
      "f_snow: 5.712\n",
      " f_ice: 10.251\n",
      " refreeze: 0.839\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 33400, acc. rate: 0.32, log(P):  494.3\n",
      "f_snow: 3.037\n",
      " f_ice: 11.207\n",
      " refreeze: 0.714\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 33500, acc. rate: 0.36, log(P):  495.4\n",
      "f_snow: 3.599\n",
      " f_ice: 10.618\n",
      " refreeze: 0.954\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 33600, acc. rate: 0.30, log(P):  493.7\n",
      "f_snow: 3.564\n",
      " f_ice: 11.234\n",
      " refreeze: 0.677\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 33700, acc. rate: 0.30, log(P):  493.1\n",
      "f_snow: 5.032\n",
      " f_ice: 10.698\n",
      " refreeze: 0.573\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 33800, acc. rate: 0.23, log(P):  493.8\n",
      "f_snow: 5.800\n",
      " f_ice: 10.801\n",
      " refreeze: 0.601\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 33900, acc. rate: 0.36, log(P):  494.8\n",
      "f_snow: 4.873\n",
      " f_ice: 11.624\n",
      " refreeze: 0.365\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 34000, acc. rate: 0.39, log(P):  493.7\n",
      "f_snow: 5.755\n",
      " f_ice: 10.112\n",
      " refreeze: 0.594\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 34100, acc. rate: 0.30, log(P):  494.3\n",
      "f_snow: 5.400\n",
      " f_ice: 11.006\n",
      " refreeze: 0.198\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 34200, acc. rate: 0.29, log(P):  493.7\n",
      "f_snow: 5.299\n",
      " f_ice: 11.424\n",
      " refreeze: 0.364\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 34300, acc. rate: 0.34, log(P):  494.4\n",
      "f_snow: 5.650\n",
      " f_ice: 9.775\n",
      " refreeze: 0.828\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 34400, acc. rate: 0.33, log(P):  493.8\n",
      "f_snow: 5.756\n",
      " f_ice: 10.072\n",
      " refreeze: 0.541\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 34500, acc. rate: 0.29, log(P):  494.4\n",
      "f_snow: 5.745\n",
      " f_ice: 11.558\n",
      " refreeze: 0.669\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 34600, acc. rate: 0.29, log(P):  494.9\n",
      "f_snow: 3.983\n",
      " f_ice: 11.726\n",
      " refreeze: 0.857\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 34700, acc. rate: 0.24, log(P):  494.2\n",
      "f_snow: 5.341\n",
      " f_ice: 9.083\n",
      " refreeze: 0.562\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 34800, acc. rate: 0.31, log(P):  494.0\n",
      "f_snow: 5.529\n",
      " f_ice: 10.911\n",
      " refreeze: 0.232\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 34900, acc. rate: 0.32, log(P):  493.5\n",
      "f_snow: 5.254\n",
      " f_ice: 9.769\n",
      " refreeze: 0.571\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 35000, acc. rate: 0.26, log(P):  493.8\n",
      "f_snow: 4.724\n",
      " f_ice: 11.067\n",
      " refreeze: 0.546\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 35100, acc. rate: 0.28, log(P):  492.9\n",
      "f_snow: 5.299\n",
      " f_ice: 10.558\n",
      " refreeze: 0.647\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 35200, acc. rate: 0.23, log(P):  493.6\n",
      "f_snow: 4.054\n",
      " f_ice: 10.281\n",
      " refreeze: 0.848\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 35300, acc. rate: 0.27, log(P):  497.7\n",
      "f_snow: 5.960\n",
      " f_ice: 10.005\n",
      " refreeze: 0.818\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 35400, acc. rate: 0.30, log(P):  497.8\n",
      "f_snow: 3.515\n",
      " f_ice: 7.998\n",
      " refreeze: 0.569\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 35500, acc. rate: 0.29, log(P):  494.7\n",
      "f_snow: 4.807\n",
      " f_ice: 9.463\n",
      " refreeze: 0.493\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 35600, acc. rate: 0.31, log(P):  493.6\n",
      "f_snow: 5.199\n",
      " f_ice: 9.674\n",
      " refreeze: 0.604\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 35700, acc. rate: 0.32, log(P):  496.0\n",
      "f_snow: 5.705\n",
      " f_ice: 10.358\n",
      " refreeze: 0.938\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 35800, acc. rate: 0.30, log(P):  493.3\n",
      "f_snow: 5.561\n",
      " f_ice: 9.933\n",
      " refreeze: 0.636\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 35900, acc. rate: 0.26, log(P):  493.3\n",
      "f_snow: 5.511\n",
      " f_ice: 11.421\n",
      " refreeze: 0.583\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 36000, acc. rate: 0.33, log(P):  495.1\n",
      "f_snow: 4.779\n",
      " f_ice: 11.721\n",
      " refreeze: 0.621\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 36100, acc. rate: 0.28, log(P):  493.9\n",
      "f_snow: 5.248\n",
      " f_ice: 11.385\n",
      " refreeze: 0.303\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 36200, acc. rate: 0.28, log(P):  493.7\n",
      "f_snow: 4.567\n",
      " f_ice: 10.092\n",
      " refreeze: 0.810\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 36300, acc. rate: 0.25, log(P):  493.0\n",
      "f_snow: 5.486\n",
      " f_ice: 10.394\n",
      " refreeze: 0.511\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 36400, acc. rate: 0.28, log(P):  492.8\n",
      "f_snow: 5.396\n",
      " f_ice: 10.791\n",
      " refreeze: 0.588\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 36500, acc. rate: 0.28, log(P):  494.2\n",
      "f_snow: 4.529\n",
      " f_ice: 10.855\n",
      " refreeze: 0.491\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 36600, acc. rate: 0.33, log(P):  495.7\n",
      "f_snow: 4.747\n",
      " f_ice: 11.397\n",
      " refreeze: 0.168\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 36700, acc. rate: 0.32, log(P):  493.0\n",
      "f_snow: 5.164\n",
      " f_ice: 10.646\n",
      " refreeze: 0.665\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 36800, acc. rate: 0.39, log(P):  496.2\n",
      "f_snow: 5.443\n",
      " f_ice: 11.755\n",
      " refreeze: 0.888\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 36900, acc. rate: 0.34, log(P):  494.4\n",
      "f_snow: 5.823\n",
      " f_ice: 11.029\n",
      " refreeze: 0.764\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 37000, acc. rate: 0.26, log(P):  493.2\n",
      "f_snow: 5.676\n",
      " f_ice: 10.738\n",
      " refreeze: 0.495\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 37100, acc. rate: 0.29, log(P):  493.8\n",
      "f_snow: 5.559\n",
      " f_ice: 11.543\n",
      " refreeze: 0.488\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 37200, acc. rate: 0.33, log(P):  497.9\n",
      "f_snow: 5.946\n",
      " f_ice: 10.850\n",
      " refreeze: 0.173\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 37300, acc. rate: 0.30, log(P):  492.8\n",
      "f_snow: 5.427\n",
      " f_ice: 10.926\n",
      " refreeze: 0.562\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 37400, acc. rate: 0.29, log(P):  493.7\n",
      "f_snow: 5.441\n",
      " f_ice: 11.554\n",
      " refreeze: 0.514\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 37500, acc. rate: 0.23, log(P):  495.1\n",
      "f_snow: 4.855\n",
      " f_ice: 9.855\n",
      " refreeze: 0.261\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 37600, acc. rate: 0.30, log(P):  495.3\n",
      "f_snow: 3.225\n",
      " f_ice: 11.747\n",
      " refreeze: 0.740\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 37700, acc. rate: 0.29, log(P):  493.1\n",
      "f_snow: 5.400\n",
      " f_ice: 10.611\n",
      " refreeze: 0.404\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 37800, acc. rate: 0.33, log(P):  498.2\n",
      "f_snow: 3.153\n",
      " f_ice: 9.583\n",
      " refreeze: 0.259\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 37900, acc. rate: 0.24, log(P):  493.1\n",
      "f_snow: 5.304\n",
      " f_ice: 10.084\n",
      " refreeze: 0.618\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 38000, acc. rate: 0.28, log(P):  497.8\n",
      "f_snow: 5.720\n",
      " f_ice: 11.523\n",
      " refreeze: 0.063\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 38100, acc. rate: 0.26, log(P):  493.1\n",
      "f_snow: 5.343\n",
      " f_ice: 10.423\n",
      " refreeze: 0.715\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 38200, acc. rate: 0.30, log(P):  494.1\n",
      "f_snow: 5.288\n",
      " f_ice: 11.341\n",
      " refreeze: 0.832\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 38300, acc. rate: 0.36, log(P):  493.0\n",
      "f_snow: 5.300\n",
      " f_ice: 10.787\n",
      " refreeze: 0.441\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 38400, acc. rate: 0.31, log(P):  495.9\n",
      "f_snow: 5.630\n",
      " f_ice: 9.723\n",
      " refreeze: 0.145\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 38500, acc. rate: 0.30, log(P):  493.3\n",
      "f_snow: 3.890\n",
      " f_ice: 10.572\n",
      " refreeze: 0.764\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 38600, acc. rate: 0.29, log(P):  494.6\n",
      "f_snow: 5.004\n",
      " f_ice: 9.954\n",
      " refreeze: 0.276\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 38700, acc. rate: 0.27, log(P):  493.5\n",
      "f_snow: 5.755\n",
      " f_ice: 10.688\n",
      " refreeze: 0.649\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 38800, acc. rate: 0.31, log(P):  496.4\n",
      "f_snow: 3.203\n",
      " f_ice: 11.403\n",
      " refreeze: 0.312\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 38900, acc. rate: 0.27, log(P):  496.2\n",
      "f_snow: 5.284\n",
      " f_ice: 10.036\n",
      " refreeze: 0.099\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 39000, acc. rate: 0.24, log(P):  494.4\n",
      "f_snow: 5.870\n",
      " f_ice: 10.845\n",
      " refreeze: 0.580\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 39100, acc. rate: 0.32, log(P):  494.6\n",
      "f_snow: 2.972\n",
      " f_ice: 10.369\n",
      " refreeze: 0.702\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 39200, acc. rate: 0.28, log(P):  493.8\n",
      "f_snow: 5.754\n",
      " f_ice: 10.651\n",
      " refreeze: 0.740\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 39300, acc. rate: 0.26, log(P):  494.4\n",
      "f_snow: 4.691\n",
      " f_ice: 9.728\n",
      " refreeze: 0.880\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 39400, acc. rate: 0.23, log(P):  492.9\n",
      "f_snow: 5.178\n",
      " f_ice: 10.948\n",
      " refreeze: 0.588\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 39500, acc. rate: 0.27, log(P):  493.4\n",
      "f_snow: 4.112\n",
      " f_ice: 10.279\n",
      " refreeze: 0.717\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 39600, acc. rate: 0.29, log(P):  493.5\n",
      "f_snow: 4.984\n",
      " f_ice: 10.857\n",
      " refreeze: 0.740\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 39700, acc. rate: 0.32, log(P):  495.8\n",
      "f_snow: 5.734\n",
      " f_ice: 11.643\n",
      " refreeze: 0.858\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 39800, acc. rate: 0.30, log(P):  493.5\n",
      "f_snow: 4.921\n",
      " f_ice: 10.800\n",
      " refreeze: 0.745\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 39900, acc. rate: 0.37, log(P):  494.0\n",
      "f_snow: 3.840\n",
      " f_ice: 9.977\n",
      " refreeze: 0.642\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 40000, acc. rate: 0.32, log(P):  493.4\n",
      "f_snow: 3.670\n",
      " f_ice: 10.708\n",
      " refreeze: 0.777\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 40100, acc. rate: 0.32, log(P):  493.5\n",
      "f_snow: 5.727\n",
      " f_ice: 10.904\n",
      " refreeze: 0.693\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 40200, acc. rate: 0.27, log(P):  495.1\n",
      "f_snow: 5.022\n",
      " f_ice: 9.108\n",
      " refreeze: 0.359\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 40300, acc. rate: 0.28, log(P):  495.1\n",
      "f_snow: 5.455\n",
      " f_ice: 8.459\n",
      " refreeze: 0.652\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 40400, acc. rate: 0.27, log(P):  493.2\n",
      "f_snow: 5.587\n",
      " f_ice: 10.271\n",
      " refreeze: 0.459\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 40500, acc. rate: 0.36, log(P):  494.5\n",
      "f_snow: 5.643\n",
      " f_ice: 9.028\n",
      " refreeze: 0.503\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 40600, acc. rate: 0.29, log(P):  494.0\n",
      "f_snow: 4.817\n",
      " f_ice: 11.349\n",
      " refreeze: 0.865\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 40700, acc. rate: 0.30, log(P):  493.4\n",
      "f_snow: 5.705\n",
      " f_ice: 10.961\n",
      " refreeze: 0.442\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 40800, acc. rate: 0.28, log(P):  493.0\n",
      "f_snow: 5.597\n",
      " f_ice: 10.685\n",
      " refreeze: 0.567\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 40900, acc. rate: 0.27, log(P):  495.3\n",
      "f_snow: 5.766\n",
      " f_ice: 9.176\n",
      " refreeze: 0.815\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 41000, acc. rate: 0.35, log(P):  493.2\n",
      "f_snow: 5.009\n",
      " f_ice: 10.683\n",
      " refreeze: 0.563\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 41100, acc. rate: 0.32, log(P):  493.2\n",
      "f_snow: 5.344\n",
      " f_ice: 11.125\n",
      " refreeze: 0.389\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 41200, acc. rate: 0.26, log(P):  495.0\n",
      "f_snow: 4.043\n",
      " f_ice: 11.657\n",
      " refreeze: 0.907\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 41300, acc. rate: 0.27, log(P):  493.6\n",
      "f_snow: 5.773\n",
      " f_ice: 10.909\n",
      " refreeze: 0.533\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 41400, acc. rate: 0.34, log(P):  494.6\n",
      "f_snow: 4.884\n",
      " f_ice: 10.029\n",
      " refreeze: 0.903\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 41500, acc. rate: 0.31, log(P):  493.4\n",
      "f_snow: 5.324\n",
      " f_ice: 10.472\n",
      " refreeze: 0.785\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 41600, acc. rate: 0.35, log(P):  492.8\n",
      "f_snow: 5.388\n",
      " f_ice: 10.815\n",
      " refreeze: 0.577\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 41700, acc. rate: 0.32, log(P):  494.8\n",
      "f_snow: 5.686\n",
      " f_ice: 11.344\n",
      " refreeze: 0.859\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 41800, acc. rate: 0.25, log(P):  494.3\n",
      "f_snow: 3.091\n",
      " f_ice: 10.382\n",
      " refreeze: 0.726\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 41900, acc. rate: 0.29, log(P):  494.3\n",
      "f_snow: 4.563\n",
      " f_ice: 10.696\n",
      " refreeze: 0.431\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 42000, acc. rate: 0.33, log(P):  495.8\n",
      "f_snow: 2.908\n",
      " f_ice: 11.162\n",
      " refreeze: 0.934\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 42100, acc. rate: 0.29, log(P):  493.0\n",
      "f_snow: 5.510\n",
      " f_ice: 10.985\n",
      " refreeze: 0.662\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 42200, acc. rate: 0.34, log(P):  494.7\n",
      "f_snow: 3.859\n",
      " f_ice: 10.159\n",
      " refreeze: 0.483\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 42300, acc. rate: 0.29, log(P):  494.6\n",
      "f_snow: 4.729\n",
      " f_ice: 10.810\n",
      " refreeze: 0.261\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 42400, acc. rate: 0.25, log(P):  497.0\n",
      "f_snow: 4.244\n",
      " f_ice: 11.533\n",
      " refreeze: 0.975\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 42500, acc. rate: 0.31, log(P):  493.6\n",
      "f_snow: 5.624\n",
      " f_ice: 11.193\n",
      " refreeze: 0.372\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 42600, acc. rate: 0.28, log(P):  493.6\n",
      "f_snow: 4.155\n",
      " f_ice: 10.028\n",
      " refreeze: 0.760\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 42700, acc. rate: 0.32, log(P):  495.0\n",
      "f_snow: 5.578\n",
      " f_ice: 11.449\n",
      " refreeze: 0.884\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 42800, acc. rate: 0.30, log(P):  493.7\n",
      "f_snow: 5.463\n",
      " f_ice: 9.894\n",
      " refreeze: 0.781\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 42900, acc. rate: 0.25, log(P):  494.5\n",
      "f_snow: 4.587\n",
      " f_ice: 9.949\n",
      " refreeze: 0.536\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 43000, acc. rate: 0.28, log(P):  495.6\n",
      "f_snow: 5.197\n",
      " f_ice: 10.890\n",
      " refreeze: 0.946\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 43100, acc. rate: 0.30, log(P):  494.4\n",
      "f_snow: 5.627\n",
      " f_ice: 9.576\n",
      " refreeze: 0.810\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 43200, acc. rate: 0.31, log(P):  494.1\n",
      "f_snow: 5.709\n",
      " f_ice: 10.155\n",
      " refreeze: 0.334\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 43300, acc. rate: 0.32, log(P):  493.3\n",
      "f_snow: 4.022\n",
      " f_ice: 10.508\n",
      " refreeze: 0.705\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 43400, acc. rate: 0.33, log(P):  496.6\n",
      "f_snow: 2.695\n",
      " f_ice: 9.651\n",
      " refreeze: 0.587\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 43500, acc. rate: 0.34, log(P):  493.6\n",
      "f_snow: 4.805\n",
      " f_ice: 10.690\n",
      " refreeze: 0.615\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 43600, acc. rate: 0.25, log(P):  494.3\n",
      "f_snow: 3.095\n",
      " f_ice: 11.102\n",
      " refreeze: 0.623\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 43700, acc. rate: 0.31, log(P):  495.3\n",
      "f_snow: 3.993\n",
      " f_ice: 11.791\n",
      " refreeze: 0.620\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 43800, acc. rate: 0.35, log(P):  493.2\n",
      "f_snow: 4.153\n",
      " f_ice: 10.849\n",
      " refreeze: 0.787\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 43900, acc. rate: 0.31, log(P):  495.0\n",
      "f_snow: 5.849\n",
      " f_ice: 9.581\n",
      " refreeze: 0.480\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 44000, acc. rate: 0.27, log(P):  493.7\n",
      "f_snow: 5.250\n",
      " f_ice: 10.390\n",
      " refreeze: 0.819\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 44100, acc. rate: 0.26, log(P):  493.4\n",
      "f_snow: 4.799\n",
      " f_ice: 10.697\n",
      " refreeze: 0.737\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 44200, acc. rate: 0.29, log(P):  495.6\n",
      "f_snow: 2.636\n",
      " f_ice: 11.256\n",
      " refreeze: 0.604\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 44300, acc. rate: 0.26, log(P):  493.7\n",
      "f_snow: 5.428\n",
      " f_ice: 10.180\n",
      " refreeze: 0.811\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 44400, acc. rate: 0.28, log(P):  493.6\n",
      "f_snow: 5.027\n",
      " f_ice: 10.041\n",
      " refreeze: 0.703\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 44500, acc. rate: 0.36, log(P):  495.0\n",
      "f_snow: 5.713\n",
      " f_ice: 9.305\n",
      " refreeze: 0.337\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 44600, acc. rate: 0.31, log(P):  493.7\n",
      "f_snow: 4.454\n",
      " f_ice: 10.018\n",
      " refreeze: 0.815\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 44700, acc. rate: 0.30, log(P):  493.1\n",
      "f_snow: 5.574\n",
      " f_ice: 10.494\n",
      " refreeze: 0.671\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 44800, acc. rate: 0.29, log(P):  493.4\n",
      "f_snow: 4.387\n",
      " f_ice: 11.259\n",
      " refreeze: 0.822\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 44900, acc. rate: 0.26, log(P):  494.6\n",
      "f_snow: 3.317\n",
      " f_ice: 11.567\n",
      " refreeze: 0.616\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 45000, acc. rate: 0.25, log(P):  493.5\n",
      "f_snow: 4.488\n",
      " f_ice: 10.732\n",
      " refreeze: 0.630\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 45100, acc. rate: 0.30, log(P):  493.1\n",
      "f_snow: 5.439\n",
      " f_ice: 11.301\n",
      " refreeze: 0.595\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 45200, acc. rate: 0.33, log(P):  493.8\n",
      "f_snow: 4.289\n",
      " f_ice: 9.797\n",
      " refreeze: 0.760\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 45300, acc. rate: 0.40, log(P):  493.4\n",
      "f_snow: 5.507\n",
      " f_ice: 11.352\n",
      " refreeze: 0.707\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 45400, acc. rate: 0.31, log(P):  495.9\n",
      "f_snow: 4.674\n",
      " f_ice: 11.391\n",
      " refreeze: 0.162\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 45500, acc. rate: 0.29, log(P):  495.6\n",
      "f_snow: 5.837\n",
      " f_ice: 9.115\n",
      " refreeze: 0.772\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 45600, acc. rate: 0.30, log(P):  493.8\n",
      "f_snow: 5.078\n",
      " f_ice: 9.815\n",
      " refreeze: 0.692\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 45700, acc. rate: 0.33, log(P):  494.2\n",
      "f_snow: 5.773\n",
      " f_ice: 11.449\n",
      " refreeze: 0.584\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 45800, acc. rate: 0.26, log(P):  494.5\n",
      "f_snow: 4.725\n",
      " f_ice: 11.508\n",
      " refreeze: 0.887\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 45900, acc. rate: 0.35, log(P):  494.7\n",
      "f_snow: 5.797\n",
      " f_ice: 11.573\n",
      " refreeze: 0.594\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 46000, acc. rate: 0.31, log(P):  494.8\n",
      "f_snow: 5.823\n",
      " f_ice: 10.865\n",
      " refreeze: 0.831\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 46100, acc. rate: 0.29, log(P):  493.4\n",
      "f_snow: 4.919\n",
      " f_ice: 10.396\n",
      " refreeze: 0.572\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 46200, acc. rate: 0.26, log(P):  493.0\n",
      "f_snow: 5.378\n",
      " f_ice: 10.645\n",
      " refreeze: 0.456\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 46300, acc. rate: 0.28, log(P):  496.6\n",
      "f_snow: 5.405\n",
      " f_ice: 7.925\n",
      " refreeze: 0.814\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 46400, acc. rate: 0.28, log(P):  494.3\n",
      "f_snow: 4.805\n",
      " f_ice: 9.807\n",
      " refreeze: 0.463\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 46500, acc. rate: 0.29, log(P):  493.1\n",
      "f_snow: 5.420\n",
      " f_ice: 10.959\n",
      " refreeze: 0.732\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 46600, acc. rate: 0.28, log(P):  494.6\n",
      "f_snow: 4.654\n",
      " f_ice: 10.606\n",
      " refreeze: 0.304\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 46700, acc. rate: 0.28, log(P):  493.4\n",
      "f_snow: 5.070\n",
      " f_ice: 10.199\n",
      " refreeze: 0.669\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 46800, acc. rate: 0.24, log(P):  494.8\n",
      "f_snow: 5.819\n",
      " f_ice: 9.771\n",
      " refreeze: 0.761\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 46900, acc. rate: 0.30, log(P):  492.8\n",
      "f_snow: 5.372\n",
      " f_ice: 10.684\n",
      " refreeze: 0.500\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 47000, acc. rate: 0.28, log(P):  495.2\n",
      "f_snow: 4.587\n",
      " f_ice: 10.081\n",
      " refreeze: 0.297\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 47100, acc. rate: 0.28, log(P):  493.3\n",
      "f_snow: 5.428\n",
      " f_ice: 10.549\n",
      " refreeze: 0.349\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 47200, acc. rate: 0.30, log(P):  494.6\n",
      "f_snow: 5.411\n",
      " f_ice: 10.868\n",
      " refreeze: 0.163\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 47300, acc. rate: 0.28, log(P):  494.3\n",
      "f_snow: 4.212\n",
      " f_ice: 9.534\n",
      " refreeze: 0.827\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 47400, acc. rate: 0.30, log(P):  492.9\n",
      "f_snow: 5.230\n",
      " f_ice: 11.004\n",
      " refreeze: 0.595\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 47500, acc. rate: 0.30, log(P):  494.3\n",
      "f_snow: 4.302\n",
      " f_ice: 11.619\n",
      " refreeze: 0.843\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 47600, acc. rate: 0.27, log(P):  495.6\n",
      "f_snow: 5.794\n",
      " f_ice: 11.342\n",
      " refreeze: 0.195\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 47700, acc. rate: 0.27, log(P):  498.9\n",
      "f_snow: 3.514\n",
      " f_ice: 11.708\n",
      " refreeze: 0.156\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 47800, acc. rate: 0.35, log(P):  495.9\n",
      "f_snow: 5.757\n",
      " f_ice: 11.717\n",
      " refreeze: 0.821\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 47900, acc. rate: 0.30, log(P):  494.3\n",
      "f_snow: 3.806\n",
      " f_ice: 11.514\n",
      " refreeze: 0.575\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 48000, acc. rate: 0.23, log(P):  493.2\n",
      "f_snow: 5.497\n",
      " f_ice: 9.990\n",
      " refreeze: 0.623\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 48100, acc. rate: 0.33, log(P):  493.7\n",
      "f_snow: 5.190\n",
      " f_ice: 11.336\n",
      " refreeze: 0.333\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 48200, acc. rate: 0.28, log(P):  498.6\n",
      "f_snow: 5.962\n",
      " f_ice: 9.647\n",
      " refreeze: 0.259\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 48300, acc. rate: 0.26, log(P):  495.9\n",
      "f_snow: 2.845\n",
      " f_ice: 9.747\n",
      " refreeze: 0.621\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 48400, acc. rate: 0.27, log(P):  494.2\n",
      "f_snow: 3.035\n",
      " f_ice: 11.026\n",
      " refreeze: 0.704\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 48500, acc. rate: 0.25, log(P):  493.6\n",
      "f_snow: 4.873\n",
      " f_ice: 10.370\n",
      " refreeze: 0.785\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 48600, acc. rate: 0.26, log(P):  494.2\n",
      "f_snow: 5.763\n",
      " f_ice: 10.180\n",
      " refreeze: 0.357\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 48700, acc. rate: 0.34, log(P):  493.6\n",
      "f_snow: 4.463\n",
      " f_ice: 10.539\n",
      " refreeze: 0.603\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 48800, acc. rate: 0.33, log(P):  493.0\n",
      "f_snow: 5.420\n",
      " f_ice: 11.180\n",
      " refreeze: 0.633\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 48900, acc. rate: 0.30, log(P):  493.9\n",
      "f_snow: 5.622\n",
      " f_ice: 9.904\n",
      " refreeze: 0.368\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 49000, acc. rate: 0.31, log(P):  494.2\n",
      "f_snow: 3.081\n",
      " f_ice: 10.647\n",
      " refreeze: 0.742\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 49100, acc. rate: 0.31, log(P):  494.3\n",
      "f_snow: 5.846\n",
      " f_ice: 10.714\n",
      " refreeze: 0.714\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 49200, acc. rate: 0.27, log(P):  493.5\n",
      "f_snow: 5.514\n",
      " f_ice: 9.789\n",
      " refreeze: 0.465\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 49300, acc. rate: 0.27, log(P):  493.6\n",
      "f_snow: 5.051\n",
      " f_ice: 9.869\n",
      " refreeze: 0.611\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 49400, acc. rate: 0.26, log(P):  494.3\n",
      "f_snow: 5.339\n",
      " f_ice: 9.464\n",
      " refreeze: 0.340\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 49500, acc. rate: 0.28, log(P):  493.1\n",
      "f_snow: 5.210\n",
      " f_ice: 10.286\n",
      " refreeze: 0.563\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 49600, acc. rate: 0.27, log(P):  493.5\n",
      "f_snow: 3.709\n",
      " f_ice: 10.887\n",
      " refreeze: 0.648\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 49700, acc. rate: 0.28, log(P):  494.5\n",
      "f_snow: 3.155\n",
      " f_ice: 11.143\n",
      " refreeze: 0.563\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 49800, acc. rate: 0.24, log(P):  493.5\n",
      "f_snow: 5.048\n",
      " f_ice: 11.185\n",
      " refreeze: 0.386\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 49900, acc. rate: 0.25, log(P):  493.5\n",
      "f_snow: 5.365\n",
      " f_ice: 11.496\n",
      " refreeze: 0.593\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 50000, acc. rate: 0.29, log(P):  494.1\n",
      "f_snow: 4.238\n",
      " f_ice: 10.076\n",
      " refreeze: 0.889\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 50100, acc. rate: 0.27, log(P):  494.8\n",
      "f_snow: 4.171\n",
      " f_ice: 11.633\n",
      " refreeze: 0.903\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 50200, acc. rate: 0.29, log(P):  495.0\n",
      "f_snow: 4.984\n",
      " f_ice: 11.639\n",
      " refreeze: 0.280\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 50300, acc. rate: 0.33, log(P):  493.0\n",
      "f_snow: 5.158\n",
      " f_ice: 10.445\n",
      " refreeze: 0.575\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 50400, acc. rate: 0.27, log(P):  493.3\n",
      "f_snow: 4.526\n",
      " f_ice: 11.123\n",
      " refreeze: 0.763\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 50500, acc. rate: 0.31, log(P):  493.6\n",
      "f_snow: 4.020\n",
      " f_ice: 10.604\n",
      " refreeze: 0.607\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 50600, acc. rate: 0.31, log(P):  493.7\n",
      "f_snow: 5.684\n",
      " f_ice: 10.090\n",
      " refreeze: 0.738\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 50700, acc. rate: 0.29, log(P):  494.2\n",
      "f_snow: 5.532\n",
      " f_ice: 11.616\n",
      " refreeze: 0.400\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 50800, acc. rate: 0.31, log(P):  493.8\n",
      "f_snow: 5.794\n",
      " f_ice: 10.675\n",
      " refreeze: 0.661\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 50900, acc. rate: 0.29, log(P):  494.5\n",
      "f_snow: 3.496\n",
      " f_ice: 10.397\n",
      " refreeze: 0.536\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 51000, acc. rate: 0.29, log(P):  494.4\n",
      "f_snow: 5.623\n",
      " f_ice: 9.972\n",
      " refreeze: 0.268\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 51100, acc. rate: 0.24, log(P):  493.2\n",
      "f_snow: 5.553\n",
      " f_ice: 10.302\n",
      " refreeze: 0.447\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 51200, acc. rate: 0.28, log(P):  493.5\n",
      "f_snow: 3.816\n",
      " f_ice: 10.775\n",
      " refreeze: 0.642\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 51300, acc. rate: 0.31, log(P):  495.8\n",
      "f_snow: 4.541\n",
      " f_ice: 9.145\n",
      " refreeze: 0.932\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 51400, acc. rate: 0.25, log(P):  493.5\n",
      "f_snow: 5.422\n",
      " f_ice: 10.240\n",
      " refreeze: 0.353\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 51500, acc. rate: 0.28, log(P):  494.7\n",
      "f_snow: 5.040\n",
      " f_ice: 9.329\n",
      " refreeze: 0.790\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 51600, acc. rate: 0.31, log(P):  494.5\n",
      "f_snow: 5.231\n",
      " f_ice: 9.268\n",
      " refreeze: 0.782\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 51700, acc. rate: 0.31, log(P):  493.5\n",
      "f_snow: 3.615\n",
      " f_ice: 10.895\n",
      " refreeze: 0.709\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 51800, acc. rate: 0.30, log(P):  494.3\n",
      "f_snow: 3.642\n",
      " f_ice: 10.217\n",
      " refreeze: 0.571\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 51900, acc. rate: 0.26, log(P):  496.0\n",
      "f_snow: 5.415\n",
      " f_ice: 8.385\n",
      " refreeze: 0.843\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 52000, acc. rate: 0.30, log(P):  494.5\n",
      "f_snow: 5.406\n",
      " f_ice: 11.174\n",
      " refreeze: 0.884\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 52100, acc. rate: 0.28, log(P):  493.8\n",
      "f_snow: 4.378\n",
      " f_ice: 11.512\n",
      " refreeze: 0.695\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 52200, acc. rate: 0.24, log(P):  494.7\n",
      "f_snow: 5.597\n",
      " f_ice: 11.721\n",
      " refreeze: 0.442\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 52300, acc. rate: 0.31, log(P):  494.6\n",
      "f_snow: 5.457\n",
      " f_ice: 8.826\n",
      " refreeze: 0.650\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 52400, acc. rate: 0.30, log(P):  494.2\n",
      "f_snow: 5.654\n",
      " f_ice: 9.411\n",
      " refreeze: 0.733\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 52500, acc. rate: 0.29, log(P):  494.4\n",
      "f_snow: 5.039\n",
      " f_ice: 10.798\n",
      " refreeze: 0.221\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 52600, acc. rate: 0.26, log(P):  494.5\n",
      "f_snow: 5.229\n",
      " f_ice: 9.931\n",
      " refreeze: 0.244\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 52700, acc. rate: 0.26, log(P):  496.2\n",
      "f_snow: 2.966\n",
      " f_ice: 11.715\n",
      " refreeze: 0.526\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 52800, acc. rate: 0.26, log(P):  493.2\n",
      "f_snow: 5.137\n",
      " f_ice: 11.228\n",
      " refreeze: 0.475\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 52900, acc. rate: 0.27, log(P):  495.6\n",
      "f_snow: 5.907\n",
      " f_ice: 10.023\n",
      " refreeze: 0.748\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 53000, acc. rate: 0.30, log(P):  495.4\n",
      "f_snow: 5.413\n",
      " f_ice: 11.761\n",
      " refreeze: 0.285\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 53100, acc. rate: 0.25, log(P):  493.5\n",
      "f_snow: 5.756\n",
      " f_ice: 10.496\n",
      " refreeze: 0.551\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 53200, acc. rate: 0.25, log(P):  493.5\n",
      "f_snow: 4.813\n",
      " f_ice: 10.772\n",
      " refreeze: 0.631\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 53300, acc. rate: 0.29, log(P):  495.3\n",
      "f_snow: 3.668\n",
      " f_ice: 11.787\n",
      " refreeze: 0.825\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 53400, acc. rate: 0.28, log(P):  493.0\n",
      "f_snow: 5.554\n",
      " f_ice: 10.455\n",
      " refreeze: 0.531\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 53500, acc. rate: 0.27, log(P):  496.0\n",
      "f_snow: 3.832\n",
      " f_ice: 11.573\n",
      " refreeze: 0.336\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 53600, acc. rate: 0.24, log(P):  494.1\n",
      "f_snow: 5.738\n",
      " f_ice: 10.928\n",
      " refreeze: 0.807\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 53700, acc. rate: 0.29, log(P):  495.5\n",
      "f_snow: 5.092\n",
      " f_ice: 8.549\n",
      " refreeze: 0.464\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 53800, acc. rate: 0.26, log(P):  493.2\n",
      "f_snow: 4.137\n",
      " f_ice: 10.949\n",
      " refreeze: 0.706\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 53900, acc. rate: 0.28, log(P):  493.5\n",
      "f_snow: 5.523\n",
      " f_ice: 11.458\n",
      " refreeze: 0.656\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 54000, acc. rate: 0.30, log(P):  496.3\n",
      "f_snow: 3.267\n",
      " f_ice: 9.430\n",
      " refreeze: 0.932\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 54100, acc. rate: 0.28, log(P):  493.7\n",
      "f_snow: 5.258\n",
      " f_ice: 11.456\n",
      " refreeze: 0.394\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 54200, acc. rate: 0.26, log(P):  493.5\n",
      "f_snow: 5.122\n",
      " f_ice: 10.617\n",
      " refreeze: 0.350\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 54300, acc. rate: 0.27, log(P):  494.9\n",
      "f_snow: 4.955\n",
      " f_ice: 11.005\n",
      " refreeze: 0.180\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 54400, acc. rate: 0.29, log(P):  495.2\n",
      "f_snow: 5.606\n",
      " f_ice: 8.454\n",
      " refreeze: 0.635\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 54500, acc. rate: 0.27, log(P):  493.5\n",
      "f_snow: 5.438\n",
      " f_ice: 10.406\n",
      " refreeze: 0.795\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 54600, acc. rate: 0.25, log(P):  493.1\n",
      "f_snow: 5.254\n",
      " f_ice: 11.250\n",
      " refreeze: 0.602\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 54700, acc. rate: 0.32, log(P):  493.6\n",
      "f_snow: 5.330\n",
      " f_ice: 9.687\n",
      " refreeze: 0.471\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 54800, acc. rate: 0.28, log(P):  494.4\n",
      "f_snow: 5.713\n",
      " f_ice: 11.239\n",
      " refreeze: 0.826\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 54900, acc. rate: 0.26, log(P):  493.8\n",
      "f_snow: 3.881\n",
      " f_ice: 10.163\n",
      " refreeze: 0.839\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 55000, acc. rate: 0.25, log(P):  493.0\n",
      "f_snow: 5.519\n",
      " f_ice: 10.806\n",
      " refreeze: 0.694\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 55100, acc. rate: 0.24, log(P):  494.5\n",
      "f_snow: 4.740\n",
      " f_ice: 10.108\n",
      " refreeze: 0.363\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 55200, acc. rate: 0.29, log(P):  493.1\n",
      "f_snow: 5.126\n",
      " f_ice: 10.977\n",
      " refreeze: 0.670\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 55300, acc. rate: 0.29, log(P):  497.0\n",
      "f_snow: 2.320\n",
      " f_ice: 11.055\n",
      " refreeze: 0.853\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 55400, acc. rate: 0.29, log(P):  493.4\n",
      "f_snow: 5.476\n",
      " f_ice: 9.760\n",
      " refreeze: 0.592\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 55500, acc. rate: 0.29, log(P):  493.2\n",
      "f_snow: 5.281\n",
      " f_ice: 10.064\n",
      " refreeze: 0.671\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 55600, acc. rate: 0.25, log(P):  493.7\n",
      "f_snow: 4.127\n",
      " f_ice: 9.961\n",
      " refreeze: 0.800\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 55700, acc. rate: 0.29, log(P):  497.0\n",
      "f_snow: 5.480\n",
      " f_ice: 9.826\n",
      " refreeze: 0.963\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 55800, acc. rate: 0.31, log(P):  494.0\n",
      "f_snow: 3.883\n",
      " f_ice: 11.019\n",
      " refreeze: 0.517\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 55900, acc. rate: 0.30, log(P):  495.1\n",
      "f_snow: 5.093\n",
      " f_ice: 9.268\n",
      " refreeze: 0.837\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 56000, acc. rate: 0.27, log(P):  494.4\n",
      "f_snow: 4.685\n",
      " f_ice: 9.993\n",
      " refreeze: 0.472\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 56100, acc. rate: 0.36, log(P):  493.6\n",
      "f_snow: 3.995\n",
      " f_ice: 10.214\n",
      " refreeze: 0.826\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 56200, acc. rate: 0.30, log(P):  494.6\n",
      "f_snow: 5.815\n",
      " f_ice: 9.600\n",
      " refreeze: 0.671\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 56300, acc. rate: 0.25, log(P):  494.0\n",
      "f_snow: 3.921\n",
      " f_ice: 9.747\n",
      " refreeze: 0.765\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 56400, acc. rate: 0.24, log(P):  492.9\n",
      "f_snow: 5.561\n",
      " f_ice: 10.670\n",
      " refreeze: 0.584\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 56500, acc. rate: 0.34, log(P):  495.0\n",
      "f_snow: 5.857\n",
      " f_ice: 11.426\n",
      " refreeze: 0.693\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 56600, acc. rate: 0.31, log(P):  493.9\n",
      "f_snow: 4.309\n",
      " f_ice: 10.231\n",
      " refreeze: 0.882\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 56700, acc. rate: 0.32, log(P):  497.7\n",
      "f_snow: 5.855\n",
      " f_ice: 11.022\n",
      " refreeze: 0.076\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 56800, acc. rate: 0.33, log(P):  494.3\n",
      "f_snow: 5.138\n",
      " f_ice: 11.660\n",
      " refreeze: 0.426\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 56900, acc. rate: 0.31, log(P):  495.1\n",
      "f_snow: 5.047\n",
      " f_ice: 8.792\n",
      " refreeze: 0.477\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 57000, acc. rate: 0.29, log(P):  496.9\n",
      "f_snow: 5.301\n",
      " f_ice: 11.903\n",
      " refreeze: 0.726\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 57100, acc. rate: 0.27, log(P):  493.8\n",
      "f_snow: 4.713\n",
      " f_ice: 11.135\n",
      " refreeze: 0.554\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 57200, acc. rate: 0.27, log(P):  492.9\n",
      "f_snow: 5.445\n",
      " f_ice: 10.755\n",
      " refreeze: 0.495\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 57300, acc. rate: 0.33, log(P):  492.8\n",
      "f_snow: 5.393\n",
      " f_ice: 10.961\n",
      " refreeze: 0.585\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 57400, acc. rate: 0.30, log(P):  493.7\n",
      "f_snow: 5.030\n",
      " f_ice: 10.007\n",
      " refreeze: 0.714\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 57500, acc. rate: 0.25, log(P):  493.2\n",
      "f_snow: 4.430\n",
      " f_ice: 11.014\n",
      " refreeze: 0.778\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 57600, acc. rate: 0.26, log(P):  493.4\n",
      "f_snow: 5.622\n",
      " f_ice: 9.943\n",
      " refreeze: 0.665\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 57700, acc. rate: 0.27, log(P):  493.0\n",
      "f_snow: 5.359\n",
      " f_ice: 10.698\n",
      " refreeze: 0.443\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 57800, acc. rate: 0.28, log(P):  493.5\n",
      "f_snow: 5.490\n",
      " f_ice: 10.569\n",
      " refreeze: 0.327\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 57900, acc. rate: 0.30, log(P):  496.3\n",
      "f_snow: 4.893\n",
      " f_ice: 9.904\n",
      " refreeze: 0.128\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 58000, acc. rate: 0.36, log(P):  493.5\n",
      "f_snow: 4.638\n",
      " f_ice: 11.274\n",
      " refreeze: 0.801\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 58100, acc. rate: 0.34, log(P):  493.7\n",
      "f_snow: 5.099\n",
      " f_ice: 10.292\n",
      " refreeze: 0.353\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 58200, acc. rate: 0.28, log(P):  499.4\n",
      "f_snow: 5.033\n",
      " f_ice: 11.647\n",
      " refreeze: 0.986\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 58300, acc. rate: 0.29, log(P):  493.4\n",
      "f_snow: 5.018\n",
      " f_ice: 10.942\n",
      " refreeze: 0.405\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 58400, acc. rate: 0.22, log(P):  493.4\n",
      "f_snow: 5.145\n",
      " f_ice: 10.005\n",
      " refreeze: 0.552\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 58500, acc. rate: 0.26, log(P):  495.0\n",
      "f_snow: 5.865\n",
      " f_ice: 11.011\n",
      " refreeze: 0.328\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 58600, acc. rate: 0.29, log(P):  495.8\n",
      "f_snow: 5.647\n",
      " f_ice: 8.801\n",
      " refreeze: 0.862\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 58700, acc. rate: 0.29, log(P):  495.2\n",
      "f_snow: 4.440\n",
      " f_ice: 11.573\n",
      " refreeze: 0.432\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 58800, acc. rate: 0.34, log(P):  495.9\n",
      "f_snow: 3.177\n",
      " f_ice: 9.378\n",
      " refreeze: 0.893\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 58900, acc. rate: 0.29, log(P):  493.1\n",
      "f_snow: 5.335\n",
      " f_ice: 10.055\n",
      " refreeze: 0.561\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 59000, acc. rate: 0.29, log(P):  493.0\n",
      "f_snow: 5.616\n",
      " f_ice: 10.598\n",
      " refreeze: 0.547\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 59100, acc. rate: 0.29, log(P):  493.6\n",
      "f_snow: 5.272\n",
      " f_ice: 11.266\n",
      " refreeze: 0.776\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 59200, acc. rate: 0.28, log(P):  493.9\n",
      "f_snow: 4.775\n",
      " f_ice: 10.500\n",
      " refreeze: 0.882\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 59300, acc. rate: 0.27, log(P):  494.2\n",
      "f_snow: 4.856\n",
      " f_ice: 10.548\n",
      " refreeze: 0.297\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 59400, acc. rate: 0.31, log(P):  493.0\n",
      "f_snow: 5.218\n",
      " f_ice: 10.511\n",
      " refreeze: 0.475\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 59500, acc. rate: 0.33, log(P):  493.3\n",
      "f_snow: 4.952\n",
      " f_ice: 10.944\n",
      " refreeze: 0.646\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 59600, acc. rate: 0.25, log(P):  493.0\n",
      "f_snow: 5.410\n",
      " f_ice: 10.253\n",
      " refreeze: 0.535\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 59700, acc. rate: 0.29, log(P):  492.8\n",
      "f_snow: 5.348\n",
      " f_ice: 10.792\n",
      " refreeze: 0.604\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 59800, acc. rate: 0.27, log(P):  494.6\n",
      "f_snow: 4.134\n",
      " f_ice: 9.257\n",
      " refreeze: 0.695\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 59900, acc. rate: 0.27, log(P):  493.8\n",
      "f_snow: 5.727\n",
      " f_ice: 11.352\n",
      " refreeze: 0.509\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 60000, acc. rate: 0.26, log(P):  496.7\n",
      "f_snow: 4.612\n",
      " f_ice: 11.895\n",
      " refreeze: 0.751\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 60100, acc. rate: 0.27, log(P):  496.4\n",
      "f_snow: 5.952\n",
      " f_ice: 10.802\n",
      " refreeze: 0.551\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 60200, acc. rate: 0.24, log(P):  493.3\n",
      "f_snow: 3.728\n",
      " f_ice: 10.837\n",
      " refreeze: 0.740\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 60300, acc. rate: 0.29, log(P):  493.7\n",
      "f_snow: 4.648\n",
      " f_ice: 11.199\n",
      " refreeze: 0.619\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 60400, acc. rate: 0.24, log(P):  493.0\n",
      "f_snow: 5.544\n",
      " f_ice: 10.284\n",
      " refreeze: 0.558\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 60500, acc. rate: 0.27, log(P):  493.6\n",
      "f_snow: 4.387\n",
      " f_ice: 10.611\n",
      " refreeze: 0.602\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 60600, acc. rate: 0.27, log(P):  494.4\n",
      "f_snow: 3.902\n",
      " f_ice: 9.899\n",
      " refreeze: 0.569\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 60700, acc. rate: 0.28, log(P):  493.4\n",
      "f_snow: 5.500\n",
      " f_ice: 10.020\n",
      " refreeze: 0.425\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 60800, acc. rate: 0.27, log(P):  493.7\n",
      "f_snow: 5.574\n",
      " f_ice: 9.509\n",
      " refreeze: 0.589\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 60900, acc. rate: 0.25, log(P):  493.6\n",
      "f_snow: 5.579\n",
      " f_ice: 9.692\n",
      " refreeze: 0.502\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 61000, acc. rate: 0.27, log(P):  494.9\n",
      "f_snow: 5.492\n",
      " f_ice: 11.712\n",
      " refreeze: 0.782\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 61100, acc. rate: 0.32, log(P):  494.7\n",
      "f_snow: 5.178\n",
      " f_ice: 10.624\n",
      " refreeze: 0.906\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 61200, acc. rate: 0.28, log(P):  492.9\n",
      "f_snow: 5.296\n",
      " f_ice: 10.573\n",
      " refreeze: 0.512\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 61300, acc. rate: 0.28, log(P):  493.6\n",
      "f_snow: 5.412\n",
      " f_ice: 10.863\n",
      " refreeze: 0.825\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 61400, acc. rate: 0.30, log(P):  494.1\n",
      "f_snow: 5.698\n",
      " f_ice: 10.110\n",
      " refreeze: 0.321\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 61500, acc. rate: 0.32, log(P):  494.1\n",
      "f_snow: 4.079\n",
      " f_ice: 10.829\n",
      " refreeze: 0.917\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 61600, acc. rate: 0.33, log(P):  494.3\n",
      "f_snow: 4.837\n",
      " f_ice: 9.722\n",
      " refreeze: 0.471\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 61700, acc. rate: 0.28, log(P):  494.2\n",
      "f_snow: 5.854\n",
      " f_ice: 10.784\n",
      " refreeze: 0.569\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 61800, acc. rate: 0.26, log(P):  493.0\n",
      "f_snow: 5.360\n",
      " f_ice: 11.185\n",
      " refreeze: 0.539\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 61900, acc. rate: 0.31, log(P):  495.6\n",
      "f_snow: 3.383\n",
      " f_ice: 9.072\n",
      " refreeze: 0.661\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 62000, acc. rate: 0.24, log(P):  494.9\n",
      "f_snow: 4.719\n",
      " f_ice: 11.743\n",
      " refreeze: 0.755\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 62100, acc. rate: 0.32, log(P):  493.7\n",
      "f_snow: 4.153\n",
      " f_ice: 9.921\n",
      " refreeze: 0.780\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 62200, acc. rate: 0.31, log(P):  496.2\n",
      "f_snow: 5.331\n",
      " f_ice: 11.802\n",
      " refreeze: 0.853\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 62300, acc. rate: 0.34, log(P):  499.9\n",
      "f_snow: 4.029\n",
      " f_ice: 10.964\n",
      " refreeze: 0.992\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 62400, acc. rate: 0.27, log(P):  494.3\n",
      "f_snow: 4.377\n",
      " f_ice: 9.678\n",
      " refreeze: 0.867\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 62500, acc. rate: 0.30, log(P):  493.2\n",
      "f_snow: 4.433\n",
      " f_ice: 10.927\n",
      " refreeze: 0.729\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 62600, acc. rate: 0.28, log(P):  499.2\n",
      "f_snow: 5.043\n",
      " f_ice: 11.917\n",
      " refreeze: 0.939\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 62700, acc. rate: 0.25, log(P):  493.0\n",
      "f_snow: 5.346\n",
      " f_ice: 10.850\n",
      " refreeze: 0.434\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 62800, acc. rate: 0.34, log(P):  493.9\n",
      "f_snow: 3.284\n",
      " f_ice: 10.781\n",
      " refreeze: 0.821\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 62900, acc. rate: 0.32, log(P):  492.8\n",
      "f_snow: 5.395\n",
      " f_ice: 10.519\n",
      " refreeze: 0.586\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 63000, acc. rate: 0.29, log(P):  494.1\n",
      "f_snow: 5.150\n",
      " f_ice: 10.871\n",
      " refreeze: 0.862\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 63100, acc. rate: 0.32, log(P):  494.4\n",
      "f_snow: 5.494\n",
      " f_ice: 11.229\n",
      " refreeze: 0.872\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 63200, acc. rate: 0.31, log(P):  494.1\n",
      "f_snow: 3.119\n",
      " f_ice: 10.908\n",
      " refreeze: 0.688\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 63300, acc. rate: 0.30, log(P):  493.2\n",
      "f_snow: 5.211\n",
      " f_ice: 10.624\n",
      " refreeze: 0.723\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 63400, acc. rate: 0.22, log(P):  494.3\n",
      "f_snow: 5.693\n",
      " f_ice: 11.598\n",
      " refreeze: 0.665\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 63500, acc. rate: 0.25, log(P):  494.4\n",
      "f_snow: 5.814\n",
      " f_ice: 11.406\n",
      " refreeze: 0.529\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 63600, acc. rate: 0.27, log(P):  493.2\n",
      "f_snow: 5.010\n",
      " f_ice: 10.605\n",
      " refreeze: 0.489\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 63700, acc. rate: 0.31, log(P):  495.1\n",
      "f_snow: 5.267\n",
      " f_ice: 11.663\n",
      " refreeze: 0.840\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 63800, acc. rate: 0.28, log(P):  494.2\n",
      "f_snow: 4.629\n",
      " f_ice: 11.325\n",
      " refreeze: 0.507\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 63900, acc. rate: 0.29, log(P):  493.4\n",
      "f_snow: 3.978\n",
      " f_ice: 10.705\n",
      " refreeze: 0.850\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 64000, acc. rate: 0.27, log(P):  493.8\n",
      "f_snow: 5.485\n",
      " f_ice: 10.582\n",
      " refreeze: 0.270\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 64100, acc. rate: 0.37, log(P):  495.8\n",
      "f_snow: 4.583\n",
      " f_ice: 8.502\n",
      " refreeze: 0.844\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 64200, acc. rate: 0.34, log(P):  494.3\n",
      "f_snow: 5.258\n",
      " f_ice: 9.536\n",
      " refreeze: 0.798\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 64300, acc. rate: 0.29, log(P):  494.3\n",
      "f_snow: 4.899\n",
      " f_ice: 9.600\n",
      " refreeze: 0.740\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 64400, acc. rate: 0.27, log(P):  492.9\n",
      "f_snow: 5.321\n",
      " f_ice: 10.619\n",
      " refreeze: 0.620\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 64500, acc. rate: 0.30, log(P):  493.3\n",
      "f_snow: 5.155\n",
      " f_ice: 10.433\n",
      " refreeze: 0.723\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 64600, acc. rate: 0.25, log(P):  493.5\n",
      "f_snow: 4.608\n",
      " f_ice: 10.864\n",
      " refreeze: 0.863\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 64700, acc. rate: 0.27, log(P):  495.9\n",
      "f_snow: 5.924\n",
      " f_ice: 9.911\n",
      " refreeze: 0.484\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 64800, acc. rate: 0.26, log(P):  493.1\n",
      "f_snow: 4.352\n",
      " f_ice: 10.919\n",
      " refreeze: 0.738\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 64900, acc. rate: 0.28, log(P):  493.4\n",
      "f_snow: 5.242\n",
      " f_ice: 11.305\n",
      " refreeze: 0.718\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 65000, acc. rate: 0.28, log(P):  494.2\n",
      "f_snow: 3.393\n",
      " f_ice: 9.990\n",
      " refreeze: 0.728\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 65100, acc. rate: 0.28, log(P):  493.1\n",
      "f_snow: 5.161\n",
      " f_ice: 11.030\n",
      " refreeze: 0.665\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 65200, acc. rate: 0.29, log(P):  493.2\n",
      "f_snow: 5.531\n",
      " f_ice: 11.300\n",
      " refreeze: 0.520\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 65300, acc. rate: 0.24, log(P):  495.0\n",
      "f_snow: 4.891\n",
      " f_ice: 10.820\n",
      " refreeze: 0.183\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 65400, acc. rate: 0.30, log(P):  493.8\n",
      "f_snow: 5.768\n",
      " f_ice: 11.205\n",
      " refreeze: 0.661\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 65500, acc. rate: 0.35, log(P):  494.9\n",
      "f_snow: 3.561\n",
      " f_ice: 9.360\n",
      " refreeze: 0.828\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 65600, acc. rate: 0.26, log(P):  493.8\n",
      "f_snow: 5.669\n",
      " f_ice: 10.512\n",
      " refreeze: 0.313\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 65700, acc. rate: 0.29, log(P):  493.6\n",
      "f_snow: 5.627\n",
      " f_ice: 11.397\n",
      " refreeze: 0.684\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 65800, acc. rate: 0.28, log(P):  495.9\n",
      "f_snow: 5.035\n",
      " f_ice: 8.712\n",
      " refreeze: 0.863\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 65900, acc. rate: 0.29, log(P):  493.8\n",
      "f_snow: 3.588\n",
      " f_ice: 11.461\n",
      " refreeze: 0.763\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 66000, acc. rate: 0.28, log(P):  494.7\n",
      "f_snow: 4.596\n",
      " f_ice: 10.543\n",
      " refreeze: 0.938\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 66100, acc. rate: 0.29, log(P):  496.6\n",
      "f_snow: 3.164\n",
      " f_ice: 8.795\n",
      " refreeze: 0.628\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 66200, acc. rate: 0.33, log(P):  494.6\n",
      "f_snow: 5.817\n",
      " f_ice: 11.476\n",
      " refreeze: 0.552\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 66300, acc. rate: 0.26, log(P):  493.2\n",
      "f_snow: 5.175\n",
      " f_ice: 10.177\n",
      " refreeze: 0.612\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 66400, acc. rate: 0.28, log(P):  493.6\n",
      "f_snow: 5.199\n",
      " f_ice: 9.736\n",
      " refreeze: 0.599\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 66500, acc. rate: 0.27, log(P):  493.6\n",
      "f_snow: 4.850\n",
      " f_ice: 10.722\n",
      " refreeze: 0.442\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 66600, acc. rate: 0.28, log(P):  493.1\n",
      "f_snow: 5.539\n",
      " f_ice: 10.747\n",
      " refreeze: 0.721\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 66700, acc. rate: 0.27, log(P):  495.3\n",
      "f_snow: 4.103\n",
      " f_ice: 10.957\n",
      " refreeze: 0.331\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 66800, acc. rate: 0.24, log(P):  493.7\n",
      "f_snow: 4.158\n",
      " f_ice: 10.326\n",
      " refreeze: 0.618\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 66900, acc. rate: 0.31, log(P):  496.0\n",
      "f_snow: 4.388\n",
      " f_ice: 9.528\n",
      " refreeze: 0.386\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 67000, acc. rate: 0.22, log(P):  493.5\n",
      "f_snow: 4.895\n",
      " f_ice: 10.508\n",
      " refreeze: 0.664\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 67100, acc. rate: 0.22, log(P):  493.3\n",
      "f_snow: 4.268\n",
      " f_ice: 10.481\n",
      " refreeze: 0.798\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 67200, acc. rate: 0.30, log(P):  493.3\n",
      "f_snow: 5.148\n",
      " f_ice: 10.632\n",
      " refreeze: 0.730\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 67300, acc. rate: 0.31, log(P):  496.2\n",
      "f_snow: 2.708\n",
      " f_ice: 11.697\n",
      " refreeze: 0.807\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 67400, acc. rate: 0.32, log(P):  493.9\n",
      "f_snow: 4.597\n",
      " f_ice: 10.873\n",
      " refreeze: 0.538\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 67500, acc. rate: 0.28, log(P):  493.6\n",
      "f_snow: 5.773\n",
      " f_ice: 10.503\n",
      " refreeze: 0.535\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 67600, acc. rate: 0.27, log(P):  492.9\n",
      "f_snow: 5.508\n",
      " f_ice: 10.492\n",
      " refreeze: 0.531\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 67700, acc. rate: 0.27, log(P):  496.0\n",
      "f_snow: 4.714\n",
      " f_ice: 11.830\n",
      " refreeze: 0.611\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 67800, acc. rate: 0.29, log(P):  495.5\n",
      "f_snow: 3.203\n",
      " f_ice: 11.666\n",
      " refreeze: 0.894\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 67900, acc. rate: 0.26, log(P):  494.5\n",
      "f_snow: 3.084\n",
      " f_ice: 11.476\n",
      " refreeze: 0.768\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 68000, acc. rate: 0.30, log(P):  496.4\n",
      "f_snow: 3.231\n",
      " f_ice: 9.019\n",
      " refreeze: 0.562\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 68100, acc. rate: 0.32, log(P):  493.6\n",
      "f_snow: 5.113\n",
      " f_ice: 10.896\n",
      " refreeze: 0.331\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 68200, acc. rate: 0.30, log(P):  495.0\n",
      "f_snow: 3.822\n",
      " f_ice: 11.568\n",
      " refreeze: 0.920\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 68300, acc. rate: 0.26, log(P):  495.0\n",
      "f_snow: 4.984\n",
      " f_ice: 11.642\n",
      " refreeze: 0.295\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 68400, acc. rate: 0.26, log(P):  495.5\n",
      "f_snow: 5.756\n",
      " f_ice: 9.606\n",
      " refreeze: 0.224\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 68500, acc. rate: 0.27, log(P):  494.8\n",
      "f_snow: 5.623\n",
      " f_ice: 10.737\n",
      " refreeze: 0.902\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 68600, acc. rate: 0.22, log(P):  493.2\n",
      "f_snow: 4.074\n",
      " f_ice: 11.124\n",
      " refreeze: 0.769\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 68700, acc. rate: 0.30, log(P):  494.1\n",
      "f_snow: 5.783\n",
      " f_ice: 11.228\n",
      " refreeze: 0.409\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 68800, acc. rate: 0.30, log(P):  493.0\n",
      "f_snow: 5.430\n",
      " f_ice: 10.596\n",
      " refreeze: 0.685\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 68900, acc. rate: 0.30, log(P):  493.6\n",
      "f_snow: 5.138\n",
      " f_ice: 10.016\n",
      " refreeze: 0.741\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 69000, acc. rate: 0.25, log(P):  492.9\n",
      "f_snow: 5.385\n",
      " f_ice: 10.795\n",
      " refreeze: 0.497\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 69100, acc. rate: 0.27, log(P):  493.2\n",
      "f_snow: 5.519\n",
      " f_ice: 10.673\n",
      " refreeze: 0.399\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 69200, acc. rate: 0.27, log(P):  493.4\n",
      "f_snow: 4.116\n",
      " f_ice: 11.186\n",
      " refreeze: 0.656\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 69300, acc. rate: 0.31, log(P):  493.3\n",
      "f_snow: 5.317\n",
      " f_ice: 9.938\n",
      " refreeze: 0.538\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 69400, acc. rate: 0.24, log(P):  493.1\n",
      "f_snow: 5.288\n",
      " f_ice: 10.268\n",
      " refreeze: 0.687\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 69500, acc. rate: 0.32, log(P):  494.4\n",
      "f_snow: 5.690\n",
      " f_ice: 11.525\n",
      " refreeze: 0.756\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 69600, acc. rate: 0.29, log(P):  492.9\n",
      "f_snow: 5.504\n",
      " f_ice: 11.005\n",
      " refreeze: 0.595\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 69700, acc. rate: 0.33, log(P):  497.6\n",
      "f_snow: 3.002\n",
      " f_ice: 8.659\n",
      " refreeze: 0.550\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 69800, acc. rate: 0.30, log(P):  496.9\n",
      "f_snow: 4.870\n",
      " f_ice: 11.476\n",
      " refreeze: 0.090\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 69900, acc. rate: 0.26, log(P):  493.2\n",
      "f_snow: 5.048\n",
      " f_ice: 10.465\n",
      " refreeze: 0.555\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 70000, acc. rate: 0.31, log(P):  495.5\n",
      "f_snow: 4.664\n",
      " f_ice: 11.686\n",
      " refreeze: 0.355\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 70100, acc. rate: 0.27, log(P):  498.1\n",
      "f_snow: 5.853\n",
      " f_ice: 11.897\n",
      " refreeze: 0.676\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 70200, acc. rate: 0.22, log(P):  494.5\n",
      "f_snow: 3.894\n",
      " f_ice: 10.007\n",
      " refreeze: 0.909\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 70300, acc. rate: 0.27, log(P):  494.2\n",
      "f_snow: 5.811\n",
      " f_ice: 10.574\n",
      " refreeze: 0.398\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 70400, acc. rate: 0.30, log(P):  493.2\n",
      "f_snow: 5.274\n",
      " f_ice: 10.099\n",
      " refreeze: 0.491\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 70500, acc. rate: 0.31, log(P):  494.8\n",
      "f_snow: 3.846\n",
      " f_ice: 11.750\n",
      " refreeze: 0.718\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 70600, acc. rate: 0.33, log(P):  495.1\n",
      "f_snow: 2.676\n",
      " f_ice: 11.035\n",
      " refreeze: 0.808\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 70700, acc. rate: 0.28, log(P):  493.6\n",
      "f_snow: 4.291\n",
      " f_ice: 10.451\n",
      " refreeze: 0.868\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 70800, acc. rate: 0.25, log(P):  493.8\n",
      "f_snow: 5.702\n",
      " f_ice: 10.708\n",
      " refreeze: 0.332\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 70900, acc. rate: 0.27, log(P):  492.9\n",
      "f_snow: 5.339\n",
      " f_ice: 10.455\n",
      " refreeze: 0.630\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 71000, acc. rate: 0.31, log(P):  493.5\n",
      "f_snow: 4.493\n",
      " f_ice: 11.319\n",
      " refreeze: 0.817\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 71100, acc. rate: 0.26, log(P):  493.9\n",
      "f_snow: 4.543\n",
      " f_ice: 10.478\n",
      " refreeze: 0.562\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 71200, acc. rate: 0.31, log(P):  495.2\n",
      "f_snow: 5.866\n",
      " f_ice: 9.472\n",
      " refreeze: 0.547\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 71300, acc. rate: 0.40, log(P):  493.6\n",
      "f_snow: 4.039\n",
      " f_ice: 10.110\n",
      " refreeze: 0.781\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 71400, acc. rate: 0.34, log(P):  493.6\n",
      "f_snow: 4.914\n",
      " f_ice: 11.149\n",
      " refreeze: 0.755\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 71500, acc. rate: 0.23, log(P):  493.2\n",
      "f_snow: 5.245\n",
      " f_ice: 10.597\n",
      " refreeze: 0.736\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 71600, acc. rate: 0.32, log(P):  493.8\n",
      "f_snow: 5.063\n",
      " f_ice: 10.329\n",
      " refreeze: 0.793\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 71700, acc. rate: 0.27, log(P):  493.1\n",
      "f_snow: 5.092\n",
      " f_ice: 10.850\n",
      " refreeze: 0.627\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 71800, acc. rate: 0.32, log(P):  493.1\n",
      "f_snow: 5.562\n",
      " f_ice: 10.360\n",
      " refreeze: 0.467\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 71900, acc. rate: 0.34, log(P):  493.0\n",
      "f_snow: 5.432\n",
      " f_ice: 10.551\n",
      " refreeze: 0.687\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 72000, acc. rate: 0.29, log(P):  494.2\n",
      "f_snow: 3.874\n",
      " f_ice: 10.960\n",
      " refreeze: 0.919\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 72100, acc. rate: 0.30, log(P):  494.2\n",
      "f_snow: 5.471\n",
      " f_ice: 11.258\n",
      " refreeze: 0.227\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 72200, acc. rate: 0.27, log(P):  493.3\n",
      "f_snow: 5.458\n",
      " f_ice: 10.575\n",
      " refreeze: 0.780\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 72300, acc. rate: 0.28, log(P):  493.6\n",
      "f_snow: 5.489\n",
      " f_ice: 11.319\n",
      " refreeze: 0.354\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 72400, acc. rate: 0.33, log(P):  495.2\n",
      "f_snow: 4.579\n",
      " f_ice: 8.844\n",
      " refreeze: 0.704\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 72500, acc. rate: 0.30, log(P):  495.1\n",
      "f_snow: 5.086\n",
      " f_ice: 11.705\n",
      " refreeze: 0.790\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 72600, acc. rate: 0.31, log(P):  493.1\n",
      "f_snow: 5.504\n",
      " f_ice: 10.192\n",
      " refreeze: 0.549\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 72700, acc. rate: 0.27, log(P):  493.3\n",
      "f_snow: 5.552\n",
      " f_ice: 10.266\n",
      " refreeze: 0.423\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 72800, acc. rate: 0.25, log(P):  493.2\n",
      "f_snow: 4.392\n",
      " f_ice: 11.006\n",
      " refreeze: 0.723\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 72900, acc. rate: 0.28, log(P):  494.9\n",
      "f_snow: 5.576\n",
      " f_ice: 11.663\n",
      " refreeze: 0.296\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 73000, acc. rate: 0.29, log(P):  493.9\n",
      "f_snow: 5.651\n",
      " f_ice: 10.057\n",
      " refreeze: 0.346\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 73100, acc. rate: 0.26, log(P):  493.4\n",
      "f_snow: 4.071\n",
      " f_ice: 11.024\n",
      " refreeze: 0.631\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 73200, acc. rate: 0.25, log(P):  493.3\n",
      "f_snow: 4.942\n",
      " f_ice: 11.102\n",
      " refreeze: 0.614\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 73300, acc. rate: 0.26, log(P):  492.9\n",
      "f_snow: 5.330\n",
      " f_ice: 10.793\n",
      " refreeze: 0.649\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 73400, acc. rate: 0.28, log(P):  495.2\n",
      "f_snow: 5.707\n",
      " f_ice: 11.136\n",
      " refreeze: 0.164\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 73500, acc. rate: 0.29, log(P):  493.1\n",
      "f_snow: 5.346\n",
      " f_ice: 10.498\n",
      " refreeze: 0.711\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 73600, acc. rate: 0.31, log(P):  493.6\n",
      "f_snow: 5.737\n",
      " f_ice: 11.071\n",
      " refreeze: 0.460\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 73700, acc. rate: 0.33, log(P):  495.1\n",
      "f_snow: 5.119\n",
      " f_ice: 9.613\n",
      " refreeze: 0.890\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 73800, acc. rate: 0.28, log(P):  493.4\n",
      "f_snow: 3.802\n",
      " f_ice: 10.829\n",
      " refreeze: 0.677\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 73900, acc. rate: 0.27, log(P):  493.9\n",
      "f_snow: 5.685\n",
      " f_ice: 11.321\n",
      " refreeze: 0.369\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 74000, acc. rate: 0.25, log(P):  493.7\n",
      "f_snow: 5.081\n",
      " f_ice: 10.895\n",
      " refreeze: 0.806\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 74100, acc. rate: 0.30, log(P):  493.6\n",
      "f_snow: 4.482\n",
      " f_ice: 10.047\n",
      " refreeze: 0.729\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 74200, acc. rate: 0.26, log(P):  494.4\n",
      "f_snow: 5.371\n",
      " f_ice: 11.103\n",
      " refreeze: 0.188\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 74300, acc. rate: 0.27, log(P):  493.0\n",
      "f_snow: 5.198\n",
      " f_ice: 10.754\n",
      " refreeze: 0.686\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 74400, acc. rate: 0.33, log(P):  493.2\n",
      "f_snow: 5.694\n",
      " f_ice: 10.753\n",
      " refreeze: 0.604\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 74500, acc. rate: 0.29, log(P):  495.5\n",
      "f_snow: 3.844\n",
      " f_ice: 11.783\n",
      " refreeze: 0.871\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 74600, acc. rate: 0.30, log(P):  493.6\n",
      "f_snow: 5.107\n",
      " f_ice: 11.459\n",
      " refreeze: 0.605\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 74700, acc. rate: 0.26, log(P):  493.4\n",
      "f_snow: 5.402\n",
      " f_ice: 11.359\n",
      " refreeze: 0.396\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 74800, acc. rate: 0.29, log(P):  494.9\n",
      "f_snow: 4.406\n",
      " f_ice: 9.033\n",
      " refreeze: 0.702\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 74900, acc. rate: 0.31, log(P):  496.0\n",
      "f_snow: 5.905\n",
      " f_ice: 11.582\n",
      " refreeze: 0.476\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 75000, acc. rate: 0.31, log(P):  493.2\n",
      "f_snow: 5.686\n",
      " f_ice: 10.740\n",
      " refreeze: 0.637\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 75100, acc. rate: 0.30, log(P):  493.6\n",
      "f_snow: 5.140\n",
      " f_ice: 9.928\n",
      " refreeze: 0.450\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 75200, acc. rate: 0.27, log(P):  494.3\n",
      "f_snow: 4.992\n",
      " f_ice: 11.654\n",
      " refreeze: 0.516\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 75300, acc. rate: 0.24, log(P):  492.9\n",
      "f_snow: 5.245\n",
      " f_ice: 10.544\n",
      " refreeze: 0.545\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 75400, acc. rate: 0.28, log(P):  493.6\n",
      "f_snow: 3.891\n",
      " f_ice: 10.822\n",
      " refreeze: 0.873\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 75500, acc. rate: 0.31, log(P):  493.6\n",
      "f_snow: 3.879\n",
      " f_ice: 11.394\n",
      " refreeze: 0.819\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 75600, acc. rate: 0.32, log(P):  496.9\n",
      "f_snow: 5.688\n",
      " f_ice: 11.798\n",
      " refreeze: 0.877\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 75700, acc. rate: 0.27, log(P):  493.2\n",
      "f_snow: 5.304\n",
      " f_ice: 10.593\n",
      " refreeze: 0.745\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 75800, acc. rate: 0.28, log(P):  495.1\n",
      "f_snow: 5.535\n",
      " f_ice: 9.665\n",
      " refreeze: 0.206\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 75900, acc. rate: 0.24, log(P):  494.3\n",
      "f_snow: 3.499\n",
      " f_ice: 9.976\n",
      " refreeze: 0.667\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 76000, acc. rate: 0.28, log(P):  494.5\n",
      "f_snow: 3.918\n",
      " f_ice: 11.699\n",
      " refreeze: 0.768\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 76100, acc. rate: 0.26, log(P):  494.0\n",
      "f_snow: 4.790\n",
      " f_ice: 11.459\n",
      " refreeze: 0.826\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 76200, acc. rate: 0.25, log(P):  494.6\n",
      "f_snow: 3.746\n",
      " f_ice: 9.618\n",
      " refreeze: 0.634\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 76300, acc. rate: 0.28, log(P):  494.3\n",
      "f_snow: 5.187\n",
      " f_ice: 10.256\n",
      " refreeze: 0.868\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 76400, acc. rate: 0.33, log(P):  493.8\n",
      "f_snow: 4.877\n",
      " f_ice: 10.940\n",
      " refreeze: 0.869\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 76500, acc. rate: 0.30, log(P):  493.1\n",
      "f_snow: 5.388\n",
      " f_ice: 10.600\n",
      " refreeze: 0.386\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 76600, acc. rate: 0.31, log(P):  493.9\n",
      "f_snow: 3.926\n",
      " f_ice: 10.283\n",
      " refreeze: 0.603\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 76700, acc. rate: 0.31, log(P):  496.6\n",
      "f_snow: 2.890\n",
      " f_ice: 11.808\n",
      " refreeze: 0.649\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 76800, acc. rate: 0.30, log(P):  494.5\n",
      "f_snow: 4.754\n",
      " f_ice: 11.546\n",
      " refreeze: 0.437\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 76900, acc. rate: 0.30, log(P):  493.1\n",
      "f_snow: 5.221\n",
      " f_ice: 11.246\n",
      " refreeze: 0.635\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 77000, acc. rate: 0.29, log(P):  493.3\n",
      "f_snow: 3.768\n",
      " f_ice: 10.901\n",
      " refreeze: 0.742\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 77100, acc. rate: 0.30, log(P):  493.4\n",
      "f_snow: 4.015\n",
      " f_ice: 10.604\n",
      " refreeze: 0.662\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 77200, acc. rate: 0.26, log(P):  493.3\n",
      "f_snow: 5.195\n",
      " f_ice: 10.491\n",
      " refreeze: 0.411\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 77300, acc. rate: 0.35, log(P):  494.7\n",
      "f_snow: 2.866\n",
      " f_ice: 11.318\n",
      " refreeze: 0.760\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 77400, acc. rate: 0.31, log(P):  493.6\n",
      "f_snow: 4.570\n",
      " f_ice: 10.713\n",
      " refreeze: 0.603\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 77500, acc. rate: 0.25, log(P):  494.7\n",
      "f_snow: 5.496\n",
      " f_ice: 9.372\n",
      " refreeze: 0.848\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 77600, acc. rate: 0.27, log(P):  494.3\n",
      "f_snow: 3.871\n",
      " f_ice: 11.582\n",
      " refreeze: 0.857\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 77700, acc. rate: 0.28, log(P):  494.3\n",
      "f_snow: 5.815\n",
      " f_ice: 10.265\n",
      " refreeze: 0.416\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 77800, acc. rate: 0.28, log(P):  496.4\n",
      "f_snow: 5.754\n",
      " f_ice: 8.059\n",
      " refreeze: 0.488\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 77900, acc. rate: 0.24, log(P):  493.0\n",
      "f_snow: 5.586\n",
      " f_ice: 10.721\n",
      " refreeze: 0.501\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 78000, acc. rate: 0.29, log(P):  495.8\n",
      "f_snow: 5.786\n",
      " f_ice: 11.714\n",
      " refreeze: 0.786\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 78100, acc. rate: 0.28, log(P):  493.2\n",
      "f_snow: 5.457\n",
      " f_ice: 11.371\n",
      " refreeze: 0.604\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 78200, acc. rate: 0.26, log(P):  494.1\n",
      "f_snow: 3.287\n",
      " f_ice: 11.364\n",
      " refreeze: 0.813\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 78300, acc. rate: 0.31, log(P):  494.6\n",
      "f_snow: 5.130\n",
      " f_ice: 9.226\n",
      " refreeze: 0.419\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 78400, acc. rate: 0.29, log(P):  494.5\n",
      "f_snow: 5.825\n",
      " f_ice: 10.109\n",
      " refreeze: 0.410\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 78500, acc. rate: 0.33, log(P):  493.5\n",
      "f_snow: 3.813\n",
      " f_ice: 10.351\n",
      " refreeze: 0.722\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 78600, acc. rate: 0.35, log(P):  493.6\n",
      "f_snow: 5.542\n",
      " f_ice: 9.712\n",
      " refreeze: 0.697\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 78700, acc. rate: 0.29, log(P):  493.6\n",
      "f_snow: 5.624\n",
      " f_ice: 10.130\n",
      " refreeze: 0.404\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 78800, acc. rate: 0.32, log(P):  493.4\n",
      "f_snow: 5.445\n",
      " f_ice: 9.993\n",
      " refreeze: 0.445\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 78900, acc. rate: 0.24, log(P):  493.3\n",
      "f_snow: 5.561\n",
      " f_ice: 10.319\n",
      " refreeze: 0.395\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 79000, acc. rate: 0.28, log(P):  493.4\n",
      "f_snow: 4.574\n",
      " f_ice: 11.152\n",
      " refreeze: 0.684\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 79100, acc. rate: 0.26, log(P):  494.1\n",
      "f_snow: 4.145\n",
      " f_ice: 9.799\n",
      " refreeze: 0.643\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 79200, acc. rate: 0.25, log(P):  493.9\n",
      "f_snow: 4.957\n",
      " f_ice: 11.407\n",
      " refreeze: 0.722\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 79300, acc. rate: 0.28, log(P):  493.4\n",
      "f_snow: 4.287\n",
      " f_ice: 11.292\n",
      " refreeze: 0.817\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 79400, acc. rate: 0.34, log(P):  495.6\n",
      "f_snow: 5.278\n",
      " f_ice: 8.467\n",
      " refreeze: 0.787\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 79500, acc. rate: 0.30, log(P):  494.2\n",
      "f_snow: 5.587\n",
      " f_ice: 11.480\n",
      " refreeze: 0.789\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 79600, acc. rate: 0.27, log(P):  496.0\n",
      "f_snow: 5.632\n",
      " f_ice: 10.263\n",
      " refreeze: 0.106\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 79700, acc. rate: 0.28, log(P):  493.9\n",
      "f_snow: 4.526\n",
      " f_ice: 11.537\n",
      " refreeze: 0.798\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 79800, acc. rate: 0.32, log(P):  493.1\n",
      "f_snow: 5.214\n",
      " f_ice: 11.241\n",
      " refreeze: 0.551\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 79900, acc. rate: 0.27, log(P):  493.7\n",
      "f_snow: 5.719\n",
      " f_ice: 10.587\n",
      " refreeze: 0.766\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 80000, acc. rate: 0.25, log(P):  494.6\n",
      "f_snow: 5.877\n",
      " f_ice: 10.304\n",
      " refreeze: 0.617\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 80100, acc. rate: 0.28, log(P):  494.0\n",
      "f_snow: 5.680\n",
      " f_ice: 11.512\n",
      " refreeze: 0.455\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 80200, acc. rate: 0.29, log(P):  493.6\n",
      "f_snow: 4.305\n",
      " f_ice: 10.880\n",
      " refreeze: 0.886\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 80300, acc. rate: 0.25, log(P):  493.2\n",
      "f_snow: 5.549\n",
      " f_ice: 11.209\n",
      " refreeze: 0.444\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 80400, acc. rate: 0.27, log(P):  493.8\n",
      "f_snow: 5.676\n",
      " f_ice: 10.727\n",
      " refreeze: 0.804\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 80500, acc. rate: 0.28, log(P):  494.7\n",
      "f_snow: 4.751\n",
      " f_ice: 11.079\n",
      " refreeze: 0.255\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 80600, acc. rate: 0.27, log(P):  493.7\n",
      "f_snow: 5.634\n",
      " f_ice: 10.745\n",
      " refreeze: 0.808\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 80700, acc. rate: 0.26, log(P):  493.3\n",
      "f_snow: 5.169\n",
      " f_ice: 10.179\n",
      " refreeze: 0.480\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 80800, acc. rate: 0.25, log(P):  494.4\n",
      "f_snow: 5.020\n",
      " f_ice: 10.625\n",
      " refreeze: 0.905\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 80900, acc. rate: 0.26, log(P):  493.1\n",
      "f_snow: 5.320\n",
      " f_ice: 10.920\n",
      " refreeze: 0.715\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 81000, acc. rate: 0.30, log(P):  496.6\n",
      "f_snow: 5.942\n",
      " f_ice: 10.051\n",
      " refreeze: 0.379\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 81100, acc. rate: 0.24, log(P):  493.8\n",
      "f_snow: 5.261\n",
      " f_ice: 9.468\n",
      " refreeze: 0.515\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 81200, acc. rate: 0.30, log(P):  494.0\n",
      "f_snow: 5.389\n",
      " f_ice: 9.348\n",
      " refreeze: 0.448\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 81300, acc. rate: 0.26, log(P):  493.0\n",
      "f_snow: 5.127\n",
      " f_ice: 10.979\n",
      " refreeze: 0.615\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 81400, acc. rate: 0.31, log(P):  493.9\n",
      "f_snow: 5.703\n",
      " f_ice: 9.975\n",
      " refreeze: 0.391\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 81500, acc. rate: 0.31, log(P):  493.1\n",
      "f_snow: 4.109\n",
      " f_ice: 10.983\n",
      " refreeze: 0.753\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 81600, acc. rate: 0.30, log(P):  492.9\n",
      "f_snow: 5.203\n",
      " f_ice: 10.933\n",
      " refreeze: 0.645\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 81700, acc. rate: 0.26, log(P):  494.1\n",
      "f_snow: 5.331\n",
      " f_ice: 9.826\n",
      " refreeze: 0.826\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 81800, acc. rate: 0.28, log(P):  494.8\n",
      "f_snow: 5.827\n",
      " f_ice: 11.402\n",
      " refreeze: 0.359\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 81900, acc. rate: 0.29, log(P):  493.7\n",
      "f_snow: 4.290\n",
      " f_ice: 11.494\n",
      " refreeze: 0.720\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 82000, acc. rate: 0.29, log(P):  494.2\n",
      "f_snow: 4.511\n",
      " f_ice: 11.618\n",
      " refreeze: 0.665\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 82100, acc. rate: 0.36, log(P):  495.3\n",
      "f_snow: 5.875\n",
      " f_ice: 9.499\n",
      " refreeze: 0.563\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 82200, acc. rate: 0.31, log(P):  492.8\n",
      "f_snow: 5.413\n",
      " f_ice: 10.587\n",
      " refreeze: 0.579\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 82300, acc. rate: 0.25, log(P):  493.5\n",
      "f_snow: 5.573\n",
      " f_ice: 11.122\n",
      " refreeze: 0.348\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 82400, acc. rate: 0.23, log(P):  494.1\n",
      "f_snow: 4.189\n",
      " f_ice: 11.485\n",
      " refreeze: 0.577\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 82500, acc. rate: 0.28, log(P):  497.5\n",
      "f_snow: 5.498\n",
      " f_ice: 11.520\n",
      " refreeze: 0.055\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 82600, acc. rate: 0.31, log(P):  495.4\n",
      "f_snow: 3.805\n",
      " f_ice: 9.504\n",
      " refreeze: 0.498\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 82700, acc. rate: 0.34, log(P):  493.3\n",
      "f_snow: 5.169\n",
      " f_ice: 10.242\n",
      " refreeze: 0.473\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 82800, acc. rate: 0.36, log(P):  494.7\n",
      "f_snow: 3.856\n",
      " f_ice: 9.575\n",
      " refreeze: 0.590\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 82900, acc. rate: 0.25, log(P):  493.6\n",
      "f_snow: 5.079\n",
      " f_ice: 11.438\n",
      " refreeze: 0.657\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 83000, acc. rate: 0.28, log(P):  494.1\n",
      "f_snow: 4.366\n",
      " f_ice: 10.636\n",
      " refreeze: 0.504\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 83100, acc. rate: 0.24, log(P):  493.6\n",
      "f_snow: 4.995\n",
      " f_ice: 11.303\n",
      " refreeze: 0.689\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 83200, acc. rate: 0.30, log(P):  494.2\n",
      "f_snow: 5.552\n",
      " f_ice: 11.640\n",
      " refreeze: 0.466\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 83300, acc. rate: 0.28, log(P):  494.2\n",
      "f_snow: 4.395\n",
      " f_ice: 10.382\n",
      " refreeze: 0.913\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 83400, acc. rate: 0.32, log(P):  494.4\n",
      "f_snow: 5.429\n",
      " f_ice: 10.199\n",
      " refreeze: 0.212\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 83500, acc. rate: 0.31, log(P):  497.6\n",
      "f_snow: 5.965\n",
      " f_ice: 11.368\n",
      " refreeze: 0.667\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 83600, acc. rate: 0.24, log(P):  495.2\n",
      "f_snow: 3.015\n",
      " f_ice: 10.366\n",
      " refreeze: 0.538\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 83700, acc. rate: 0.30, log(P):  494.5\n",
      "f_snow: 5.822\n",
      " f_ice: 9.705\n",
      " refreeze: 0.509\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 83800, acc. rate: 0.29, log(P):  494.5\n",
      "f_snow: 5.627\n",
      " f_ice: 10.014\n",
      " refreeze: 0.241\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 83900, acc. rate: 0.31, log(P):  495.2\n",
      "f_snow: 5.206\n",
      " f_ice: 11.811\n",
      " refreeze: 0.568\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 84000, acc. rate: 0.28, log(P):  494.2\n",
      "f_snow: 4.935\n",
      " f_ice: 10.457\n",
      " refreeze: 0.284\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 84100, acc. rate: 0.24, log(P):  493.9\n",
      "f_snow: 4.672\n",
      " f_ice: 9.913\n",
      " refreeze: 0.682\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 84200, acc. rate: 0.28, log(P):  494.0\n",
      "f_snow: 5.468\n",
      " f_ice: 10.217\n",
      " refreeze: 0.268\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 84300, acc. rate: 0.30, log(P):  495.0\n",
      "f_snow: 2.929\n",
      " f_ice: 10.119\n",
      " refreeze: 0.813\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 84400, acc. rate: 0.31, log(P):  495.3\n",
      "f_snow: 5.710\n",
      " f_ice: 11.393\n",
      " refreeze: 0.885\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 84500, acc. rate: 0.27, log(P):  493.6\n",
      "f_snow: 5.578\n",
      " f_ice: 10.326\n",
      " refreeze: 0.790\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 84600, acc. rate: 0.23, log(P):  493.5\n",
      "f_snow: 4.817\n",
      " f_ice: 10.692\n",
      " refreeze: 0.574\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 84700, acc. rate: 0.28, log(P):  493.4\n",
      "f_snow: 4.066\n",
      " f_ice: 10.998\n",
      " refreeze: 0.649\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 84800, acc. rate: 0.26, log(P):  493.1\n",
      "f_snow: 5.191\n",
      " f_ice: 10.754\n",
      " refreeze: 0.426\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 84900, acc. rate: 0.28, log(P):  495.1\n",
      "f_snow: 5.008\n",
      " f_ice: 11.714\n",
      " refreeze: 0.322\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 85000, acc. rate: 0.27, log(P):  494.8\n",
      "f_snow: 5.153\n",
      " f_ice: 11.660\n",
      " refreeze: 0.797\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 85100, acc. rate: 0.26, log(P):  494.4\n",
      "f_snow: 5.735\n",
      " f_ice: 9.507\n",
      " refreeze: 0.737\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 85200, acc. rate: 0.26, log(P):  494.6\n",
      "f_snow: 5.387\n",
      " f_ice: 8.758\n",
      " refreeze: 0.589\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 85300, acc. rate: 0.30, log(P):  494.0\n",
      "f_snow: 3.408\n",
      " f_ice: 11.359\n",
      " refreeze: 0.638\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 85400, acc. rate: 0.30, log(P):  494.7\n",
      "f_snow: 5.066\n",
      " f_ice: 11.568\n",
      " refreeze: 0.842\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 85500, acc. rate: 0.30, log(P):  494.6\n",
      "f_snow: 3.723\n",
      " f_ice: 10.936\n",
      " refreeze: 0.429\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 85600, acc. rate: 0.30, log(P):  495.4\n",
      "f_snow: 5.667\n",
      " f_ice: 8.517\n",
      " refreeze: 0.464\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 85700, acc. rate: 0.28, log(P):  495.5\n",
      "f_snow: 5.925\n",
      " f_ice: 10.497\n",
      " refreeze: 0.546\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 85800, acc. rate: 0.24, log(P):  494.0\n",
      "f_snow: 4.820\n",
      " f_ice: 11.452\n",
      " refreeze: 0.520\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 85900, acc. rate: 0.27, log(P):  493.7\n",
      "f_snow: 3.401\n",
      " f_ice: 10.704\n",
      " refreeze: 0.757\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 86000, acc. rate: 0.28, log(P):  495.7\n",
      "f_snow: 5.247\n",
      " f_ice: 9.717\n",
      " refreeze: 0.923\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 86100, acc. rate: 0.32, log(P):  494.3\n",
      "f_snow: 3.032\n",
      " f_ice: 11.138\n",
      " refreeze: 0.682\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 86200, acc. rate: 0.29, log(P):  494.1\n",
      "f_snow: 4.883\n",
      " f_ice: 10.388\n",
      " refreeze: 0.886\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 86300, acc. rate: 0.33, log(P):  494.2\n",
      "f_snow: 5.359\n",
      " f_ice: 10.127\n",
      " refreeze: 0.863\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 86400, acc. rate: 0.31, log(P):  493.6\n",
      "f_snow: 4.523\n",
      " f_ice: 11.422\n",
      " refreeze: 0.743\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 86500, acc. rate: 0.29, log(P):  493.8\n",
      "f_snow: 5.622\n",
      " f_ice: 10.600\n",
      " refreeze: 0.295\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 86600, acc. rate: 0.34, log(P):  497.3\n",
      "f_snow: 5.001\n",
      " f_ice: 11.766\n",
      " refreeze: 0.123\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 86700, acc. rate: 0.31, log(P):  492.9\n",
      "f_snow: 5.482\n",
      " f_ice: 10.981\n",
      " refreeze: 0.661\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 86800, acc. rate: 0.27, log(P):  494.0\n",
      "f_snow: 5.224\n",
      " f_ice: 11.632\n",
      " refreeze: 0.593\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 86900, acc. rate: 0.25, log(P):  493.3\n",
      "f_snow: 5.547\n",
      " f_ice: 10.080\n",
      " refreeze: 0.683\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 87000, acc. rate: 0.27, log(P):  493.4\n",
      "f_snow: 4.730\n",
      " f_ice: 11.129\n",
      " refreeze: 0.747\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 87100, acc. rate: 0.33, log(P):  496.0\n",
      "f_snow: 4.464\n",
      " f_ice: 11.348\n",
      " refreeze: 0.192\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 87200, acc. rate: 0.26, log(P):  493.5\n",
      "f_snow: 5.147\n",
      " f_ice: 10.803\n",
      " refreeze: 0.327\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 87300, acc. rate: 0.29, log(P):  495.4\n",
      "f_snow: 2.927\n",
      " f_ice: 10.838\n",
      " refreeze: 0.489\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 87400, acc. rate: 0.27, log(P):  493.3\n",
      "f_snow: 5.137\n",
      " f_ice: 10.076\n",
      " refreeze: 0.645\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 87500, acc. rate: 0.29, log(P):  493.3\n",
      "f_snow: 3.884\n",
      " f_ice: 10.521\n",
      " refreeze: 0.740\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 87600, acc. rate: 0.27, log(P):  493.9\n",
      "f_snow: 5.576\n",
      " f_ice: 11.426\n",
      " refreeze: 0.357\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 87700, acc. rate: 0.32, log(P):  495.2\n",
      "f_snow: 4.238\n",
      " f_ice: 11.264\n",
      " refreeze: 0.953\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 87800, acc. rate: 0.28, log(P):  494.1\n",
      "f_snow: 5.310\n",
      " f_ice: 11.675\n",
      " refreeze: 0.582\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 87900, acc. rate: 0.33, log(P):  493.1\n",
      "f_snow: 5.201\n",
      " f_ice: 10.649\n",
      " refreeze: 0.429\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 88000, acc. rate: 0.27, log(P):  494.9\n",
      "f_snow: 5.061\n",
      " f_ice: 11.142\n",
      " refreeze: 0.172\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 88100, acc. rate: 0.31, log(P):  494.1\n",
      "f_snow: 5.018\n",
      " f_ice: 9.588\n",
      " refreeze: 0.480\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 88200, acc. rate: 0.38, log(P):  494.9\n",
      "f_snow: 3.509\n",
      " f_ice: 10.987\n",
      " refreeze: 0.430\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 88300, acc. rate: 0.32, log(P):  494.8\n",
      "f_snow: 3.976\n",
      " f_ice: 11.639\n",
      " refreeze: 0.898\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 88400, acc. rate: 0.32, log(P):  493.5\n",
      "f_snow: 5.438\n",
      " f_ice: 9.987\n",
      " refreeze: 0.755\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 88500, acc. rate: 0.37, log(P):  493.2\n",
      "f_snow: 3.930\n",
      " f_ice: 11.032\n",
      " refreeze: 0.730\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 88600, acc. rate: 0.34, log(P):  493.2\n",
      "f_snow: 4.346\n",
      " f_ice: 10.891\n",
      " refreeze: 0.758\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 88700, acc. rate: 0.32, log(P):  493.6\n",
      "f_snow: 5.143\n",
      " f_ice: 11.415\n",
      " refreeze: 0.434\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 88800, acc. rate: 0.34, log(P):  499.1\n",
      "f_snow: 3.954\n",
      " f_ice: 11.871\n",
      " refreeze: 0.973\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 88900, acc. rate: 0.35, log(P):  493.5\n",
      "f_snow: 3.534\n",
      " f_ice: 10.828\n",
      " refreeze: 0.766\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 89000, acc. rate: 0.29, log(P):  497.0\n",
      "f_snow: 3.686\n",
      " f_ice: 11.901\n",
      " refreeze: 0.651\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 89100, acc. rate: 0.29, log(P):  493.1\n",
      "f_snow: 5.380\n",
      " f_ice: 11.254\n",
      " refreeze: 0.656\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 89200, acc. rate: 0.28, log(P):  494.0\n",
      "f_snow: 3.904\n",
      " f_ice: 9.767\n",
      " refreeze: 0.724\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 89300, acc. rate: 0.29, log(P):  494.3\n",
      "f_snow: 5.301\n",
      " f_ice: 9.654\n",
      " refreeze: 0.312\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 89400, acc. rate: 0.31, log(P):  494.2\n",
      "f_snow: 5.723\n",
      " f_ice: 9.738\n",
      " refreeze: 0.393\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 89500, acc. rate: 0.36, log(P):  494.5\n",
      "f_snow: 5.292\n",
      " f_ice: 11.651\n",
      " refreeze: 0.769\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 89600, acc. rate: 0.30, log(P):  492.9\n",
      "f_snow: 5.568\n",
      " f_ice: 10.802\n",
      " refreeze: 0.609\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 89700, acc. rate: 0.31, log(P):  495.5\n",
      "f_snow: 3.933\n",
      " f_ice: 10.724\n",
      " refreeze: 0.326\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 89800, acc. rate: 0.29, log(P):  493.8\n",
      "f_snow: 3.523\n",
      " f_ice: 11.359\n",
      " refreeze: 0.679\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 89900, acc. rate: 0.28, log(P):  495.5\n",
      "f_snow: 5.900\n",
      " f_ice: 9.922\n",
      " refreeze: 0.418\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 90000, acc. rate: 0.25, log(P):  494.6\n",
      "f_snow: 3.217\n",
      " f_ice: 10.139\n",
      " refreeze: 0.866\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 90100, acc. rate: 0.26, log(P):  492.9\n",
      "f_snow: 5.279\n",
      " f_ice: 10.910\n",
      " refreeze: 0.622\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 90200, acc. rate: 0.27, log(P):  493.9\n",
      "f_snow: 5.756\n",
      " f_ice: 11.143\n",
      " refreeze: 0.390\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 90300, acc. rate: 0.24, log(P):  495.4\n",
      "f_snow: 4.626\n",
      " f_ice: 11.633\n",
      " refreeze: 0.346\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 90400, acc. rate: 0.35, log(P):  494.5\n",
      "f_snow: 5.228\n",
      " f_ice: 9.178\n",
      " refreeze: 0.764\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 90500, acc. rate: 0.29, log(P):  495.3\n",
      "f_snow: 5.720\n",
      " f_ice: 8.596\n",
      " refreeze: 0.645\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 90600, acc. rate: 0.32, log(P):  493.6\n",
      "f_snow: 4.217\n",
      " f_ice: 10.511\n",
      " refreeze: 0.599\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 90700, acc. rate: 0.30, log(P):  493.5\n",
      "f_snow: 3.824\n",
      " f_ice: 10.997\n",
      " refreeze: 0.637\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 90800, acc. rate: 0.27, log(P):  493.4\n",
      "f_snow: 4.601\n",
      " f_ice: 10.842\n",
      " refreeze: 0.648\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 90900, acc. rate: 0.26, log(P):  492.8\n",
      "f_snow: 5.367\n",
      " f_ice: 10.765\n",
      " refreeze: 0.635\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 91000, acc. rate: 0.26, log(P):  493.2\n",
      "f_snow: 5.648\n",
      " f_ice: 11.033\n",
      " refreeze: 0.474\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 91100, acc. rate: 0.27, log(P):  493.3\n",
      "f_snow: 5.347\n",
      " f_ice: 10.023\n",
      " refreeze: 0.708\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 91200, acc. rate: 0.28, log(P):  493.5\n",
      "f_snow: 4.802\n",
      " f_ice: 11.146\n",
      " refreeze: 0.703\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 91300, acc. rate: 0.29, log(P):  496.6\n",
      "f_snow: 5.377\n",
      " f_ice: 11.703\n",
      " refreeze: 0.121\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 91400, acc. rate: 0.24, log(P):  494.3\n",
      "f_snow: 5.439\n",
      " f_ice: 11.510\n",
      " refreeze: 0.820\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 91500, acc. rate: 0.37, log(P):  495.0\n",
      "f_snow: 5.116\n",
      " f_ice: 10.232\n",
      " refreeze: 0.173\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 91600, acc. rate: 0.31, log(P):  496.3\n",
      "f_snow: 5.155\n",
      " f_ice: 11.813\n",
      " refreeze: 0.851\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 91700, acc. rate: 0.37, log(P):  496.3\n",
      "f_snow: 5.733\n",
      " f_ice: 11.850\n",
      " refreeze: 0.478\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 91800, acc. rate: 0.34, log(P):  493.2\n",
      "f_snow: 5.063\n",
      " f_ice: 10.373\n",
      " refreeze: 0.603\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 91900, acc. rate: 0.37, log(P):  493.5\n",
      "f_snow: 5.347\n",
      " f_ice: 11.404\n",
      " refreeze: 0.722\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 92000, acc. rate: 0.31, log(P):  493.3\n",
      "f_snow: 4.209\n",
      " f_ice: 10.552\n",
      " refreeze: 0.798\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 92100, acc. rate: 0.29, log(P):  493.1\n",
      "f_snow: 5.571\n",
      " f_ice: 10.843\n",
      " refreeze: 0.422\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 92200, acc. rate: 0.29, log(P):  493.6\n",
      "f_snow: 4.611\n",
      " f_ice: 10.596\n",
      " refreeze: 0.630\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 92300, acc. rate: 0.29, log(P):  493.4\n",
      "f_snow: 3.703\n",
      " f_ice: 10.674\n",
      " refreeze: 0.690\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 92400, acc. rate: 0.26, log(P):  492.9\n",
      "f_snow: 5.516\n",
      " f_ice: 10.898\n",
      " refreeze: 0.621\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 92500, acc. rate: 0.29, log(P):  493.2\n",
      "f_snow: 4.370\n",
      " f_ice: 10.883\n",
      " refreeze: 0.752\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 92600, acc. rate: 0.34, log(P):  493.9\n",
      "f_snow: 5.817\n",
      " f_ice: 10.826\n",
      " refreeze: 0.662\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 92700, acc. rate: 0.25, log(P):  494.8\n",
      "f_snow: 4.330\n",
      " f_ice: 9.103\n",
      " refreeze: 0.813\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 92800, acc. rate: 0.27, log(P):  495.2\n",
      "f_snow: 5.842\n",
      " f_ice: 11.585\n",
      " refreeze: 0.469\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 92900, acc. rate: 0.28, log(P):  494.4\n",
      "f_snow: 5.252\n",
      " f_ice: 10.300\n",
      " refreeze: 0.878\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 93000, acc. rate: 0.28, log(P):  493.1\n",
      "f_snow: 5.533\n",
      " f_ice: 11.130\n",
      " refreeze: 0.477\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 93100, acc. rate: 0.34, log(P):  494.1\n",
      "f_snow: 5.304\n",
      " f_ice: 11.653\n",
      " refreeze: 0.451\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 93200, acc. rate: 0.34, log(P):  494.9\n",
      "f_snow: 3.730\n",
      " f_ice: 9.858\n",
      " refreeze: 0.918\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 93300, acc. rate: 0.30, log(P):  493.3\n",
      "f_snow: 5.575\n",
      " f_ice: 10.522\n",
      " refreeze: 0.392\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 93400, acc. rate: 0.27, log(P):  494.0\n",
      "f_snow: 5.777\n",
      " f_ice: 11.331\n",
      " refreeze: 0.525\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 93500, acc. rate: 0.32, log(P):  494.9\n",
      "f_snow: 3.354\n",
      " f_ice: 11.691\n",
      " refreeze: 0.724\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 93600, acc. rate: 0.27, log(P):  493.3\n",
      "f_snow: 4.619\n",
      " f_ice: 10.927\n",
      " refreeze: 0.728\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 93700, acc. rate: 0.30, log(P):  494.0\n",
      "f_snow: 3.145\n",
      " f_ice: 11.124\n",
      " refreeze: 0.745\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 93800, acc. rate: 0.31, log(P):  493.6\n",
      "f_snow: 3.584\n",
      " f_ice: 11.058\n",
      " refreeze: 0.832\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 93900, acc. rate: 0.29, log(P):  497.2\n",
      "f_snow: 5.839\n",
      " f_ice: 11.818\n",
      " refreeze: 0.304\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 94000, acc. rate: 0.26, log(P):  494.3\n",
      "f_snow: 5.784\n",
      " f_ice: 9.781\n",
      " refreeze: 0.706\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 94100, acc. rate: 0.28, log(P):  494.8\n",
      "f_snow: 4.096\n",
      " f_ice: 11.733\n",
      " refreeze: 0.833\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 94200, acc. rate: 0.32, log(P):  493.8\n",
      "f_snow: 3.956\n",
      " f_ice: 9.925\n",
      " refreeze: 0.754\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 94300, acc. rate: 0.29, log(P):  493.6\n",
      "f_snow: 5.335\n",
      " f_ice: 11.414\n",
      " refreeze: 0.742\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 94400, acc. rate: 0.30, log(P):  495.0\n",
      "f_snow: 4.876\n",
      " f_ice: 11.727\n",
      " refreeze: 0.633\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 94500, acc. rate: 0.30, log(P):  493.1\n",
      "f_snow: 5.122\n",
      " f_ice: 11.120\n",
      " refreeze: 0.543\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 94600, acc. rate: 0.30, log(P):  497.4\n",
      "f_snow: 5.795\n",
      " f_ice: 10.262\n",
      " refreeze: 0.961\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 94700, acc. rate: 0.30, log(P):  496.0\n",
      "f_snow: 5.603\n",
      " f_ice: 11.134\n",
      " refreeze: 0.097\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 94800, acc. rate: 0.31, log(P):  493.6\n",
      "f_snow: 4.938\n",
      " f_ice: 11.217\n",
      " refreeze: 0.444\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 94900, acc. rate: 0.27, log(P):  495.8\n",
      "f_snow: 2.915\n",
      " f_ice: 10.019\n",
      " refreeze: 0.520\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 95000, acc. rate: 0.23, log(P):  495.3\n",
      "f_snow: 5.656\n",
      " f_ice: 8.449\n",
      " refreeze: 0.541\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 95100, acc. rate: 0.29, log(P):  493.6\n",
      "f_snow: 4.644\n",
      " f_ice: 11.357\n",
      " refreeze: 0.731\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 95200, acc. rate: 0.26, log(P):  496.3\n",
      "f_snow: 5.422\n",
      " f_ice: 7.808\n",
      " refreeze: 0.558\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 95300, acc. rate: 0.31, log(P):  495.3\n",
      "f_snow: 4.990\n",
      " f_ice: 11.682\n",
      " refreeze: 0.265\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 95400, acc. rate: 0.26, log(P):  493.8\n",
      "f_snow: 4.966\n",
      " f_ice: 10.858\n",
      " refreeze: 0.323\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 95500, acc. rate: 0.26, log(P):  494.9\n",
      "f_snow: 2.983\n",
      " f_ice: 10.176\n",
      " refreeze: 0.833\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 95600, acc. rate: 0.30, log(P):  493.7\n",
      "f_snow: 4.759\n",
      " f_ice: 10.151\n",
      " refreeze: 0.788\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 95700, acc. rate: 0.29, log(P):  493.2\n",
      "f_snow: 5.353\n",
      " f_ice: 10.967\n",
      " refreeze: 0.362\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 95800, acc. rate: 0.30, log(P):  494.5\n",
      "f_snow: 5.633\n",
      " f_ice: 9.677\n",
      " refreeze: 0.832\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 95900, acc. rate: 0.26, log(P):  493.2\n",
      "f_snow: 5.599\n",
      " f_ice: 10.541\n",
      " refreeze: 0.712\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 96000, acc. rate: 0.31, log(P):  493.4\n",
      "f_snow: 4.838\n",
      " f_ice: 10.969\n",
      " refreeze: 0.777\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 96100, acc. rate: 0.22, log(P):  493.4\n",
      "f_snow: 5.407\n",
      " f_ice: 9.794\n",
      " refreeze: 0.641\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 96200, acc. rate: 0.25, log(P):  494.1\n",
      "f_snow: 5.180\n",
      " f_ice: 9.279\n",
      " refreeze: 0.529\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 96300, acc. rate: 0.27, log(P):  494.1\n",
      "f_snow: 5.796\n",
      " f_ice: 11.290\n",
      " refreeze: 0.667\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 96400, acc. rate: 0.28, log(P):  493.7\n",
      "f_snow: 4.615\n",
      " f_ice: 11.298\n",
      " refreeze: 0.636\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 96500, acc. rate: 0.28, log(P):  492.9\n",
      "f_snow: 5.483\n",
      " f_ice: 11.050\n",
      " refreeze: 0.576\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 96600, acc. rate: 0.25, log(P):  494.1\n",
      "f_snow: 4.617\n",
      " f_ice: 9.855\n",
      " refreeze: 0.863\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 96700, acc. rate: 0.26, log(P):  493.8\n",
      "f_snow: 5.572\n",
      " f_ice: 9.534\n",
      " refreeze: 0.661\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 96800, acc. rate: 0.32, log(P):  493.4\n",
      "f_snow: 5.251\n",
      " f_ice: 10.952\n",
      " refreeze: 0.787\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 96900, acc. rate: 0.28, log(P):  496.9\n",
      "f_snow: 2.382\n",
      " f_ice: 10.113\n",
      " refreeze: 0.777\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 97000, acc. rate: 0.31, log(P):  493.9\n",
      "f_snow: 4.526\n",
      " f_ice: 10.423\n",
      " refreeze: 0.890\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 97100, acc. rate: 0.33, log(P):  497.2\n",
      "f_snow: 5.343\n",
      " f_ice: 11.107\n",
      " refreeze: 0.971\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 97200, acc. rate: 0.35, log(P):  494.9\n",
      "f_snow: 5.452\n",
      " f_ice: 8.676\n",
      " refreeze: 0.701\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 97300, acc. rate: 0.27, log(P):  493.3\n",
      "f_snow: 5.522\n",
      " f_ice: 11.372\n",
      " refreeze: 0.619\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 97400, acc. rate: 0.25, log(P):  494.0\n",
      "f_snow: 5.162\n",
      " f_ice: 10.839\n",
      " refreeze: 0.843\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 97500, acc. rate: 0.28, log(P):  493.5\n",
      "f_snow: 5.613\n",
      " f_ice: 11.012\n",
      " refreeze: 0.343\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 97600, acc. rate: 0.33, log(P):  494.9\n",
      "f_snow: 3.733\n",
      " f_ice: 9.956\n",
      " refreeze: 0.489\n",
      "\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "sample: 97700, acc. rate: 0.26, log(P):  493.5\n",
      "f_snow: 5.586\n",
      " f_ice: 9.824\n",
      " refreeze: 0.648\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 97800, acc. rate: 0.34, log(P):  493.6\n",
      "f_snow: 4.705\n",
      " f_ice: 10.857\n",
      " refreeze: 0.868\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 97900, acc. rate: 0.24, log(P):  494.7\n",
      "f_snow: 5.621\n",
      " f_ice: 8.996\n",
      " refreeze: 0.748\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 98000, acc. rate: 0.28, log(P):  493.7\n",
      "f_snow: 4.963\n",
      " f_ice: 11.167\n",
      " refreeze: 0.804\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 98100, acc. rate: 0.23, log(P):  493.6\n",
      "f_snow: 4.407\n",
      " f_ice: 10.974\n",
      " refreeze: 0.591\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 98200, acc. rate: 0.29, log(P):  497.6\n",
      "f_snow: 2.492\n",
      " f_ice: 11.658\n",
      " refreeze: 0.916\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 98300, acc. rate: 0.29, log(P):  493.6\n",
      "f_snow: 4.867\n",
      " f_ice: 11.200\n",
      " refreeze: 0.489\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 98400, acc. rate: 0.25, log(P):  493.8\n",
      "f_snow: 5.719\n",
      " f_ice: 11.183\n",
      " refreeze: 0.394\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 98500, acc. rate: 0.26, log(P):  495.6\n",
      "f_snow: 4.997\n",
      " f_ice: 10.327\n",
      " refreeze: 0.951\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 98600, acc. rate: 0.30, log(P):  493.5\n",
      "f_snow: 5.530\n",
      " f_ice: 11.408\n",
      " refreeze: 0.696\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 98700, acc. rate: 0.28, log(P):  494.1\n",
      "f_snow: 5.214\n",
      " f_ice: 9.226\n",
      " refreeze: 0.611\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 98800, acc. rate: 0.30, log(P):  494.3\n",
      "f_snow: 5.707\n",
      " f_ice: 10.770\n",
      " refreeze: 0.258\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 98900, acc. rate: 0.30, log(P):  493.4\n",
      "f_snow: 4.144\n",
      " f_ice: 11.161\n",
      " refreeze: 0.832\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 99000, acc. rate: 0.29, log(P):  494.4\n",
      "f_snow: 4.202\n",
      " f_ice: 10.740\n",
      " refreeze: 0.932\n",
      "\n",
      "===============================================\n",
      "///////////////////////////////////////////////\n",
      "Saving samples for model 0\n",
      "///////////////////////////////////////////////\n",
      "===============================================\n",
      "sample: 99100, acc. rate: 0.26, log(P):  493.5\n",
      "f_snow: 5.589\n",
      " f_ice: 10.575\n",
      " refreeze: 0.794\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 99200, acc. rate: 0.29, log(P):  494.2\n",
      "f_snow: 4.920\n",
      " f_ice: 9.804\n",
      " refreeze: 0.423\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 99300, acc. rate: 0.27, log(P):  493.6\n",
      "f_snow: 5.238\n",
      " f_ice: 10.618\n",
      " refreeze: 0.309\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 99400, acc. rate: 0.28, log(P):  495.8\n",
      "f_snow: 5.937\n",
      " f_ice: 10.955\n",
      " refreeze: 0.555\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 99500, acc. rate: 0.26, log(P):  494.0\n",
      "f_snow: 4.119\n",
      " f_ice: 10.021\n",
      " refreeze: 0.881\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 99600, acc. rate: 0.24, log(P):  495.8\n",
      "f_snow: 5.522\n",
      " f_ice: 11.842\n",
      " refreeze: 0.410\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 99700, acc. rate: 0.26, log(P):  493.2\n",
      "f_snow: 5.468\n",
      " f_ice: 11.272\n",
      " refreeze: 0.682\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 99800, acc. rate: 0.29, log(P):  493.0\n",
      "f_snow: 5.167\n",
      " f_ice: 10.638\n",
      " refreeze: 0.580\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "sample: 99900, acc. rate: 0.32, log(P):  495.8\n",
      "f_snow: 5.407\n",
      " f_ice: 11.840\n",
      " refreeze: 0.377\n",
      "\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "    from scipy.stats import beta, uniform\n",
    "    from scipy.special import gamma\n",
    "\n",
    "    device=\"cuda\"\n",
    "    nu = 1\n",
    "    n_iters=100000\n",
    "    n_draws=100000\n",
    "    n_prior_samples=100000\n",
    "\n",
    "    temp_test, precip_test, _, _, _, _ = read_observation(thinning_factor=1)\n",
    "    std_dev_test = np.zeros_like(temp_test) + 4\n",
    "    \n",
    "    f_snow_test = 3.0\n",
    "    f_ice_test = 8.0\n",
    "    refreeze_test = 0.0\n",
    "    pdd = TorchPDDModel(\n",
    "        pdd_factor_snow=f_snow_test,\n",
    "        pdd_factor_ice=f_ice_test,\n",
    "        refreeze_snow=refreeze_test,\n",
    "        refreeze_ice=refreeze_test,\n",
    "    )\n",
    "    result = pdd(temp_test, precip_test, std_dev_test)\n",
    "\n",
    "    M_test = result[\"melt\"]\n",
    "    A_test = result[\"accu\"]\n",
    "    R_test = result[\"refreeze\"]\n",
    "    \n",
    "    Y_test = torch.vstack((M_test, A_test, R_test)).T.type(torch.FloatTensor)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for k, row in df.iterrows():   \n",
    "        m_f_snow = row[\"f_snow\"]\n",
    "        m_f_ice = row[\"f_ice\"]\n",
    "        m_refreeze = row[\"refreeze\"]\n",
    "        X.append(torch.from_numpy(np.hstack((temp_test.T, precip_test.T, np.tile(m_f_snow, (temp_test.shape[1], 1)), np.tile(m_f_ice, (temp_test.shape[1], 1)), np.tile(m_refreeze, (temp_test.shape[1], 1))))))\n",
    "\n",
    "    \n",
    "    X_test = torch.vstack(X).type(torch.FloatTensor)\n",
    "    X_test_mean = X_test.nanmean(axis=0)\n",
    "    X_test_std = X_test.std(axis=0)\n",
    "    \n",
    "    X_test_norm = torch.nan_to_num((X_test - X_test_mean) / X_test_std)\n",
    "    \n",
    "    X_P_mean = X_test_mean[-3::].to(device)\n",
    "    X_P_std = X_test_std[-3::].to(device)\n",
    "    X_P_prior = X_test_norm[:,-3::].to(device)\n",
    "    X_I_prior = X_test_norm[:, :-3].to(device)\n",
    "    \n",
    "    X_min = X_test_norm.cpu().numpy().min(axis=0)\n",
    "    X_max = X_test_norm.cpu().numpy().max(axis=0)\n",
    "\n",
    "    sigma = 0.01\n",
    "\n",
    "    rho = 1.0 / (1e4**2)\n",
    "    point_area = 1800 ** 2\n",
    "    K = point_area * rho\n",
    "    sigma_hat = np.sqrt(sigma**2 / K**2)\n",
    "\n",
    "\n",
    "    X_min = torch.tensor(X_min, dtype=torch.float32, device=device)\n",
    "    X_max = torch.tensor(X_max, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Needs\n",
    "    # alpha_b, beta_b: float\n",
    "    # alpha: float\n",
    "    # nu: float\n",
    "    # gamma\n",
    "    # sigma_hat\n",
    "    X_P_0 = torch.tensor(X_P_prior.mean(axis=0),\n",
    "                         requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "    X_I_0 = torch.tensor(X_I_prior.mean(axis=0),\n",
    "                         requires_grad=True, dtype=torch.float, device=device)\n",
    "    \n",
    "    X_P_min = X_min[-3:]\n",
    "    X_P_max = X_max[-3:]\n",
    "    \n",
    "    U_target = Y_test.to(device)\n",
    "\n",
    "\n",
    "    X_P_keys = [\"f_snow\", \"f_ice\", \"refreeze\"]\n",
    "    mala = MALASampler(e.to(device), emulator_dir=emulator_dir)\n",
    "    X_map = mala.find_MAP(X_P_0, X_I_0, U_target, X_P_min, X_P_max)\n",
    "    \n",
    "    # To reproduce the paper, n_iters should be 10^5\n",
    "    X_posterior = mala.MALA(\n",
    "        X_map,\n",
    "        X_I_0,\n",
    "        X_P_min,\n",
    "        X_P_max,\n",
    "        U_target,\n",
    "        n_iters=n_iters,\n",
    "        model_index=int(model_index),\n",
    "        save_interval=1000,\n",
    "        print_interval=100,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7b78268f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([66377, 3])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_norm[:, -3::].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "103cb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import beta, gaussian_kde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7567f409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "57e292ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading posterior samples\n",
      "\n",
      "  -- pddemulator/posterior_samples/X_posterior_model_0.csv.gz\n",
      "Merging posteriors into dataframe\n",
      "Saving figure to pddemulator/posterior.pdf\n",
      "Saving figure to pddemulator/emulator_posterior.pdf\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 388.8x201.6 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 388.8x403.2 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    X_std = X_P_std\n",
    "    X_mean = X_P_mean\n",
    "    frac = 1.0\n",
    "    lw = 1\n",
    "    color_prior = \"b\"\n",
    "    X_list = []\n",
    "    X_keys= [\"f_snow\", \"f_ice\", \"refreeze\"]\n",
    "    X_Prior = (X_P_prior.detach().cpu().numpy() * X_P_std[-3::] .detach().cpu().numpy() + X_P_mean[-3::] .detach().cpu().numpy())\n",
    "    keys_dict = {\"f_ice\": \"$f_{\\mathrm{ice}}$\", \"f_snow\": \"$f_{\\mathrm{snoe}}$\", \"refreeze\": \"$r$\"}\n",
    "    p = Path(f\"{emulator_dir}/posterior_samples/\")\n",
    "    print(\"Loading posterior samples\\n\")\n",
    "    for m, m_file in enumerate(sorted(p.glob(\"X_posterior_model_*.csv.gz\"))):\n",
    "        print(f\"  -- {m_file}\")\n",
    "        df = pd.read_csv(m_file).sample(frac=frac)\n",
    "        if \"Unnamed: 0\" in df.columns:\n",
    "            df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "        model = m_file.name.split(\"_\")[-1].split(\".\")[0]\n",
    "        df[\"Model\"] = int(model)\n",
    "        X_list.append(df)\n",
    "\n",
    "    print(f\"Merging posteriors into dataframe\")\n",
    "    posterior_df = pd.concat(X_list)\n",
    "\n",
    "    X_posterior = posterior_df.drop(columns=[\"Model\"]).values\n",
    "    C_0 = np.corrcoef((X_posterior - X_posterior.mean(axis=0)).T)\n",
    "    Cn_0 = (np.sign(C_0) * C_0 ** 2 + 1) / 2.0\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(5.4, 2.8))\n",
    "    fig.subplots_adjust(hspace=0.0, wspace=0.0)\n",
    "    for i in range(3):\n",
    "        min_val = min(X_Prior[:, i].min(), X_posterior[:, i].min())\n",
    "        max_val = max(X_Prior[:, i].max(), X_posterior[:, i].max())\n",
    "        bins = np.linspace(min_val, max_val, 30)\n",
    "        X_prior_hist, b = np.histogram(X_Prior[:, i] , bins, density=True)\n",
    "        X_posterior_hist, _ = np.histogram(X_posterior[:, i], bins, density=True)\n",
    "        b = 0.5 * (b[1:] + b[:-1])\n",
    "        axs[i].plot(\n",
    "            b,\n",
    "            X_posterior_hist * 0.5,\n",
    "            color=\"0.5\",\n",
    "            linewidth=lw * 0.25,\n",
    "            linestyle=\"solid\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "\n",
    "    figfile = f\"{emulator_dir}/posterior.pdf\"\n",
    "    print(f\"Saving figure to {figfile}\")\n",
    "    fig.savefig(figfile)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(5.4, 5.6))\n",
    "    fig.subplots_adjust(hspace=0.0, wspace=0.0)\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if i > j:\n",
    "\n",
    "                axs[i, j].scatter(\n",
    "                    X_posterior[:, j],\n",
    "                    X_posterior[:, i],\n",
    "                    c=\"#31a354\",\n",
    "                    s=0.05,\n",
    "                    alpha=0.01,\n",
    "                    label=\"Posterior\",\n",
    "                    rasterized=True,\n",
    "                )\n",
    "\n",
    "                min_val = min(X_Prior[:, i].min(), X_posterior[:, i].min())\n",
    "                max_val = max(X_Prior[:, i].max(), X_posterior[:, i].max())\n",
    "                bins_y = np.linspace(min_val, max_val, 30)\n",
    "\n",
    "                min_val = min(X_Prior[:, j].min(), X_posterior[:, j].min())\n",
    "                max_val = max(X_Prior[:, j].max(), X_posterior[:, j].max())\n",
    "                bins_x = np.linspace(min_val, max_val, 30)\n",
    "\n",
    "                v = gaussian_kde(X_posterior[:, [j, i]].T)\n",
    "                bx = 0.5 * (bins_x[1:] + bins_x[:-1])\n",
    "                by = 0.5 * (bins_y[1:] + bins_y[:-1])\n",
    "                Bx, By = np.meshgrid(bx, by)\n",
    "\n",
    "                axs[i, j].contour(\n",
    "                    Bx,\n",
    "                    By,\n",
    "                    v(np.vstack((Bx.ravel(), By.ravel()))).reshape(Bx.shape),\n",
    "                    7,\n",
    "                    linewidths=0.5,\n",
    "                    colors=\"black\",\n",
    "                )\n",
    "\n",
    "                axs[i, j].set_xlim(X_Prior[:, j].min(), X_Prior[:, j].max())\n",
    "                axs[i, j].set_ylim(X_Prior[:, i].min(), X_Prior[:, i].max())\n",
    "\n",
    "            elif i < j:\n",
    "                patch_upper = Polygon(\n",
    "                    np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 1.0], [1.0, 0.0]]),\n",
    "                    facecolor=plt.cm.seismic(Cn_0[i, j]),\n",
    "                )\n",
    "                axs[i, j].add_patch(patch_upper)\n",
    "                if C_0[i, j] > -0.5:\n",
    "                    color = \"black\"\n",
    "                else:\n",
    "                    color = \"white\"\n",
    "                axs[i, j].text(\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    \"{0:.2f}\".format(C_0[i, j]),\n",
    "                    fontsize=6,\n",
    "                    horizontalalignment=\"center\",\n",
    "                    verticalalignment=\"center\",\n",
    "                    transform=axs[i, j].transAxes,\n",
    "                    color=color,\n",
    "                )\n",
    "\n",
    "            elif i == j:\n",
    "                min_val = min(X_Prior[:, i].min(), X_posterior[:, i].min())\n",
    "                max_val = max(X_Prior[:, i].max(), X_posterior[:, i].max())\n",
    "                bins = np.linspace(min_val, max_val, 30)\n",
    "                X_prior_hist, b = np.histogram(X_Prior[:, i], bins, density=True)\n",
    "                X_posterior_hist, _ = np.histogram(\n",
    "                    X_posterior[:, i], bins, density=True\n",
    "                )\n",
    "                b = 0.5 * (b[1:] + b[:-1])\n",
    "\n",
    "                axs[i, j].plot(\n",
    "                    b,\n",
    "                    X_prior_hist,\n",
    "                    color=color_prior,\n",
    "                    linewidth=lw,\n",
    "                    label=\"Prior\",\n",
    "                    linestyle=\"solid\",\n",
    "                )\n",
    "\n",
    "                all_models = posterior_df[\"Model\"].unique()\n",
    "                for k, m_model in enumerate(all_models):\n",
    "                    m_df = posterior_df[posterior_df[\"Model\"] == m_model].drop(\n",
    "                        columns=[\"Model\"]\n",
    "                    )\n",
    "                    X_model_posterior = m_df.values\n",
    "                    X_model_posterior_hist, _ = np.histogram(\n",
    "                        X_model_posterior[:, i], _, density=True\n",
    "                    )\n",
    "                    if k == 0:\n",
    "                        axs[i, j].plot(\n",
    "                            b,\n",
    "                            X_model_posterior_hist * 0.5,\n",
    "                            color=\"0.5\",\n",
    "                            linewidth=lw * 0.25,\n",
    "                            linestyle=\"solid\",\n",
    "                            alpha=0.5,\n",
    "                            label=\"Posterior (BayesBag)\",\n",
    "                        )\n",
    "                    else:\n",
    "                        axs[i, j].plot(\n",
    "                            b,\n",
    "                            X_model_posterior_hist * 0.5,\n",
    "                            color=\"0.5\",\n",
    "                            linewidth=lw * 0.25,\n",
    "                            linestyle=\"solid\",\n",
    "                            alpha=0.5,\n",
    "                        )\n",
    "\n",
    "                axs[i, j].plot(\n",
    "                    b,\n",
    "                    X_posterior_hist,\n",
    "                    color=\"black\",\n",
    "                    linewidth=lw,\n",
    "                    linestyle=\"solid\",\n",
    "                    label=\"Posterior\",\n",
    "                )\n",
    "\n",
    "                axs[i, j].set_xlim(min_val, max_val)\n",
    "\n",
    "            else:\n",
    "                axs[i, j].remove()\n",
    "\n",
    "    for i, ax in enumerate(axs[:, 0]):\n",
    "        ax.set_ylabel(keys_dict[X_keys[i]])\n",
    "\n",
    "    for j, ax in enumerate(axs[-1, :]):\n",
    "        ax.set_xlabel(keys_dict[X_keys[j]])\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "        plt.setp(ax.xaxis.get_minorticklabels(), rotation=45)\n",
    "        if j > 0:\n",
    "            ax.tick_params(axis=\"y\", which=\"both\", length=0)\n",
    "            ax.yaxis.set_minor_formatter(NullFormatter())\n",
    "            ax.yaxis.set_major_formatter(NullFormatter())\n",
    "\n",
    "    for ax in axs[:-1, 0].ravel():\n",
    "        ax.xaxis.set_major_formatter(NullFormatter())\n",
    "        ax.xaxis.set_minor_formatter(NullFormatter())\n",
    "        ax.tick_params(axis=\"x\", which=\"both\", length=0)\n",
    "\n",
    "    for ax in axs[:-1, 1:].ravel():\n",
    "        ax.xaxis.set_major_formatter(NullFormatter())\n",
    "        ax.xaxis.set_minor_formatter(NullFormatter())\n",
    "        ax.yaxis.set_major_formatter(NullFormatter())\n",
    "        ax.yaxis.set_minor_formatter(NullFormatter())\n",
    "        ax.tick_params(axis=\"both\", which=\"both\", length=0)\n",
    "\n",
    "    l_prior = Line2D([], [], c=color_prior, lw=lw, ls=\"solid\", label=\"Prior\")\n",
    "    l_post = Line2D([], [], c=\"k\", lw=lw, ls=\"solid\", label=\"Posterior\")\n",
    "    l_post_b = Line2D(\n",
    "        [], [], c=\"0.25\", lw=lw * 0.25, ls=\"solid\", label=\"Posterior (BayesBag)\"\n",
    "    )\n",
    "\n",
    "    legend = fig.legend(\n",
    "        handles=[l_prior, l_post, l_post_b], bbox_to_anchor=(0.3, 0.955)\n",
    "    )\n",
    "    legend.get_frame().set_linewidth(0.0)\n",
    "    legend.get_frame().set_alpha(0.0)\n",
    "\n",
    "    figfile = f\"{emulator_dir}/emulator_posterior.pdf\"\n",
    "    print(f\"Saving figure to {figfile}\")\n",
    "    fig.savefig(figfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed74959",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84257391",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e5a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02617982",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ce49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join([f\"-{k} {d[k]}\" for k in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3727a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 100)\n",
    "a = torch.distributions.Binomial(total_count=9,probs=torch.tensor(x)).log_prob(torch.tensor([6])).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62557bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "plt.plot(x, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec308745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
