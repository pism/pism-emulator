{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c11f2619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torchmetrics.utilities.checks import _check_same_shape\n",
    "from torchmetrics import Metric\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from pismemulator.metrics import AbsoluteError, absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26d54cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_observation(file=\"DMI-HIRHAM5_1980_MM.nc\", thin=1):\n",
    "    \"\"\"\n",
    "    Read and return Obs\n",
    "    \"\"\"\n",
    "\n",
    "    with xr.open_dataset(file) as Obs:\n",
    "\n",
    "        stacked = Obs.stack(z=(\"rlat\", \"rlon\"))\n",
    "        ncl_stacked = Obs.stack(z=(\"ncl4\", \"ncl5\"))\n",
    "\n",
    "        temp = stacked.tas.dropna(dim=\"z\").values\n",
    "        rainfall = stacked.rainfall.dropna(dim=\"z\").values / 1000\n",
    "        snowfall = stacked.snfall.dropna(dim=\"z\").values / 1000\n",
    "        smb = stacked.gld.dropna(dim=\"z\").values / 1000\n",
    "        refreeze = ncl_stacked.rfrz.dropna(dim=\"z\").values / 1000\n",
    "        melt = stacked.snmel.dropna(dim=\"z\").values / 1000\n",
    "        precip = rainfall + snowfall\n",
    "\n",
    "    return (\n",
    "        temp[...,::thin],\n",
    "        precip[...,::thin],\n",
    "        refreeze.sum(axis=0)[...,::thin],\n",
    "        snowfall.sum(axis=0)[...,::thin],\n",
    "        melt.sum(axis=0)[...,::thin],\n",
    "        smb.sum(axis=0)[...,::thin],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732ac367",
   "metadata": {},
   "outputs": [],
   "source": [
    "T, P, R, A, M, B = read_observation(thin=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e4ff67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 66377)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d882c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDDEmulator(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_parameters: int,\n",
    "        n_outputs: int,\n",
    "        hparams,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        n_layers = self.hparams.n_layers\n",
    "        n_hidden = self.hparams.n_hidden\n",
    "\n",
    "        if isinstance(n_hidden, int):\n",
    "            n_hidden = [n_hidden] * (n_layers - 1)\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.l_first = nn.Linear(n_parameters, n_hidden[0])\n",
    "        self.norm_first = nn.LayerNorm(n_hidden[0])\n",
    "        self.dropout_first = nn.Dropout(p=0.0)\n",
    "\n",
    "        models = []\n",
    "        for n in range(n_layers - 2):\n",
    "            models.append(\n",
    "                nn.Sequential(\n",
    "                    OrderedDict(\n",
    "                        [\n",
    "                            (\"Linear\", nn.Linear(n_hidden[n], n_hidden[n + 1])),\n",
    "                            (\"LayerNorm\", nn.LayerNorm(n_hidden[n + 1])),\n",
    "                            (\"Dropout\", nn.Dropout(p=0.1)),\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        self.dnn = nn.ModuleList(models)\n",
    "        self.l_last = nn.Linear(n_hidden[-1], n_outputs)\n",
    "\n",
    "        self.train_ae = AbsoluteError()\n",
    "        self.test_ae = AbsoluteError()\n",
    "\n",
    "    def forward(self, x, add_mean=False):\n",
    "        # Pass the input tensor through each of our operations\n",
    "\n",
    "        a = self.l_first(x)\n",
    "        a = self.norm_first(a)\n",
    "        a = self.dropout_first(a)\n",
    "        z = torch.relu(a)\n",
    "\n",
    "        for dnn in self.dnn:\n",
    "            a = dnn(z)\n",
    "            z = torch.relu(a) + z\n",
    "\n",
    "        return self.l_last(z)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"NNEmulator\")\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "        parser.add_argument(\"--n_hidden\", default=128)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=0.1)\n",
    "\n",
    "        return parent_parser\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), self.hparams.learning_rate, weight_decay=0.0\n",
    "        )\n",
    "        # This is an approximation to Doug's version:\n",
    "        scheduler = {\n",
    "            \"scheduler\": ExponentialLR(optimizer, 0.9975, verbose=True),\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, f, o, _ = batch\n",
    "        f_pred = self.forward(x)\n",
    "        loss = absolute_error(f_pred, f, o)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, f, o, o_0 = batch\n",
    "        f_pred = self.forward(x)\n",
    "\n",
    "        self.log(\"train_loss\", self.train_ae(f_pred, f, o))\n",
    "        self.log(\"test_loss\", self.test_ae(f_pred, f, o_0))\n",
    "\n",
    "        return {\"x\": x, \"f\": f, \"f_pred\": f_pred, \"o\": o, \"o_0\": o_0}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            self.train_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_loss\",\n",
    "            self.test_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83bc210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchPDDModel(torch.nn.modules.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    # Copyright (c) 2013--2018, Julien Seguinot <seguinot@vaw.baug.ethz.ch>\n",
    "    # GNU General Public License v3.0+ (https://www.gnu.org/licenses/gpl-3.0.txt)\n",
    "\n",
    "    A positive degree day model for glacier surface mass balance\n",
    "\n",
    "    Return a callable Positive Degree Day (PDD) model instance.\n",
    "\n",
    "    Model parameters are held as public attributes, and can be set using\n",
    "    corresponding keyword arguments at initialization time:\n",
    "\n",
    "    *pdd_factor_snow* : float\n",
    "        Positive degree-day factor for snow.\n",
    "    *pdd_factor_ice* : float\n",
    "        Positive degree-day factor for ice.\n",
    "    *refreeze_snow* : float\n",
    "        Refreezing fraction of melted snow.\n",
    "    *refreeze_ice* : float\n",
    "        Refreezing fraction of melted ice.\n",
    "    *temp_snow* : float\n",
    "        Temperature at which all precipitation falls as snow.\n",
    "    *temp_rain* : float\n",
    "        Temperature at which all precipitation falls as rain.\n",
    "    *interpolate_rule* : [ 'linear' | 'nearest' | 'zero' |\n",
    "                           'slinear' | 'quadratic' | 'cubic' ]\n",
    "        Interpolation rule passed to `scipy.interpolate.interp1d`.\n",
    "    *interpolate_n*: int\n",
    "        Number of points used in interpolations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pdd_factor_snow=3,\n",
    "        pdd_factor_ice=8,\n",
    "        refreeze_snow=0.0,\n",
    "        refreeze_ice=0.0,\n",
    "        temp_snow=0.0,\n",
    "        temp_rain=2.0,\n",
    "        interpolate_rule=\"linear\",\n",
    "        interpolate_n=52,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # set pdd model parameters\n",
    "        self.pdd_factor_snow = pdd_factor_snow\n",
    "        self.pdd_factor_ice = pdd_factor_ice\n",
    "        self.refreeze_snow = refreeze_snow\n",
    "        self.refreeze_ice = refreeze_ice\n",
    "        self.temp_snow = temp_snow\n",
    "        self.temp_rain = temp_rain\n",
    "        self.interpolate_rule = interpolate_rule\n",
    "        self.interpolate_n = interpolate_n\n",
    "\n",
    "    def forward(self, temp, prec, stdv=0.0):\n",
    "        \"\"\"Run the positive degree day model.\n",
    "\n",
    "        Use temperature, precipitation, and standard deviation of temperature\n",
    "        to compute the number of positive degree days, accumulation and melt\n",
    "        surface mass fluxes, and the resulting surface mass balance.\n",
    "\n",
    "        *temp*: array_like\n",
    "            Input near-surface air temperature in degrees Celcius.\n",
    "        *prec*: array_like\n",
    "            Input precipitation rate in meter per year.\n",
    "        *stdv*: array_like (default 0.0)\n",
    "            Input standard deviation of near-surface air temperature in Kelvin.\n",
    "\n",
    "        By default, inputs are N-dimensional arrays whose first dimension is\n",
    "        interpreted as time and as periodic. Arrays of dimensions\n",
    "        N-1 are interpreted as constant in time and expanded to N dimensions.\n",
    "        Arrays of dimension 0 and numbers are interpreted as constant in time\n",
    "        and space and will be expanded too. The largest input array determines\n",
    "        the number of dimensions N.\n",
    "\n",
    "        Return the number of positive degree days ('pdd'), surface mass balance\n",
    "        ('smb'), and many other output variables in a dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        # ensure numpy arrays\n",
    "        temp = torch.asarray(temp)\n",
    "        prec = torch.asarray(prec)\n",
    "        stdv = torch.asarray(stdv)\n",
    "\n",
    "        # expand arrays to the largest shape\n",
    "        maxshape = max(temp.shape, prec.shape, stdv.shape)\n",
    "        temp = self._expand(temp, maxshape)\n",
    "        prec = self._expand(prec, maxshape)\n",
    "        stdv = self._expand(stdv, maxshape)\n",
    "\n",
    "        # interpolate time-series\n",
    "        if self.interpolate_n >= 1:\n",
    "            temp = self._interpolate(temp)\n",
    "            prec = self._interpolate(prec)\n",
    "            stdv = self._interpolate(stdv)\n",
    "\n",
    "        # compute accumulation and pdd\n",
    "        accu_rate = self.accu_rate(temp, prec)\n",
    "        inst_pdd = self.inst_pdd(temp, stdv)\n",
    "\n",
    "        # initialize snow depth, melt and refreeze rates\n",
    "        snow_depth = torch.zeros_like(temp)\n",
    "        snow_melt_rate = torch.zeros_like(temp)\n",
    "        ice_melt_rate = torch.zeros_like(temp)\n",
    "        snow_refreeze_rate = torch.zeros_like(temp)\n",
    "        ice_refreeze_rate = torch.zeros_like(temp)\n",
    "\n",
    "        snow_depth[:-1] = torch.clone(snow_depth[1:])\n",
    "        snow_depth = snow_depth + accu_rate\n",
    "        snow_melt_rate, ice_melt_rate = self.melt_rates(snow_depth, inst_pdd)\n",
    "        snow_depth = snow_depth - snow_melt_rate\n",
    "\n",
    "        melt_rate = snow_melt_rate + ice_melt_rate\n",
    "        snow_refreeze_rate = self.refreeze_snow * snow_melt_rate\n",
    "        ice_refreeze_rate = self.refreeze_ice * ice_melt_rate\n",
    "        refreeze_rate = snow_refreeze_rate + ice_refreeze_rate\n",
    "        runoff_rate = melt_rate - refreeze_rate\n",
    "        inst_smb = accu_rate - runoff_rate\n",
    "\n",
    "        # output\n",
    "        return {\n",
    "            \"temp\": temp,\n",
    "            \"prec\": prec,\n",
    "            \"stdv\": stdv,\n",
    "            \"inst_pdd\": inst_pdd,\n",
    "            \"accu_rate\": accu_rate,\n",
    "            \"snow_melt_rate\": snow_melt_rate,\n",
    "            \"ice_melt_rate\": ice_melt_rate,\n",
    "            \"melt_rate\": melt_rate,\n",
    "            \"snow_refreeze_rate\": snow_refreeze_rate,\n",
    "            \"ice_refreeze_rate\": ice_refreeze_rate,\n",
    "            \"refreeze_rate\": refreeze_rate,\n",
    "            \"runoff_rate\": runoff_rate,\n",
    "            \"inst_smb\": inst_smb,\n",
    "            \"snow_depth\": snow_depth,\n",
    "            \"pdd\": self._integrate(inst_pdd),\n",
    "            \"accu\": self._integrate(accu_rate),\n",
    "            \"snow_melt\": self._integrate(snow_melt_rate),\n",
    "            \"ice_melt\": self._integrate(ice_melt_rate),\n",
    "            \"melt\": self._integrate(melt_rate),\n",
    "            \"runoff\": self._integrate(runoff_rate),\n",
    "            \"refreeze\": self._integrate(refreeze_rate),\n",
    "            \"smb\": self._integrate(inst_smb),\n",
    "        }\n",
    "\n",
    "    def _expand(self, array, shape):\n",
    "        \"\"\"Expand an array to the given shape\"\"\"\n",
    "        if array.shape == shape:\n",
    "            res = array\n",
    "        elif array.shape == (1, shape[1], shape[2]):\n",
    "            res = np.asarray([array[0]] * shape[0])\n",
    "        elif array.shape == shape[1:]:\n",
    "            res = np.asarray([array] * shape[0])\n",
    "        elif array.shape == ():\n",
    "            res = array * torch.ones(shape)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"could not expand array of shape %s to %s\" % (array.shape, shape)\n",
    "            )\n",
    "        return res\n",
    "\n",
    "    def _integrate(self, array):\n",
    "        \"\"\"Integrate an array over one year\"\"\"\n",
    "        return torch.sum(array, axis=0) / (self.interpolate_n - 1)\n",
    "\n",
    "    def _interpolate(self, array):\n",
    "        \"\"\"Interpolate an array through one year.\"\"\"\n",
    "\n",
    "        from scipy.interpolate import interp1d\n",
    "\n",
    "        rule = self.interpolate_rule\n",
    "        npts = self.interpolate_n\n",
    "        oldx = (torch.arange(len(array) + 2) - 0.5) / len(array)\n",
    "        oldy = torch.vstack((array[-1], array, array[0]))\n",
    "        newx = (torch.arange(npts) + 0.5) / npts  # use 0.0 for PISM-like behaviour\n",
    "        newy = interp1d(oldx, oldy, kind=rule, axis=0)(newx)\n",
    "\n",
    "        return torch.from_numpy(newy)\n",
    "\n",
    "    def inst_pdd(self, temp, stdv):\n",
    "        \"\"\"Compute instantaneous positive degree days from temperature.\n",
    "\n",
    "        Use near-surface air temperature and standard deviation to compute\n",
    "        instantaneous positive degree days (effective temperature for melt,\n",
    "        unit degrees C) using an integral formulation (Calov and Greve, 2005).\n",
    "\n",
    "        *temp*: array_like\n",
    "            Near-surface air temperature in degrees Celcius.\n",
    "        *stdv*: array_like\n",
    "            Standard deviation of near-surface air temperature in Kelvin.\n",
    "        \"\"\"\n",
    "\n",
    "        # compute positive part of temperature everywhere\n",
    "        positivepart = torch.greater(temp, 0) * temp\n",
    "\n",
    "        # compute Calov and Greve (2005) integrand, ignoring division by zero\n",
    "        normtemp = temp / (torch.sqrt(torch.tensor(2)) * stdv)\n",
    "        calovgreve = stdv / torch.sqrt(torch.tensor(2) * torch.pi) * torch.exp(\n",
    "            -(normtemp**2)\n",
    "        ) + temp / 2 * torch.erfc(-normtemp)\n",
    "\n",
    "        # use positive part where sigma is zero and Calov and Greve elsewhere\n",
    "        teff = torch.where(stdv == 0.0, positivepart, calovgreve)\n",
    "\n",
    "        # convert to degree-days\n",
    "        return teff * 365.242198781\n",
    "\n",
    "    def accu_rate(self, temp, prec):\n",
    "        \"\"\"Compute accumulation rate from temperature and precipitation.\n",
    "\n",
    "        The fraction of precipitation that falls as snow decreases linearly\n",
    "        from one to zero between temperature thresholds defined by the\n",
    "        `temp_snow` and `temp_rain` attributes.\n",
    "\n",
    "        *temp*: array_like\n",
    "            Near-surface air temperature in degrees Celcius.\n",
    "        *prec*: array_like\n",
    "            Precipitation rate in meter per year.\n",
    "        \"\"\"\n",
    "\n",
    "        # compute snow fraction as a function of temperature\n",
    "        reduced_temp = (self.temp_rain - temp) / (self.temp_rain - self.temp_snow)\n",
    "        snowfrac = torch.clip(reduced_temp, 0, 1)\n",
    "\n",
    "        # return accumulation rate\n",
    "        return snowfrac * prec\n",
    "\n",
    "    def melt_rates(self, snow, pdd):\n",
    "        \"\"\"Compute melt rates from snow precipitation and pdd sum.\n",
    "\n",
    "        Snow melt is computed from the number of positive degree days (*pdd*)\n",
    "        and the `pdd_factor_snow` model attribute. If all snow is melted and\n",
    "        some energy (PDD) remains, ice melt is computed using `pdd_factor_ice`.\n",
    "\n",
    "        *snow*: array_like\n",
    "            Snow precipitation rate.\n",
    "        *pdd*: array_like\n",
    "            Number of positive degree days.\n",
    "        \"\"\"\n",
    "\n",
    "        # parse model parameters for readability\n",
    "        ddf_snow = self.pdd_factor_snow / 1e3\n",
    "        ddf_ice = self.pdd_factor_ice / 1e3\n",
    "\n",
    "        # compute a potential snow melt\n",
    "        pot_snow_melt = ddf_snow * pdd\n",
    "\n",
    "        # effective snow melt can't exceed amount of snow\n",
    "        snow_melt = torch.minimum(snow, pot_snow_melt)\n",
    "\n",
    "        # ice melt is proportional to excess snow melt\n",
    "        ice_melt = (pot_snow_melt - snow_melt) * ddf_ice / ddf_snow\n",
    "\n",
    "        # return melt rates\n",
    "        return (snow_melt, ice_melt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0859e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.sample import saltelli\n",
    "from pyDOE import lhs\n",
    "from scipy.stats.distributions import truncnorm, gamma, uniform, randint\n",
    "method = \"lhs\"\n",
    "n_prior_samples = 100\n",
    "np.random.seed(2)\n",
    "\n",
    "distributions = {\n",
    "    \"f_snow\": uniform(\n",
    "        loc=2.0, scale=4.0\n",
    "    ), \n",
    "    \"f_ice\": uniform(\n",
    "        loc=3.0, scale=9\n",
    "    ),  # uniform between 3 and 3.5  AS16 best value: 3.25\n",
    "    \"refreeze\": uniform(loc=0, scale=1.0),  # uniform between 0.25 and 0.95\n",
    "}\n",
    "# Names of all the variables\n",
    "keys = [x for x in distributions.keys()]\n",
    "\n",
    "# Describe the Problem\n",
    "problem = {\"num_vars\": len(keys), \"names\": keys, \"bounds\": [[0, 1]] * len(keys)}\n",
    "\n",
    "# Generate uniform samples (i.e. one unit hypercube)\n",
    "if method == \"saltelli\":\n",
    "    unif_sample = saltelli.sample(problem, n_prior_samples, calc_second_order=False)\n",
    "elif method == \"lhs\":\n",
    "    unif_sample = lhs(len(keys), n_prior_samples)\n",
    "else:\n",
    "    print(f\"Method {method} not available\")\n",
    "\n",
    "# To hold the transformed variables\n",
    "dist_sample = np.zeros_like(unif_sample)\n",
    "\n",
    "# Now transform the unit hypercube to the prescribed distributions\n",
    "# For each variable, transform with the inverse of the CDF (inv(CDF)=ppf)\n",
    "for i, key in enumerate(keys):\n",
    "    dist_sample[:, i] = distributions[key].ppf(unif_sample[:, i])\n",
    "\n",
    "# Save to CSV file using Pandas DataFrame and to_csv method\n",
    "header = keys\n",
    "# Convert to Pandas dataframe, append column headers, output as csv\n",
    "df = pd.DataFrame(data=dist_sample, columns=header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d52677e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    nt, ns = T.shape\n",
    "    \n",
    "    std_dev = np.zeros_like(T)\n",
    "    X = []\n",
    "    Y = []\n",
    "    for k, row in df.iterrows():   \n",
    "        pdd = TorchPDDModel(\n",
    "            pdd_factor_snow=row[\"f_snow\"],\n",
    "            pdd_factor_ice=row[\"f_ice\"],\n",
    "            refreeze_snow=row[\"refreeze\"],\n",
    "            refreeze_ice=row[\"refreeze\"],\n",
    "        )\n",
    "        result = pdd(T, P, std_dev)\n",
    "\n",
    "        M_obs = result[\"melt\"]\n",
    "        A_obs = result[\"accu\"]\n",
    "        R_obs = result[\"refreeze\"]\n",
    "        \n",
    "        Y.append(torch.vstack((M_obs, A_obs, R_obs)).T)\n",
    "        X.append(torch.from_numpy(np.hstack((T.T, P.T, std_dev.T, np.tile(row.values, (ns, 1))))))\n",
    "\n",
    "    X = torch.vstack(X).type(torch.FloatTensor)\n",
    "    Y = torch.vstack(Y).type(torch.FloatTensor)\n",
    "    n_samples, n_parameters = X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cd2201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13275400, 39]) torch.Size([13275400, 3])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2a45752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.2993) tensor(50.2006)\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "X_mean = X.mean(axis=0)\n",
    "X_std = X.std(axis=0)\n",
    "X_s = (X - X_mean) / X_std\n",
    "X_s[(torch.isnan(X_s)) | torch.isneginf(X_s)] = 0\n",
    "print(X_s.min(), X_s.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d3d5fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50.2006)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_s.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "237942c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "    import os\n",
    "    from scipy.stats import dirichlet\n",
    "\n",
    "    model_index = 0\n",
    "    torch.manual_seed(0)\n",
    "    pl.seed_everything(0)\n",
    "    np.random.seed(model_index)\n",
    "    emulator_dir = \"pddemulator\"\n",
    "\n",
    "    if not os.path.isdir(emulator_dir):\n",
    "        os.makedirs(emulator_dir)\n",
    "        os.makedirs(os.path.join(emulator_dir, \"emulator\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f4d63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) 2021 Andy Aschwanden, Douglas C Brinkerhoff\n",
    "#\n",
    "# This file is part of pism-emulator.\n",
    "#\n",
    "# PISM-EMULATOR is free software; you can redistribute it and/or modify it under the\n",
    "# terms of the GNU General Public License as published by the Free Software\n",
    "# Foundation; either version 3 of the License, or (at your option) any later\n",
    "# version.\n",
    "#\n",
    "# PISM-EMULATOR is distributed in the hope that it will be useful, but WITHOUT ANY\n",
    "# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n",
    "# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n",
    "# details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with PISM; if not, write to the Free Software\n",
    "# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torchmetrics.utilities.checks import _check_same_shape\n",
    "from torchmetrics import Metric\n",
    "\n",
    "\n",
    "def _absolute_error_update(\n",
    "    preds: Tensor, target: Tensor, omegas: Tensor) -> Tensor:\n",
    "    _check_same_shape(preds, target)\n",
    "    diff = torch.abs(preds - target)\n",
    "    sum_abs_error = torch.sum(diff * diff, axis=1)\n",
    "    absolute_error = torch.sum(sum_abs_error * omegas.squeeze())\n",
    "    return absolute_error\n",
    "\n",
    "\n",
    "def _absolute_error_compute(absolute_error) -> Tensor:\n",
    "    return absolute_error\n",
    "\n",
    "\n",
    "def absolute_error(\n",
    "    preds: Tensor, target: Tensor, omegas: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes squared absolute error\n",
    "    Args:\n",
    "        preds: estimated labels\n",
    "        target: ground truth labels\n",
    "        omegas: weights\n",
    "        area: area of each cell\n",
    "    Return:\n",
    "        Tensor with absolute error\n",
    "    Example:\n",
    "        >>> x = torch.tensor([[0, 1, 2, 3], [1, 2, 3, 4]]).T\n",
    "        >>> y = torch.tensor([[0, 1, 2, 1], [2, 3, 4, 4]]).T\n",
    "        >>> o = torch.tensor([0.25, 0.25, 0.3, 0.2])\n",
    "        >>> a = torch.tensor([0.25, 0.25])\n",
    "        >>> absolute_error(x, y, o, a)\n",
    "        tensor(0.4000)\n",
    "    \"\"\"\n",
    "    sum_abs_error = _absolute_error_update(preds, target, omegas)\n",
    "    return _absolute_error_compute(sum_abs_error)\n",
    "\n",
    "\n",
    "class AbsoluteError(Metric):\n",
    "    def __init__(self, compute_on_step: bool = True, dist_sync_on_step=False):\n",
    "        # call `self.add_state`for every internal state that is needed for the metrics computations\n",
    "        # dist_reduce_fx indicates the function that should be used to reduce\n",
    "        # state from multiple processes\n",
    "        super().__init__(\n",
    "            compute_on_step=compute_on_step, dist_sync_on_step=dist_sync_on_step\n",
    "        )\n",
    "\n",
    "        self.add_state(\"sum_abs_error\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: Tensor, target: Tensor, omegas: Tensor):\n",
    "        \"\"\"\n",
    "        Update state with predictions and targets, and area.\n",
    "        Args:\n",
    "            preds: Predictions from model\n",
    "            target: Ground truth values\n",
    "            omegas: Weights\n",
    "            area: Area of each cell\n",
    "        \"\"\"\n",
    "        sum_abs_error = _absolute_error_update(preds, target, omegas)\n",
    "        self.sum_abs_error += sum_abs_error\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Computes absolute error over state.\n",
    "        \"\"\"\n",
    "        return _absolute_error_compute(self.sum_abs_error)\n",
    "\n",
    "    @property\n",
    "    def is_differentiable(self):\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05a2f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDDDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X,\n",
    "        Y,\n",
    "        omegas,\n",
    "        omegas_0,\n",
    "        batch_size: int = 128,\n",
    "        train_size: float = 0.9,\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.omegas = omegas\n",
    "        self.omegas_0 = omegas_0\n",
    "        self.batch_size = batch_size\n",
    "        self.train_size = train_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "\n",
    "        all_data = TensorDataset(self.X, self.Y, self.omegas, self.omegas_0)\n",
    "        self.all_data = all_data\n",
    "\n",
    "        training_data, val_data = train_test_split(\n",
    "            all_data, train_size=self.train_size, random_state=0\n",
    "        )\n",
    "        self.training_data = training_data\n",
    "        self.test_data = training_data\n",
    "\n",
    "        self.val_data = val_data\n",
    "        train_all_loader = DataLoader(\n",
    "            dataset=all_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.train_all_loader = train_all_loader\n",
    "        val_all_loader = DataLoader(\n",
    "            dataset=all_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.val_all_loader = val_all_loader\n",
    "        train_loader = DataLoader(\n",
    "            dataset=training_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = train_loader\n",
    "        val_loader = DataLoader(\n",
    "            dataset=val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def prepare_data(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self.val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64b35190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | l_first       | Linear        | 5.1 K \n",
      "1 | norm_first    | LayerNorm     | 256   \n",
      "2 | dropout_first | Dropout       | 0     \n",
      "3 | dnn           | ModuleList    | 50.3 K\n",
      "4 | l_last        | Linear        | 387   \n",
      "5 | train_ae      | AbsoluteError | 0     \n",
      "6 | test_ae       | AbsoluteError | 0     \n",
      "------------------------------------------------\n",
      "56.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "56.1 K    Total params\n",
      "0.224     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 0:  30%|█████████████████████████████████████▍                                                                                      | 62638/207430 [05:35<12:54, 186.96it/s, loss=5.61e-05, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/miniconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1051: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "    print(f\"Training model {model_index}\")\n",
    "    omegas = torch.Tensor(dirichlet.rvs(np.ones(n_samples))).T\n",
    "    omegas = omegas.type(torch.FloatTensor)\n",
    "    omegas_0 = torch.ones_like(omegas) / len(omegas)\n",
    "    area = torch.ones_like(omegas)\n",
    "    train_size = 1.0\n",
    "    num_workers = 8\n",
    "    hparams = {\"n_layers\": 5, \"n_hidden\": 128, \"batch_size\": 128, \"learning_rate\": 0.1}\n",
    "    \n",
    "    if train_size == 1.0:\n",
    "        data_loader = PDDDataModule(X_s, Y, omegas, omegas_0, num_workers=num_workers)\n",
    "    else:\n",
    "        data_loader = PDDDataModule(\n",
    "            X_s, Y, omegas, omegas_0, train_size=train_size, num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    data_loader.setup()\n",
    "    e = PDDEmulator(\n",
    "        n_parameters,\n",
    "        3,\n",
    "        hparams,\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        auto_lr_find=True,\n",
    "        max_epochs=100,\n",
    "        gpus=1,\n",
    "#        deterministic=True,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    if train_size == 1.0:\n",
    "        train_loader = data_loader.train_all_loader\n",
    "        val_loader = data_loader.val_all_loader\n",
    "    else:\n",
    "        train_loader = data_loader.train_loader\n",
    "        val_loader = data_loader.val_loader\n",
    "\n",
    "        \n",
    "    # lr_finder = trainer.tuner.lr_find(e, train_loader, val_loader)\n",
    "    # fig = lr_finder.plot(suggest=True) # Plot\n",
    "    # fig.show()\n",
    "    trainer.fit(e, train_loader, val_loader)\n",
    "    torch.save(e.state_dict(), f\"{emulator_dir}/emulator/emulator_{model_index}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e2e05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24e61c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MALASampler(object):\n",
    "    \"\"\"\n",
    "    MALA Sampler\n",
    "\n",
    "    Author: Douglas C Brinkerhoff, University of Montana\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model, alpha_b=3.0, beta_b=3.0, alpha=0.01, emulator_dir=\"./emulator\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model.eval()\n",
    "        self.alpha = alpha\n",
    "        self.alpha_b = alpha_b\n",
    "        self.beta_b = beta_b\n",
    "        self.emulator_dir = emulator_dir\n",
    "\n",
    "    def find_MAP(self, X, X_I, Y_target, X_min, X_max, n_iters=50, print_interval=10):\n",
    "        print(\"***********************************************\")\n",
    "        print(\"***********************************************\")\n",
    "        print(\"Finding MAP point\")\n",
    "        print(\"***********************************************\")\n",
    "        print(\"***********************************************\")\n",
    "        # Line search distances\n",
    "        alphas = np.logspace(-4, 0, 11)\n",
    "        # Find MAP point\n",
    "        for i in range(n_iters):\n",
    "            log_pi, g, _, Hinv, log_det_Hinv = self.get_log_like_gradient_and_hessian(\n",
    "                X, X_I, Y_target, X_min, X_max, compute_hessian=True\n",
    "            )\n",
    "            p = Hinv @ -g\n",
    "            alpha_index = np.nanargmin(\n",
    "                [\n",
    "                    self.get_log_like_gradient_and_hessian(\n",
    "                        X + alpha * p, X_I, Y_target, X_min, X_max, compute_hessian=False\n",
    "                    )\n",
    "                    .detach()\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                    for alpha in alphas\n",
    "                ]\n",
    "            )\n",
    "            mu = X + alphas[alpha_index] * p\n",
    "            X.data = mu.data\n",
    "            if i % print_interval == 0:\n",
    "                print(\"===============================================\")\n",
    "                print(f\"iter: {i:d}, log(P): {log_pi:.1f}\\n\")\n",
    "                print(\n",
    "                    \"\".join(\n",
    "                        [\n",
    "                            f\"{key}: {(val * std + mean):.3f}\\n\"\n",
    "                            for key, val, std, mean in zip(\n",
    "                                X_keys,\n",
    "                                X.data.cpu().numpy(),\n",
    "                                X_P_std,\n",
    "                                X_P_mean,\n",
    "                            )\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "                print(\"===============================================\")\n",
    "        return X\n",
    "\n",
    "    def V(self, X, X_I, Y_target, X_bar):\n",
    "        # model result is in log space\n",
    "        X_IP = torch.hstack((X_P, X_I))\n",
    "        Y_pred = self.model(X_IP)\n",
    "        r = Y_pred - Y_target\n",
    "        L1 = torch.sum(\n",
    "            np.log(gamma((nu + 1) / 2.0))\n",
    "            - np.log(gamma(nu / 2.0))\n",
    "            - np.log(np.sqrt(np.pi * nu) * sigma_hat)\n",
    "            - (nu + 1) / 2.0 * torch.log(1 + 1.0 / nu * (r / sigma_hat) ** 2)\n",
    "        )\n",
    "        L2 = torch.sum(\n",
    "            (self.alpha_b - 1) * torch.log(X_bar)\n",
    "            + (self.beta_b - 1) * torch.log(1 - X_bar)\n",
    "        )\n",
    "\n",
    "        return -(self.alpha * L1 + L2)\n",
    "\n",
    "    def get_log_like_gradient_and_hessian(\n",
    "        self, X, X_I, Y_target, X_min, X_max, eps=1e-2, compute_hessian=False\n",
    "    ):\n",
    "\n",
    "        X_bar = (X - X_min) / (X_max - X_min)\n",
    "        log_pi = self.V(X, X_I, Y_target, X_bar)\n",
    "        if compute_hessian:\n",
    "            g = torch.autograd.grad(log_pi, X, retain_graph=True, create_graph=True)[0]\n",
    "            H = torch.stack(\n",
    "                [torch.autograd.grad(e, X, retain_graph=True)[0] for e in g]\n",
    "            )\n",
    "            lamda, Q = torch.linalg.eig(H)\n",
    "            lamda, Q = lamda.type(torch.float), Q.type(torch.float)\n",
    "            lamda_prime = torch.sqrt(lamda ** 2 + eps)\n",
    "            lamda_prime_inv = 1.0 / torch.sqrt(lamda ** 2 + eps)\n",
    "            H = Q @ torch.diag(lamda_prime) @ Q.T\n",
    "            Hinv = Q @ torch.diag(lamda_prime_inv) @ Q.T\n",
    "            log_det_Hinv = torch.sum(torch.log(lamda_prime_inv))\n",
    "            return log_pi, g, H, Hinv, log_det_Hinv\n",
    "        else:\n",
    "            return log_pi\n",
    "\n",
    "    def draw_sample(self, mu, cov, eps=1e-10):\n",
    "        L = torch.linalg.cholesky(cov + eps * torch.eye(cov.shape[0], device=device))\n",
    "        return mu + L @ torch.randn(L.shape[0], device=device)\n",
    "\n",
    "    def get_proposal_likelihood(self, Y, mu, inverse_cov, log_det_cov):\n",
    "        return -0.5 * log_det_cov - 0.5 * (Y - mu) @ inverse_cov @ (Y - mu)\n",
    "\n",
    "    def MALA_step(self, X, X_I, Y_target, X_min, X_max, h, local_data=None):\n",
    "        if local_data is not None:\n",
    "            pass\n",
    "        else:\n",
    "            local_data = self.get_log_like_gradient_and_hessian(\n",
    "                X, X_I, Y_target, X_min, X_max, compute_hessian=True\n",
    "            )\n",
    "\n",
    "        log_pi, _, H, Hinv, log_det_Hinv = local_data\n",
    "\n",
    "        X_ = self.draw_sample(X, 2 * h * Hinv).detach()\n",
    "        X_.requires_grad = True\n",
    "\n",
    "        log_pi_ = self.get_log_like_gradient_and_hessian(\n",
    "            X_, X_I, Y_target, X_min, X_max, compute_hessian=False\n",
    "        )\n",
    "\n",
    "        logq = self.get_proposal_likelihood(X_, X, H / (2 * h), log_det_Hinv)\n",
    "        logq_ = self.get_proposal_likelihood(X, X_, H / (2 * h), log_det_Hinv)\n",
    "\n",
    "        log_alpha = -log_pi_ + logq_ + log_pi - logq\n",
    "        alpha = torch.exp(min(log_alpha, torch.tensor([0.0], device=device)))\n",
    "        u = torch.rand(1, device=device)\n",
    "        if u <= alpha and log_alpha != np.inf:\n",
    "            X.data = X_.data\n",
    "            local_data = self.get_log_like_gradient_and_hessian(\n",
    "                X, X_I, Y_target, X_min, X_max, compute_hessian=True\n",
    "            )\n",
    "            s = 1\n",
    "        else:\n",
    "            s = 0\n",
    "        return X, local_data, s\n",
    "\n",
    "    def MALA(\n",
    "        self,\n",
    "        X,\n",
    "        X_I,\n",
    "        Y_target,\n",
    "        n_iters=10001,\n",
    "        h=0.1,\n",
    "        h_max=1.0,\n",
    "        acc_target=0.25,\n",
    "        k=0.01,\n",
    "        beta=0.99,\n",
    "        model_index=0,\n",
    "        save_interval=1000,\n",
    "        print_interval=50,\n",
    "    ):\n",
    "        print(\"***********************************************\")\n",
    "        print(\"***********************************************\")\n",
    "        print(\n",
    "            \"Running Metropolis-Adjusted Langevin Algorithm for model index {0}\".format(\n",
    "                model_index\n",
    "            )\n",
    "        )\n",
    "        print(\"***********************************************\")\n",
    "        print(\"***********************************************\")\n",
    "\n",
    "        posterior_dir = f\"{self.emulator_dir}/posterior_samples/\"\n",
    "        if not os.path.isdir(posterior_dir):\n",
    "            os.makedirs(posterior_dir)\n",
    "\n",
    "        local_data = None\n",
    "        m_vars = []\n",
    "        acc = acc_target\n",
    "        print(n_iters)\n",
    "        for i in range(n_iters):\n",
    "            X, local_data, s = self.MALA_step(\n",
    "                X, X_I, Y_target, X_min, X_max, h, local_data=local_data\n",
    "            )\n",
    "            m_vars.append(X.detach())\n",
    "            acc = beta * acc + (1 - beta) * s\n",
    "            h = min(h * (1 + k * np.sign(acc - acc_target)), h_max)\n",
    "            if i % print_interval == 0:\n",
    "                print(\"===============================================\")\n",
    "                print(\n",
    "                    \"sample: {0:d}, acc. rate: {1:4.2f}, log(P): {2:6.1f}\".format(\n",
    "                        i, acc, local_data[0].item()\n",
    "                    )\n",
    "                )\n",
    "                print(\n",
    "                    \" \".join(\n",
    "                        [\n",
    "                            f\"{key}: {(val * std + mean):.3f}\\n\"\n",
    "                            for key, val, std, mean in zip(\n",
    "                                X_keys,\n",
    "                                X.data.cpu().numpy(),\n",
    "                                X_P_std,\n",
    "                                X_P_mean,\n",
    "                            )\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "                print(\"===============================================\")\n",
    "\n",
    "            if i % save_interval == 0:\n",
    "                print(\"///////////////////////////////////////////////\")\n",
    "                print(\"Saving samples for model {0}\".format(model_index))\n",
    "                print(\"///////////////////////////////////////////////\")\n",
    "                X_posterior = torch.stack(m_vars).cpu().numpy()\n",
    "                df = pd.DataFrame(\n",
    "                    data=X_posterior.astype(\"float32\") * X_P_std.cpu().numpy()\n",
    "                    + X_P_mean.cpu().numpy(),\n",
    "                    columns=X_keys,\n",
    "                )\n",
    "                df.to_csv(\n",
    "                    posterior_dir + \"X_posterior_model_{0}.csv.gz\".format(model_index),\n",
    "                    compression=\"infer\",\n",
    "                )\n",
    "        X_posterior = torch.stack(m_vars).cpu().numpy()\n",
    "        return X_posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a069a21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "***********************************************\n",
      "Finding MAP point\n",
      "***********************************************\n",
      "***********************************************\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 6.33 GiB (GPU 0; 23.69 GiB total capacity; 16.77 GiB already allocated; 4.99 GiB free; 16.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m U_target \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     51\u001b[0m mala \u001b[38;5;241m=\u001b[39m MALASampler(e\u001b[38;5;241m.\u001b[39mto(device), emulator_dir\u001b[38;5;241m=\u001b[39memulator_dir)\n\u001b[0;32m---> 52\u001b[0m X_map \u001b[38;5;241m=\u001b[39m \u001b[43mmala\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_MAP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_I\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# To reproduce the paper, n_iters should be 10^5\u001b[39;00m\n\u001b[1;32m     54\u001b[0m X_posterior \u001b[38;5;241m=\u001b[39m mala\u001b[38;5;241m.\u001b[39mMALA(\n\u001b[1;32m     55\u001b[0m     X_map,\n\u001b[1;32m     56\u001b[0m     X_I,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     print_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     62\u001b[0m )\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mMALASampler.find_MAP\u001b[0;34m(self, X, X_I, Y_target, X_min, X_max, n_iters, print_interval)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Find MAP point\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iters):\n\u001b[0;32m---> 28\u001b[0m     log_pi, g, _, Hinv, log_det_Hinv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_log_like_gradient_and_hessian\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_I\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     p \u001b[38;5;241m=\u001b[39m Hinv \u001b[38;5;241m@\u001b[39m \u001b[38;5;241m-\u001b[39mg\n\u001b[1;32m     32\u001b[0m     alpha_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanargmin(\n\u001b[1;32m     33\u001b[0m         [\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_log_like_gradient_and_hessian(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m         ]\n\u001b[1;32m     42\u001b[0m     )\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mMALASampler.get_log_like_gradient_and_hessian\u001b[0;34m(self, X, X_I, Y_target, X_min, X_max, eps, compute_hessian)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_log_like_gradient_and_hessian\u001b[39m(\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, X_I, Y_target, X_min, X_max, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, compute_hessian\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     84\u001b[0m ):\n\u001b[1;32m     86\u001b[0m     X_bar \u001b[38;5;241m=\u001b[39m (X \u001b[38;5;241m-\u001b[39m X_min) \u001b[38;5;241m/\u001b[39m (X_max \u001b[38;5;241m-\u001b[39m X_min)\n\u001b[0;32m---> 87\u001b[0m     log_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_I\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compute_hessian:\n\u001b[1;32m     89\u001b[0m         g \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(log_pi, X, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mMALASampler.V\u001b[0;34m(self, X, X_I, Y_target, X_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mV\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, X_I, Y_target, X_bar):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# model result is in log space\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     X_IP \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack((X_P, X_I))\n\u001b[0;32m---> 67\u001b[0m     Y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_IP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     r \u001b[38;5;241m=\u001b[39m Y_pred \u001b[38;5;241m-\u001b[39m Y_target\n\u001b[1;32m     69\u001b[0m     L1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m     70\u001b[0m         np\u001b[38;5;241m.\u001b[39mlog(gamma((nu \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m))\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(gamma(nu \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m))\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m*\u001b[39m nu) \u001b[38;5;241m*\u001b[39m sigma_hat)\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;241m-\u001b[39m (nu \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m nu \u001b[38;5;241m*\u001b[39m (r \u001b[38;5;241m/\u001b[39m sigma_hat) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     74\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mPDDEmulator.forward\u001b[0;34m(self, x, add_mean)\u001b[0m\n\u001b[1;32m     46\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first(a)\n\u001b[1;32m     47\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_first(a)\n\u001b[0;32m---> 48\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dnn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdnn:\n\u001b[1;32m     51\u001b[0m     a \u001b[38;5;241m=\u001b[39m dnn(z)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 6.33 GiB (GPU 0; 23.69 GiB total capacity; 16.77 GiB already allocated; 4.99 GiB free; 16.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "    from scipy.stats import beta\n",
    "    from scipy.special import gamma\n",
    "\n",
    "    X_keys = keys\n",
    "    device=\"cuda\"\n",
    "    nu = 1\n",
    "    n_iters=10000\n",
    "    X_P = X_s[:,-3::].to(device)\n",
    "    X_I = X_s[:,:-3].to(device)\n",
    "    X_P_mean = X_mean[-3::].to(device)\n",
    "    X_P_std = X_std[-3::].to(device)\n",
    "\n",
    "    \n",
    "    X_min = X_P.cpu().numpy().min(axis=0)\n",
    "    X_max = X_P.cpu().numpy().max(axis=0)\n",
    "\n",
    "    sigma = 0.1\n",
    "\n",
    "    rho = 1.0 / (1e4**2)\n",
    "    point_area = 1800 ** 2\n",
    "    K = point_area * rho\n",
    "    sigma_hat = np.sqrt(sigma**2 / K**2)\n",
    "\n",
    "    # Eq 52\n",
    "    # this is 2.0 in the paper\n",
    "    alpha_b = 3.0\n",
    "    beta_b = 3.0\n",
    "    X_prior = (\n",
    "        beta.rvs(alpha_b, beta_b, size=(100000, 3))\n",
    "        * (X_max - X_min)\n",
    "        + X_min\n",
    "    )\n",
    "    X_0 = torch.tensor(\n",
    "        X_prior.mean(axis=0), requires_grad=True, dtype=torch.float, device=device\n",
    "    )\n",
    "    # This is required for\n",
    "    # X_bar = (X - X_min) / (X_max - X_min)\n",
    "    # to work\n",
    "\n",
    "    X_min = torch.tensor(X_min, dtype=torch.float32, device=device)\n",
    "    X_max = torch.tensor(X_max, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Needs\n",
    "    # alpha_b, beta_b: float\n",
    "    # alpha: float\n",
    "    # nu: float\n",
    "    # gamma\n",
    "    # sigma_hat\n",
    "    U_target = Y.to(device)\n",
    "\n",
    "    mala = MALASampler(e.to(device), emulator_dir=emulator_dir)\n",
    "    X_map = mala.find_MAP(X_0, X_I, U_target, X_min, X_max)\n",
    "    # To reproduce the paper, n_iters should be 10^5\n",
    "    X_posterior = mala.MALA(\n",
    "        X_map,\n",
    "        X_I,\n",
    "        U_target,\n",
    "        n_iters=n_iters,\n",
    "        model_index=int(model_index),\n",
    "        save_interval=1000,\n",
    "        print_interval=100,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea2f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f765bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s * X_std + X_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e292ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_P[:, 0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed74959",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84257391",
   "metadata": {},
   "outputs": [],
   "source": [
    "d          ={ \"stress_balance.blatter.coarsening_factor\": 4,\n",
    "            \"blatter_Mz\": 17,\n",
    "            \"bp_ksp_type\": \"gmres\",\n",
    "            \"bp_pc_type\": \"mg\",\n",
    "            \"bp_pc_mg_levels\": 3,\n",
    "            \"bp_mg_levels_ksp_type\": \"richardson\",\n",
    "            \"bp_mg_levels_pc_type\": \"sor\",\n",
    "            \"bp_mg_coarse_ksp_type\": \"gmres\",\n",
    "            \"bp_mg_coarse_pc_type\": \"bjacobi\",\n",
    "            \"bp_snes_monitor_ratio\": \"\",\n",
    "            \"bp_ksp_monitor\": \"\",\n",
    "            \"bp_ksp_view_singularvalues\": \"\",\n",
    "            \"bp_snes_ksp_ew\": 1,\n",
    "            \"bp_snes_ksp_ew_version\": 3,\n",
    "            \"stress_balance.ice_free_thickness_standard\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e5a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02617982",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"-{key} {val}\" for key,val in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ce49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join([f\"-{k} {d[k]}\" for k in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3727a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 100)\n",
    "a = torch.distributions.Binomial(total_count=9,probs=torch.tensor(x)).log_prob(torch.tensor([6])).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62557bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "plt.plot(x, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec308745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
