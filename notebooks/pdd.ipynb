{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c11f2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torchmetrics.utilities.checks import _check_same_shape\n",
    "from torchmetrics import Metric\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from pismemulator.metrics import AbsoluteError, absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d882c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDDEmulator(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_parameters: int,\n",
    "        hparams,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        n_layers = self.hparams.n_layers\n",
    "        n_hidden = self.hparams.n_hidden\n",
    "\n",
    "        if isinstance(n_hidden, int):\n",
    "            n_hidden = [n_hidden] * (n_layers - 1)\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.l_first = nn.Linear(n_parameters, n_hidden[0])\n",
    "        self.norm_first = nn.LayerNorm(n_hidden[0])\n",
    "        self.dropout_first = nn.Dropout(p=0.0)\n",
    "\n",
    "        models = []\n",
    "        for n in range(n_layers - 2):\n",
    "            models.append(\n",
    "                nn.Sequential(\n",
    "                    OrderedDict(\n",
    "                        [\n",
    "                            (\"Linear\", nn.Linear(n_hidden[n], n_hidden[n + 1])),\n",
    "                            (\"LayerNorm\", nn.LayerNorm(n_hidden[n + 1])),\n",
    "                            (\"Dropout\", nn.Dropout(p=0.1)),\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        self.dnn = nn.ModuleList(models)\n",
    "        self.l_last = nn.Linear(n_hidden[-1], 1)\n",
    "\n",
    "        self.train_ae = AbsoluteError()\n",
    "        self.test_ae = AbsoluteError()\n",
    "\n",
    "    def forward(self, x, add_mean=False):\n",
    "        # Pass the input tensor through each of our operations\n",
    "\n",
    "        a = self.l_first(x)\n",
    "        a = self.norm_first(a)\n",
    "        a = self.dropout_first(a)\n",
    "        z = torch.relu(a)\n",
    "\n",
    "        for dnn in self.dnn:\n",
    "            a = dnn(z)\n",
    "            z = torch.relu(a) + z\n",
    "\n",
    "        return self.l_last(z)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"NNEmulator\")\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "        parser.add_argument(\"--n_hidden\", default=128)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=0.01)\n",
    "\n",
    "        return parent_parser\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), self.hparams.learning_rate, weight_decay=0.0\n",
    "        )\n",
    "        # This is an approximation to Doug's version:\n",
    "        scheduler = {\n",
    "            \"scheduler\": ExponentialLR(optimizer, 0.9975, verbose=True),\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, f, o, _ = batch\n",
    "        f_pred = self.forward(x)\n",
    "        loss = absolute_error(f_pred, f, o)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, f, o, o_0 = batch\n",
    "        f_pred = self.forward(x)\n",
    "\n",
    "        self.log(\"train_loss\", self.train_ae(f_pred, f, o))\n",
    "        self.log(\"test_loss\", self.test_ae(f_pred, f, o_0))\n",
    "\n",
    "        return {\"x\": x, \"f\": f, \"f_pred\": f_pred, \"o\": o, \"o_0\": o_0}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            self.train_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_loss\",\n",
    "            self.test_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "83bc210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchPDDModel(torch.nn.modules.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    # Copyright (c) 2013--2018, Julien Seguinot <seguinot@vaw.baug.ethz.ch>\n",
    "    # GNU General Public License v3.0+ (https://www.gnu.org/licenses/gpl-3.0.txt)\n",
    "\n",
    "    A positive degree day model for glacier surface mass balance\n",
    "\n",
    "    Return a callable Positive Degree Day (PDD) model instance.\n",
    "\n",
    "    Model parameters are held as public attributes, and can be set using\n",
    "    corresponding keyword arguments at initialization time:\n",
    "\n",
    "    *pdd_factor_snow* : float\n",
    "        Positive degree-day factor for snow.\n",
    "    *pdd_factor_ice* : float\n",
    "        Positive degree-day factor for ice.\n",
    "    *refreeze_snow* : float\n",
    "        Refreezing fraction of melted snow.\n",
    "    *refreeze_ice* : float\n",
    "        Refreezing fraction of melted ice.\n",
    "    *temp_snow* : float\n",
    "        Temperature at which all precipitation falls as snow.\n",
    "    *temp_rain* : float\n",
    "        Temperature at which all precipitation falls as rain.\n",
    "    *interpolate_rule* : [ 'linear' | 'nearest' | 'zero' |\n",
    "                           'slinear' | 'quadratic' | 'cubic' ]\n",
    "        Interpolation rule passed to `scipy.interpolate.interp1d`.\n",
    "    *interpolate_n*: int\n",
    "        Number of points used in interpolations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pdd_factor_snow=3,\n",
    "        pdd_factor_ice=8,\n",
    "        refreeze_snow=0.0,\n",
    "        refreeze_ice=0.0,\n",
    "        temp_snow=0.0,\n",
    "        temp_rain=2.0,\n",
    "        interpolate_rule=\"linear\",\n",
    "        interpolate_n=52,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # set pdd model parameters\n",
    "        self.pdd_factor_snow = pdd_factor_snow\n",
    "        self.pdd_factor_ice = pdd_factor_ice\n",
    "        self.refreeze_snow = refreeze_snow\n",
    "        self.refreeze_ice = refreeze_ice\n",
    "        self.temp_snow = temp_snow\n",
    "        self.temp_rain = temp_rain\n",
    "        self.interpolate_rule = interpolate_rule\n",
    "        self.interpolate_n = interpolate_n\n",
    "\n",
    "    def forward(self, temp, prec, stdv=0.0):\n",
    "        \"\"\"Run the positive degree day model.\n",
    "\n",
    "        Use temperature, precipitation, and standard deviation of temperature\n",
    "        to compute the number of positive degree days, accumulation and melt\n",
    "        surface mass fluxes, and the resulting surface mass balance.\n",
    "\n",
    "        *temp*: array_like\n",
    "            Input near-surface air temperature in degrees Celcius.\n",
    "        *prec*: array_like\n",
    "            Input precipitation rate in meter per year.\n",
    "        *stdv*: array_like (default 0.0)\n",
    "            Input standard deviation of near-surface air temperature in Kelvin.\n",
    "\n",
    "        By default, inputs are N-dimensional arrays whose first dimension is\n",
    "        interpreted as time and as periodic. Arrays of dimensions\n",
    "        N-1 are interpreted as constant in time and expanded to N dimensions.\n",
    "        Arrays of dimension 0 and numbers are interpreted as constant in time\n",
    "        and space and will be expanded too. The largest input array determines\n",
    "        the number of dimensions N.\n",
    "\n",
    "        Return the number of positive degree days ('pdd'), surface mass balance\n",
    "        ('smb'), and many other output variables in a dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        # ensure numpy arrays\n",
    "        temp = torch.asarray(temp)\n",
    "        prec = torch.asarray(prec)\n",
    "        stdv = torch.asarray(stdv)\n",
    "\n",
    "        # expand arrays to the largest shape\n",
    "        maxshape = max(temp.shape, prec.shape, stdv.shape)\n",
    "        temp = self._expand(temp, maxshape)\n",
    "        prec = self._expand(prec, maxshape)\n",
    "        stdv = self._expand(stdv, maxshape)\n",
    "\n",
    "        # interpolate time-series\n",
    "        if self.interpolate_n >= 1:\n",
    "            temp = self._interpolate(temp)\n",
    "            prec = self._interpolate(prec)\n",
    "            stdv = self._interpolate(stdv)\n",
    "\n",
    "        # compute accumulation and pdd\n",
    "        accu_rate = self.accu_rate(temp, prec)\n",
    "        inst_pdd = self.inst_pdd(temp, stdv)\n",
    "\n",
    "        # initialize snow depth, melt and refreeze rates\n",
    "        snow_depth = torch.zeros_like(temp)\n",
    "        snow_melt_rate = torch.zeros_like(temp)\n",
    "        ice_melt_rate = torch.zeros_like(temp)\n",
    "        snow_refreeze_rate = torch.zeros_like(temp)\n",
    "        ice_refreeze_rate = torch.zeros_like(temp)\n",
    "\n",
    "        snow_depth[:-1] = torch.clone(snow_depth[1:])\n",
    "        snow_depth = snow_depth + accu_rate\n",
    "        snow_melt_rate, ice_melt_rate = self.melt_rates(snow_depth, inst_pdd)\n",
    "        snow_depth = snow_depth - snow_melt_rate\n",
    "\n",
    "        melt_rate = snow_melt_rate + ice_melt_rate\n",
    "        snow_refreeze_rate = self.refreeze_snow * snow_melt_rate\n",
    "        ice_refreeze_rate = self.refreeze_ice * ice_melt_rate\n",
    "        refreeze_rate = snow_refreeze_rate + ice_refreeze_rate\n",
    "        runoff_rate = melt_rate - refreeze_rate\n",
    "        inst_smb = accu_rate - runoff_rate\n",
    "\n",
    "        # output\n",
    "        return {\n",
    "            \"temp\": temp,\n",
    "            \"prec\": prec,\n",
    "            \"stdv\": stdv,\n",
    "            \"inst_pdd\": inst_pdd,\n",
    "            \"accu_rate\": accu_rate,\n",
    "            \"snow_melt_rate\": snow_melt_rate,\n",
    "            \"ice_melt_rate\": ice_melt_rate,\n",
    "            \"melt_rate\": melt_rate,\n",
    "            \"snow_refreeze_rate\": snow_refreeze_rate,\n",
    "            \"ice_refreeze_rate\": ice_refreeze_rate,\n",
    "            \"refreeze_rate\": refreeze_rate,\n",
    "            \"runoff_rate\": runoff_rate,\n",
    "            \"inst_smb\": inst_smb,\n",
    "            \"snow_depth\": snow_depth,\n",
    "            \"pdd\": self._integrate(inst_pdd),\n",
    "            \"accu\": self._integrate(accu_rate),\n",
    "            \"snow_melt\": self._integrate(snow_melt_rate),\n",
    "            \"ice_melt\": self._integrate(ice_melt_rate),\n",
    "            \"melt\": self._integrate(melt_rate),\n",
    "            \"runoff\": self._integrate(runoff_rate),\n",
    "            \"refreeze\": self._integrate(refreeze_rate),\n",
    "            \"smb\": self._integrate(inst_smb),\n",
    "        }\n",
    "\n",
    "    def _expand(self, array, shape):\n",
    "        \"\"\"Expand an array to the given shape\"\"\"\n",
    "        if array.shape == shape:\n",
    "            res = array\n",
    "        elif array.shape == (1, shape[1], shape[2]):\n",
    "            res = np.asarray([array[0]] * shape[0])\n",
    "        elif array.shape == shape[1:]:\n",
    "            res = np.asarray([array] * shape[0])\n",
    "        elif array.shape == ():\n",
    "            res = array * torch.ones(shape)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"could not expand array of shape %s to %s\" % (array.shape, shape)\n",
    "            )\n",
    "        return res\n",
    "\n",
    "    def _integrate(self, array):\n",
    "        \"\"\"Integrate an array over one year\"\"\"\n",
    "        return torch.sum(array, axis=0) / (self.interpolate_n - 1)\n",
    "\n",
    "    def _interpolate(self, array):\n",
    "        \"\"\"Interpolate an array through one year.\"\"\"\n",
    "\n",
    "        from scipy.interpolate import interp1d\n",
    "\n",
    "        rule = self.interpolate_rule\n",
    "        npts = self.interpolate_n\n",
    "        oldx = (torch.arange(len(array) + 2) - 0.5) / len(array)\n",
    "        oldy = torch.vstack((array[-1], array, array[0]))\n",
    "        newx = (torch.arange(npts) + 0.5) / npts  # use 0.0 for PISM-like behaviour\n",
    "        newy = interp1d(oldx, oldy, kind=rule, axis=0)(newx)\n",
    "\n",
    "        return torch.from_numpy(newy)\n",
    "\n",
    "    def inst_pdd(self, temp, stdv):\n",
    "        \"\"\"Compute instantaneous positive degree days from temperature.\n",
    "\n",
    "        Use near-surface air temperature and standard deviation to compute\n",
    "        instantaneous positive degree days (effective temperature for melt,\n",
    "        unit degrees C) using an integral formulation (Calov and Greve, 2005).\n",
    "\n",
    "        *temp*: array_like\n",
    "            Near-surface air temperature in degrees Celcius.\n",
    "        *stdv*: array_like\n",
    "            Standard deviation of near-surface air temperature in Kelvin.\n",
    "        \"\"\"\n",
    "\n",
    "        # compute positive part of temperature everywhere\n",
    "        positivepart = torch.greater(temp, 0) * temp\n",
    "\n",
    "        # compute Calov and Greve (2005) integrand, ignoring division by zero\n",
    "        normtemp = temp / (torch.sqrt(torch.tensor(2)) * stdv)\n",
    "        calovgreve = stdv / torch.sqrt(torch.tensor(2) * torch.pi) * torch.exp(\n",
    "            -(normtemp**2)\n",
    "        ) + temp / 2 * torch.erfc(-normtemp)\n",
    "\n",
    "        # use positive part where sigma is zero and Calov and Greve elsewhere\n",
    "        teff = torch.where(stdv == 0.0, positivepart, calovgreve)\n",
    "\n",
    "        # convert to degree-days\n",
    "        return teff * 365.242198781\n",
    "\n",
    "    def accu_rate(self, temp, prec):\n",
    "        \"\"\"Compute accumulation rate from temperature and precipitation.\n",
    "\n",
    "        The fraction of precipitation that falls as snow decreases linearly\n",
    "        from one to zero between temperature thresholds defined by the\n",
    "        `temp_snow` and `temp_rain` attributes.\n",
    "\n",
    "        *temp*: array_like\n",
    "            Near-surface air temperature in degrees Celcius.\n",
    "        *prec*: array_like\n",
    "            Precipitation rate in meter per year.\n",
    "        \"\"\"\n",
    "\n",
    "        # compute snow fraction as a function of temperature\n",
    "        reduced_temp = (self.temp_rain - temp) / (self.temp_rain - self.temp_snow)\n",
    "        snowfrac = torch.clip(reduced_temp, 0, 1)\n",
    "\n",
    "        # return accumulation rate\n",
    "        return snowfrac * prec\n",
    "\n",
    "    def melt_rates(self, snow, pdd):\n",
    "        \"\"\"Compute melt rates from snow precipitation and pdd sum.\n",
    "\n",
    "        Snow melt is computed from the number of positive degree days (*pdd*)\n",
    "        and the `pdd_factor_snow` model attribute. If all snow is melted and\n",
    "        some energy (PDD) remains, ice melt is computed using `pdd_factor_ice`.\n",
    "\n",
    "        *snow*: array_like\n",
    "            Snow precipitation rate.\n",
    "        *pdd*: array_like\n",
    "            Number of positive degree days.\n",
    "        \"\"\"\n",
    "\n",
    "        # parse model parameters for readability\n",
    "        ddf_snow = self.pdd_factor_snow / 1e3\n",
    "        ddf_ice = self.pdd_factor_ice / 1e3\n",
    "\n",
    "        # compute a potential snow melt\n",
    "        pot_snow_melt = ddf_snow * pdd\n",
    "\n",
    "        # effective snow melt can't exceed amount of snow\n",
    "        snow_melt = torch.minimum(snow, pot_snow_melt)\n",
    "\n",
    "        # ice melt is proportional to excess snow melt\n",
    "        ice_melt = (pot_snow_melt - snow_melt) * ddf_ice / ddf_snow\n",
    "\n",
    "        # return melt rates\n",
    "        return (snow_melt, ice_melt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "e0859e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.sample import saltelli\n",
    "from pyDOE import lhs\n",
    "from scipy.stats.distributions import truncnorm, gamma, uniform, randint\n",
    "method = \"lhs\"\n",
    "n_prior_samples = 100\n",
    "np.random.seed(2)\n",
    "\n",
    "distributions = {\n",
    "    \"f_snow\": uniform(\n",
    "        loc=2.0, scale=4.0\n",
    "    ), \n",
    "    \"f_ice\": uniform(\n",
    "        loc=3.0, scale=9\n",
    "    ),  # uniform between 3 and 3.5  AS16 best value: 3.25\n",
    "    \"refreeze\": uniform(loc=0, scale=1.0),  # uniform between 0.25 and 0.95\n",
    "}\n",
    "# Names of all the variables\n",
    "keys = [x for x in distributions.keys()]\n",
    "\n",
    "# Describe the Problem\n",
    "problem = {\"num_vars\": len(keys), \"names\": keys, \"bounds\": [[0, 1]] * len(keys)}\n",
    "\n",
    "# Generate uniform samples (i.e. one unit hypercube)\n",
    "if method == \"saltelli\":\n",
    "    unif_sample = saltelli.sample(problem, n_prior_samples, calc_second_order=False)\n",
    "elif method == \"lhs\":\n",
    "    unif_sample = lhs(len(keys), n_prior_samples)\n",
    "else:\n",
    "    print(f\"Method {method} not available\")\n",
    "\n",
    "# To hold the transformed variables\n",
    "dist_sample = np.zeros_like(unif_sample)\n",
    "\n",
    "# Now transform the unit hypercube to the prescribed distributions\n",
    "# For each variable, transform with the inverse of the CDF (inv(CDF)=ppf)\n",
    "for i, key in enumerate(keys):\n",
    "    dist_sample[:, i] = distributions[key].ppf(unif_sample[:, i])\n",
    "\n",
    "# Save to CSV file using Pandas DataFrame and to_csv method\n",
    "header = keys\n",
    "# Convert to Pandas dataframe, append column headers, output as csv\n",
    "df = pd.DataFrame(data=dist_sample, columns=header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d52677e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000000, 39]) torch.Size([1000000, 1])\n"
     ]
    }
   ],
   "source": [
    "    n = 100\n",
    "    m = 12\n",
    "\n",
    "    lx = ly = 750000\n",
    "    x = np.linspace(-lx, lx, n)\n",
    "    y = np.linspace(-ly, ly, n)\n",
    "    t = (np.arange(12) + 0.5) / 12\n",
    "\n",
    "    # assign temperature and precipitation values\n",
    "    (yy, xx) = np.meshgrid(y, x)\n",
    "    temp = np.zeros((m, n, n), dtype=float)\n",
    "    prec = np.zeros((m, n, n), dtype=float)\n",
    "    stdv = np.zeros((m, n, n), dtype=float)\n",
    "    for i in range(len(t)):\n",
    "        temp[i] = -10 * yy / ly - 5 * np.cos(i * 2 * np.pi / 12)\n",
    "        prec[i] = xx / lx * (np.sign(xx) - np.cos(i * 2 * np.pi / 12))\n",
    "        stdv[i] = (2 + xx / lx - yy / ly) * (1 - np.cos(i * 2 * np.pi / 12))\n",
    "\n",
    "    T_obs = temp.reshape(m, -1)\n",
    "    P_obs = prec.reshape(m, -1)\n",
    "    std_dev = stdv.reshape(m, -1)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for k, row in df.iterrows():   \n",
    "        pdd = TorchPDDModel(\n",
    "            pdd_factor_snow=row[\"f_snow\"],\n",
    "            pdd_factor_ice=row[\"f_ice\"],\n",
    "            refreeze_snow=row[\"refreeze\"],\n",
    "            refreeze_ice=row[\"refreeze\"],\n",
    "        )\n",
    "        result = pdd(T_obs, P_obs, std_dev)\n",
    "\n",
    "        M_obs = result[\"melt\"]\n",
    "        Y.append(M_obs)\n",
    "        X.append(torch.from_numpy(np.hstack((T_obs.T, P_obs.T, std_dev.T, np.tile(row.values, (n**2, 1))))))\n",
    "\n",
    "    X = torch.vstack(X).type(torch.FloatTensor)\n",
    "    Y = torch.hstack(Y).type(torch.FloatTensor).reshape(-1, 1)\n",
    "    n_samples, n_parameters = X.shape\n",
    "    print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "e2a45752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 10000)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "237942c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "    import os\n",
    "    from scipy.stats import dirichlet\n",
    "\n",
    "    model_index = 0\n",
    "    torch.manual_seed(0)\n",
    "    pl.seed_everything(0)\n",
    "    np.random.seed(model_index)\n",
    "    emulator_dir = \"pddemulator\"\n",
    "\n",
    "    if not os.path.isdir(emulator_dir):\n",
    "        os.makedirs(emulator_dir)\n",
    "        os.makedirs(os.path.join(emulator_dir, \"emulator\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "05a2f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDDDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X,\n",
    "        Y,\n",
    "        omegas,\n",
    "        omegas_0,\n",
    "        batch_size: int = 128,\n",
    "        train_size: float = 0.9,\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.omegas = omegas\n",
    "        self.omegas_0 = omegas_0\n",
    "        self.batch_size = batch_size\n",
    "        self.train_size = train_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "\n",
    "        all_data = TensorDataset(self.X, self.Y, self.omegas, self.omegas_0)\n",
    "        self.all_data = all_data\n",
    "\n",
    "        training_data, val_data = train_test_split(\n",
    "            all_data, train_size=self.train_size, random_state=0\n",
    "        )\n",
    "        self.training_data = training_data\n",
    "        self.test_data = training_data\n",
    "\n",
    "        self.val_data = val_data\n",
    "        train_all_loader = DataLoader(\n",
    "            dataset=all_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.train_all_loader = train_all_loader\n",
    "        val_all_loader = DataLoader(\n",
    "            dataset=all_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.val_all_loader = val_all_loader\n",
    "        train_loader = DataLoader(\n",
    "            dataset=training_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = train_loader\n",
    "        val_loader = DataLoader(\n",
    "            dataset=val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def prepare_data(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self.val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b35190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | l_first       | Linear        | 5.1 K \n",
      "1 | norm_first    | LayerNorm     | 256   \n",
      "2 | dropout_first | Dropout       | 0     \n",
      "3 | dnn           | ModuleList    | 50.3 K\n",
      "4 | l_last        | Linear        | 129   \n",
      "5 | train_ae      | AbsoluteError | 0     \n",
      "6 | test_ae       | AbsoluteError | 0     \n",
      "------------------------------------------------\n",
      "55.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.8 K    Total params\n",
      "0.223     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97ef2b0b43843ffbd35ddbb1ea232bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 9.9750e-03.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 9.9501e-03.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 9.9252e-03.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 9.9004e-03.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 9.8756e-03.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    print(f\"Training model {model_index}\")\n",
    "    omegas = torch.Tensor(dirichlet.rvs(np.ones(n_samples))).T\n",
    "    omegas = omegas.type(torch.FloatTensor)\n",
    "    omegas_0 = torch.ones_like(omegas) / len(omegas)\n",
    "    area = torch.ones_like(omegas)\n",
    "    train_size = 1.0\n",
    "    num_workers = 1\n",
    "    hparams = {\"n_layers\": 5, \"n_hidden\": 128, \"batch_size\": 128, \"learning_rate\": 0.01}\n",
    "    \n",
    "    if train_size == 1.0:\n",
    "        data_loader = PDDDataModule(X, Y, omegas, omegas_0, num_workers=num_workers)\n",
    "    else:\n",
    "        data_loader = PDDDataModule(\n",
    "            X, Y, omegas, omegas_0, train_size=train_size, num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    data_loader.setup()\n",
    "    e = PDDEmulator(\n",
    "        n_parameters,\n",
    "        hparams,\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        deterministic=True,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    if train_size == 1.0:\n",
    "        train_loader = data_loader.train_all_loader\n",
    "        val_loader = data_loader.val_all_loader\n",
    "    else:\n",
    "        train_loader = data_loader.train_loader\n",
    "        val_loader = data_loader.val_loader\n",
    "\n",
    "    trainer.fit(e, train_loader, val_loader)\n",
    "    torch.save(e.state_dict(), f\"{emulator_dir}/emulator/emulator_{model_index}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "0f4d63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) 2021 Andy Aschwanden, Douglas C Brinkerhoff\n",
    "#\n",
    "# This file is part of pism-emulator.\n",
    "#\n",
    "# PISM-EMULATOR is free software; you can redistribute it and/or modify it under the\n",
    "# terms of the GNU General Public License as published by the Free Software\n",
    "# Foundation; either version 3 of the License, or (at your option) any later\n",
    "# version.\n",
    "#\n",
    "# PISM-EMULATOR is distributed in the hope that it will be useful, but WITHOUT ANY\n",
    "# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n",
    "# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n",
    "# details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with PISM; if not, write to the Free Software\n",
    "# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torchmetrics.utilities.checks import _check_same_shape\n",
    "from torchmetrics import Metric\n",
    "\n",
    "\n",
    "def _absolute_error_update(\n",
    "    preds: Tensor, target: Tensor, omegas: Tensor) -> Tensor:\n",
    "    _check_same_shape(preds, target)\n",
    "    diff = torch.abs(preds - target)\n",
    "    sum_abs_error = torch.sum(diff * diff, axis=1)\n",
    "    absolute_error = torch.sum(sum_abs_error * omegas.squeeze())\n",
    "    return absolute_error\n",
    "\n",
    "\n",
    "def _absolute_error_compute(absolute_error) -> Tensor:\n",
    "    return absolute_error\n",
    "\n",
    "\n",
    "def absolute_error(\n",
    "    preds: Tensor, target: Tensor, omegas: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes squared absolute error\n",
    "    Args:\n",
    "        preds: estimated labels\n",
    "        target: ground truth labels\n",
    "        omegas: weights\n",
    "        area: area of each cell\n",
    "    Return:\n",
    "        Tensor with absolute error\n",
    "    Example:\n",
    "        >>> x = torch.tensor([[0, 1, 2, 3], [1, 2, 3, 4]]).T\n",
    "        >>> y = torch.tensor([[0, 1, 2, 1], [2, 3, 4, 4]]).T\n",
    "        >>> o = torch.tensor([0.25, 0.25, 0.3, 0.2])\n",
    "        >>> a = torch.tensor([0.25, 0.25])\n",
    "        >>> absolute_error(x, y, o, a)\n",
    "        tensor(0.4000)\n",
    "    \"\"\"\n",
    "    sum_abs_error = _absolute_error_update(preds, target, omegas)\n",
    "    return _absolute_error_compute(sum_abs_error)\n",
    "\n",
    "\n",
    "class AbsoluteError(Metric):\n",
    "    def __init__(self, compute_on_step: bool = True, dist_sync_on_step=False):\n",
    "        # call `self.add_state`for every internal state that is needed for the metrics computations\n",
    "        # dist_reduce_fx indicates the function that should be used to reduce\n",
    "        # state from multiple processes\n",
    "        super().__init__(\n",
    "            compute_on_step=compute_on_step, dist_sync_on_step=dist_sync_on_step\n",
    "        )\n",
    "\n",
    "        self.add_state(\"sum_abs_error\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: Tensor, target: Tensor, omegas: Tensor):\n",
    "        \"\"\"\n",
    "        Update state with predictions and targets, and area.\n",
    "        Args:\n",
    "            preds: Predictions from model\n",
    "            target: Ground truth values\n",
    "            omegas: Weights\n",
    "            area: Area of each cell\n",
    "        \"\"\"\n",
    "        sum_abs_error = _absolute_error_update(preds, target, omegas)\n",
    "        self.sum_abs_error += sum_abs_error\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Computes absolute error over state.\n",
    "        \"\"\"\n",
    "        return _absolute_error_compute(self.sum_abs_error)\n",
    "\n",
    "    @property\n",
    "    def is_differentiable(self):\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "24e61c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a069a21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39, 1])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omegas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0ea2f066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7f765bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "57e292ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.type_as?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fed74959",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84257391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
