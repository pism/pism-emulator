{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6b2f8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torchmetrics.utilities.checks import _check_same_shape\n",
    "from torchmetrics import Metric\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from pismemulator.metrics import AbsoluteError, absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "785590f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDDEmulator(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_parameters: int,\n",
    "        hparams,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        n_layers = self.hparams.n_layers\n",
    "        n_hidden = self.hparams.n_hidden\n",
    "\n",
    "        if isinstance(n_hidden, int):\n",
    "            n_hidden = [n_hidden] * (n_layers - 1)\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.l_first = nn.Linear(n_parameters, n_hidden[0])\n",
    "        self.norm_first = nn.LayerNorm(n_hidden[0])\n",
    "        self.dropout_first = nn.Dropout(p=0.0)\n",
    "\n",
    "        models = []\n",
    "        for n in range(n_layers - 2):\n",
    "            models.append(\n",
    "                nn.Sequential(\n",
    "                    OrderedDict(\n",
    "                        [\n",
    "                            (\"Linear\", nn.Linear(n_hidden[n], n_hidden[n + 1])),\n",
    "                            (\"LayerNorm\", nn.LayerNorm(n_hidden[n + 1])),\n",
    "                            (\"Dropout\", nn.Dropout(p=0.1)),\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        self.dnn = nn.ModuleList(models)\n",
    "        self.l_last = nn.Linear(n_hidden[-1], 1)\n",
    "\n",
    "        self.train_ae = AbsoluteError()\n",
    "        self.test_ae = AbsoluteError()\n",
    "\n",
    "    def forward(self, x, add_mean=False):\n",
    "        # Pass the input tensor through each of our operations\n",
    "\n",
    "        a = self.l_first(x)\n",
    "        a = self.norm_first(a)\n",
    "        a = self.dropout_first(a)\n",
    "        z = torch.relu(a)\n",
    "\n",
    "        for dnn in self.dnn:\n",
    "            a = dnn(z)\n",
    "            z = torch.relu(a) + z\n",
    "\n",
    "        return self.l_last(z)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"NNEmulator\")\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "        parser.add_argument(\"--n_hidden\", default=128)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=0.01)\n",
    "\n",
    "        return parent_parser\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), self.hparams.learning_rate, weight_decay=0.0\n",
    "        )\n",
    "        # This is an approximation to Doug's version:\n",
    "        scheduler = {\n",
    "            \"scheduler\": ExponentialLR(optimizer, 0.9975, verbose=True),\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, f, o, _ = batch\n",
    "        f_pred = self.forward(x)\n",
    "        loss = absolute_error(f_pred, f, o)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, f, o, o_0 = batch\n",
    "        f_pred = self.forward(x)\n",
    "\n",
    "        self.log(\"train_loss\", self.train_ae(f_pred, f, o))\n",
    "        self.log(\"test_loss\", self.test_ae(f_pred, f, o_0))\n",
    "\n",
    "        return {\"x\": x, \"f\": f, \"f_pred\": f_pred, \"o\": o, \"o_0\": o_0}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            self.train_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_loss\",\n",
    "            self.test_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "57e3213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchPDDModel(torch.nn.modules.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    # Copyright (c) 2013--2018, Julien Seguinot <seguinot@vaw.baug.ethz.ch>\n",
    "    # GNU General Public License v3.0+ (https://www.gnu.org/licenses/gpl-3.0.txt)\n",
    "\n",
    "    A positive degree day model for glacier surface mass balance\n",
    "\n",
    "    Return a callable Positive Degree Day (PDD) model instance.\n",
    "\n",
    "    Model parameters are held as public attributes, and can be set using\n",
    "    corresponding keyword arguments at initialization time:\n",
    "\n",
    "    *pdd_factor_snow* : float\n",
    "        Positive degree-day factor for snow.\n",
    "    *pdd_factor_ice* : float\n",
    "        Positive degree-day factor for ice.\n",
    "    *refreeze_snow* : float\n",
    "        Refreezing fraction of melted snow.\n",
    "    *refreeze_ice* : float\n",
    "        Refreezing fraction of melted ice.\n",
    "    *temp_snow* : float\n",
    "        Temperature at which all precipitation falls as snow.\n",
    "    *temp_rain* : float\n",
    "        Temperature at which all precipitation falls as rain.\n",
    "    *interpolate_rule* : [ 'linear' | 'nearest' | 'zero' |\n",
    "                           'slinear' | 'quadratic' | 'cubic' ]\n",
    "        Interpolation rule passed to `scipy.interpolate.interp1d`.\n",
    "    *interpolate_n*: int\n",
    "        Number of points used in interpolations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pdd_factor_snow=3,\n",
    "        pdd_factor_ice=8,\n",
    "        refreeze_snow=0.0,\n",
    "        refreeze_ice=0.0,\n",
    "        temp_snow=0.0,\n",
    "        temp_rain=2.0,\n",
    "        interpolate_rule=\"linear\",\n",
    "        interpolate_n=52,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # set pdd model parameters\n",
    "        self.pdd_factor_snow = pdd_factor_snow\n",
    "        self.pdd_factor_ice = pdd_factor_ice\n",
    "        self.refreeze_snow = refreeze_snow\n",
    "        self.refreeze_ice = refreeze_ice\n",
    "        self.temp_snow = temp_snow\n",
    "        self.temp_rain = temp_rain\n",
    "        self.interpolate_rule = interpolate_rule\n",
    "        self.interpolate_n = interpolate_n\n",
    "\n",
    "    def forward(self, temp, prec, stdv=0.0):\n",
    "        \"\"\"Run the positive degree day model.\n",
    "\n",
    "        Use temperature, precipitation, and standard deviation of temperature\n",
    "        to compute the number of positive degree days, accumulation and melt\n",
    "        surface mass fluxes, and the resulting surface mass balance.\n",
    "\n",
    "        *temp*: array_like\n",
    "            Input near-surface air temperature in degrees Celcius.\n",
    "        *prec*: array_like\n",
    "            Input precipitation rate in meter per year.\n",
    "        *stdv*: array_like (default 0.0)\n",
    "            Input standard deviation of near-surface air temperature in Kelvin.\n",
    "\n",
    "        By default, inputs are N-dimensional arrays whose first dimension is\n",
    "        interpreted as time and as periodic. Arrays of dimensions\n",
    "        N-1 are interpreted as constant in time and expanded to N dimensions.\n",
    "        Arrays of dimension 0 and numbers are interpreted as constant in time\n",
    "        and space and will be expanded too. The largest input array determines\n",
    "        the number of dimensions N.\n",
    "\n",
    "        Return the number of positive degree days ('pdd'), surface mass balance\n",
    "        ('smb'), and many other output variables in a dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        # ensure numpy arrays\n",
    "        temp = torch.asarray(temp)\n",
    "        prec = torch.asarray(prec)\n",
    "        stdv = torch.asarray(stdv)\n",
    "\n",
    "        # expand arrays to the largest shape\n",
    "        maxshape = max(temp.shape, prec.shape, stdv.shape)\n",
    "        temp = self._expand(temp, maxshape)\n",
    "        prec = self._expand(prec, maxshape)\n",
    "        stdv = self._expand(stdv, maxshape)\n",
    "\n",
    "        # interpolate time-series\n",
    "        if self.interpolate_n >= 1:\n",
    "            temp = self._interpolate(temp)\n",
    "            prec = self._interpolate(prec)\n",
    "            stdv = self._interpolate(stdv)\n",
    "\n",
    "        # compute accumulation and pdd\n",
    "        accu_rate = self.accu_rate(temp, prec)\n",
    "        inst_pdd = self.inst_pdd(temp, stdv)\n",
    "\n",
    "        # initialize snow depth, melt and refreeze rates\n",
    "        snow_depth = torch.zeros_like(temp)\n",
    "        snow_melt_rate = torch.zeros_like(temp)\n",
    "        ice_melt_rate = torch.zeros_like(temp)\n",
    "        snow_refreeze_rate = torch.zeros_like(temp)\n",
    "        ice_refreeze_rate = torch.zeros_like(temp)\n",
    "\n",
    "        snow_depth[:-1] = torch.clone(snow_depth[1:])\n",
    "        snow_depth = snow_depth + accu_rate\n",
    "        snow_melt_rate, ice_melt_rate = self.melt_rates(snow_depth, inst_pdd)\n",
    "        snow_depth = snow_depth - snow_melt_rate\n",
    "\n",
    "        melt_rate = snow_melt_rate + ice_melt_rate\n",
    "        snow_refreeze_rate = self.refreeze_snow * snow_melt_rate\n",
    "        ice_refreeze_rate = self.refreeze_ice * ice_melt_rate\n",
    "        refreeze_rate = snow_refreeze_rate + ice_refreeze_rate\n",
    "        runoff_rate = melt_rate - refreeze_rate\n",
    "        inst_smb = accu_rate - runoff_rate\n",
    "\n",
    "        # output\n",
    "        return {\n",
    "            \"temp\": temp,\n",
    "            \"prec\": prec,\n",
    "            \"stdv\": stdv,\n",
    "            \"inst_pdd\": inst_pdd,\n",
    "            \"accu_rate\": accu_rate,\n",
    "            \"snow_melt_rate\": snow_melt_rate,\n",
    "            \"ice_melt_rate\": ice_melt_rate,\n",
    "            \"melt_rate\": melt_rate,\n",
    "            \"snow_refreeze_rate\": snow_refreeze_rate,\n",
    "            \"ice_refreeze_rate\": ice_refreeze_rate,\n",
    "            \"refreeze_rate\": refreeze_rate,\n",
    "            \"runoff_rate\": runoff_rate,\n",
    "            \"inst_smb\": inst_smb,\n",
    "            \"snow_depth\": snow_depth,\n",
    "            \"pdd\": self._integrate(inst_pdd),\n",
    "            \"accu\": self._integrate(accu_rate),\n",
    "            \"snow_melt\": self._integrate(snow_melt_rate),\n",
    "            \"ice_melt\": self._integrate(ice_melt_rate),\n",
    "            \"melt\": self._integrate(melt_rate),\n",
    "            \"runoff\": self._integrate(runoff_rate),\n",
    "            \"refreeze\": self._integrate(refreeze_rate),\n",
    "            \"smb\": self._integrate(inst_smb),\n",
    "        }\n",
    "\n",
    "    def _expand(self, array, shape):\n",
    "        \"\"\"Expand an array to the given shape\"\"\"\n",
    "        if array.shape == shape:\n",
    "            res = array\n",
    "        elif array.shape == (1, shape[1], shape[2]):\n",
    "            res = np.asarray([array[0]] * shape[0])\n",
    "        elif array.shape == shape[1:]:\n",
    "            res = np.asarray([array] * shape[0])\n",
    "        elif array.shape == ():\n",
    "            res = array * torch.ones(shape)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"could not expand array of shape %s to %s\" % (array.shape, shape)\n",
    "            )\n",
    "        return res\n",
    "\n",
    "    def _integrate(self, array):\n",
    "        \"\"\"Integrate an array over one year\"\"\"\n",
    "        return torch.sum(array, axis=0) / (self.interpolate_n - 1)\n",
    "\n",
    "    def _interpolate(self, array):\n",
    "        \"\"\"Interpolate an array through one year.\"\"\"\n",
    "\n",
    "        from scipy.interpolate import interp1d\n",
    "\n",
    "        rule = self.interpolate_rule\n",
    "        npts = self.interpolate_n\n",
    "        oldx = (torch.arange(len(array) + 2) - 0.5) / len(array)\n",
    "        oldy = torch.vstack((array[-1], array, array[0]))\n",
    "        newx = (torch.arange(npts) + 0.5) / npts  # use 0.0 for PISM-like behaviour\n",
    "        newy = interp1d(oldx, oldy, kind=rule, axis=0)(newx)\n",
    "\n",
    "        return torch.from_numpy(newy)\n",
    "\n",
    "    def inst_pdd(self, temp, stdv):\n",
    "        \"\"\"Compute instantaneous positive degree days from temperature.\n",
    "\n",
    "        Use near-surface air temperature and standard deviation to compute\n",
    "        instantaneous positive degree days (effective temperature for melt,\n",
    "        unit degrees C) using an integral formulation (Calov and Greve, 2005).\n",
    "\n",
    "        *temp*: array_like\n",
    "            Near-surface air temperature in degrees Celcius.\n",
    "        *stdv*: array_like\n",
    "            Standard deviation of near-surface air temperature in Kelvin.\n",
    "        \"\"\"\n",
    "\n",
    "        # compute positive part of temperature everywhere\n",
    "        positivepart = torch.greater(temp, 0) * temp\n",
    "\n",
    "        # compute Calov and Greve (2005) integrand, ignoring division by zero\n",
    "        normtemp = temp / (torch.sqrt(torch.tensor(2)) * stdv)\n",
    "        calovgreve = stdv / torch.sqrt(torch.tensor(2) * torch.pi) * torch.exp(\n",
    "            -(normtemp**2)\n",
    "        ) + temp / 2 * torch.erfc(-normtemp)\n",
    "\n",
    "        # use positive part where sigma is zero and Calov and Greve elsewhere\n",
    "        teff = torch.where(stdv == 0.0, positivepart, calovgreve)\n",
    "\n",
    "        # convert to degree-days\n",
    "        return teff * 365.242198781\n",
    "\n",
    "    def accu_rate(self, temp, prec):\n",
    "        \"\"\"Compute accumulation rate from temperature and precipitation.\n",
    "\n",
    "        The fraction of precipitation that falls as snow decreases linearly\n",
    "        from one to zero between temperature thresholds defined by the\n",
    "        `temp_snow` and `temp_rain` attributes.\n",
    "\n",
    "        *temp*: array_like\n",
    "            Near-surface air temperature in degrees Celcius.\n",
    "        *prec*: array_like\n",
    "            Precipitation rate in meter per year.\n",
    "        \"\"\"\n",
    "\n",
    "        # compute snow fraction as a function of temperature\n",
    "        reduced_temp = (self.temp_rain - temp) / (self.temp_rain - self.temp_snow)\n",
    "        snowfrac = torch.clip(reduced_temp, 0, 1)\n",
    "\n",
    "        # return accumulation rate\n",
    "        return snowfrac * prec\n",
    "\n",
    "    def melt_rates(self, snow, pdd):\n",
    "        \"\"\"Compute melt rates from snow precipitation and pdd sum.\n",
    "\n",
    "        Snow melt is computed from the number of positive degree days (*pdd*)\n",
    "        and the `pdd_factor_snow` model attribute. If all snow is melted and\n",
    "        some energy (PDD) remains, ice melt is computed using `pdd_factor_ice`.\n",
    "\n",
    "        *snow*: array_like\n",
    "            Snow precipitation rate.\n",
    "        *pdd*: array_like\n",
    "            Number of positive degree days.\n",
    "        \"\"\"\n",
    "\n",
    "        # parse model parameters for readability\n",
    "        ddf_snow = self.pdd_factor_snow / 1e3\n",
    "        ddf_ice = self.pdd_factor_ice / 1e3\n",
    "\n",
    "        # compute a potential snow melt\n",
    "        pot_snow_melt = ddf_snow * pdd\n",
    "\n",
    "        # effective snow melt can't exceed amount of snow\n",
    "        snow_melt = torch.minimum(snow, pot_snow_melt)\n",
    "\n",
    "        # ice melt is proportional to excess snow melt\n",
    "        ice_melt = (pot_snow_melt - snow_melt) * ddf_ice / ddf_snow\n",
    "\n",
    "        # return melt rates\n",
    "        return (snow_melt, ice_melt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "0b6f71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.sample import saltelli\n",
    "from pyDOE import lhs\n",
    "from scipy.stats.distributions import truncnorm, gamma, uniform, randint\n",
    "method = \"lhs\"\n",
    "n_prior_samples = 100\n",
    "np.random.seed(2)\n",
    "\n",
    "distributions = {\n",
    "    \"f_snow\": uniform(\n",
    "        loc=2.0, scale=4.0\n",
    "    ), \n",
    "    \"f_ice\": uniform(\n",
    "        loc=3.0, scale=9\n",
    "    ),  # uniform between 3 and 3.5  AS16 best value: 3.25\n",
    "    \"refreeze\": uniform(loc=0, scale=1.0),  # uniform between 0.25 and 0.95\n",
    "}\n",
    "# Names of all the variables\n",
    "keys = [x for x in distributions.keys()]\n",
    "\n",
    "# Describe the Problem\n",
    "problem = {\"num_vars\": len(keys), \"names\": keys, \"bounds\": [[0, 1]] * len(keys)}\n",
    "\n",
    "# Generate uniform samples (i.e. one unit hypercube)\n",
    "if method == \"saltelli\":\n",
    "    unif_sample = saltelli.sample(problem, n_prior_samples, calc_second_order=False)\n",
    "elif method == \"lhs\":\n",
    "    unif_sample = lhs(len(keys), n_prior_samples)\n",
    "else:\n",
    "    print(f\"Method {method} not available\")\n",
    "\n",
    "# To hold the transformed variables\n",
    "dist_sample = np.zeros_like(unif_sample)\n",
    "\n",
    "# Now transform the unit hypercube to the prescribed distributions\n",
    "# For each variable, transform with the inverse of the CDF (inv(CDF)=ppf)\n",
    "for i, key in enumerate(keys):\n",
    "    dist_sample[:, i] = distributions[key].ppf(unif_sample[:, i])\n",
    "\n",
    "# Save to CSV file using Pandas DataFrame and to_csv method\n",
    "header = keys\n",
    "# Convert to Pandas dataframe, append column headers, output as csv\n",
    "df = pd.DataFrame(data=dist_sample, columns=header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "8a598d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) 2021 Andy Aschwanden, Douglas C Brinkerhoff\n",
    "#\n",
    "# This file is part of pism-emulator.\n",
    "#\n",
    "# PISM-EMULATOR is free software; you can redistribute it and/or modify it under the\n",
    "# terms of the GNU General Public License as published by the Free Software\n",
    "# Foundation; either version 3 of the License, or (at your option) any later\n",
    "# version.\n",
    "#\n",
    "# PISM-EMULATOR is distributed in the hope that it will be useful, but WITHOUT ANY\n",
    "# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n",
    "# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n",
    "# details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with PISM; if not, write to the Free Software\n",
    "# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torchmetrics.utilities.checks import _check_same_shape\n",
    "from torchmetrics import Metric\n",
    "\n",
    "\n",
    "def _absolute_error_update(\n",
    "    preds: Tensor, target: Tensor, omegas: Tensor) -> Tensor:\n",
    "    _check_same_shape(preds, target)\n",
    "    diff = torch.abs(preds - target)\n",
    "    sum_abs_error = torch.sum(diff * diff, axis=1)\n",
    "    absolute_error = torch.sum(sum_abs_error * omegas.squeeze())\n",
    "    return absolute_error\n",
    "\n",
    "\n",
    "def _absolute_error_compute(absolute_error) -> Tensor:\n",
    "    return absolute_error\n",
    "\n",
    "\n",
    "def absolute_error(\n",
    "    preds: Tensor, target: Tensor, omegas: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes squared absolute error\n",
    "    Args:\n",
    "        preds: estimated labels\n",
    "        target: ground truth labels\n",
    "        omegas: weights\n",
    "        area: area of each cell\n",
    "    Return:\n",
    "        Tensor with absolute error\n",
    "    Example:\n",
    "        >>> x = torch.tensor([[0, 1, 2, 3], [1, 2, 3, 4]]).T\n",
    "        >>> y = torch.tensor([[0, 1, 2, 1], [2, 3, 4, 4]]).T\n",
    "        >>> o = torch.tensor([0.25, 0.25, 0.3, 0.2])\n",
    "        >>> a = torch.tensor([0.25, 0.25])\n",
    "        >>> absolute_error(x, y, o, a)\n",
    "        tensor(0.4000)\n",
    "    \"\"\"\n",
    "    sum_abs_error = _absolute_error_update(preds, target, omegas)\n",
    "    return _absolute_error_compute(sum_abs_error)\n",
    "\n",
    "\n",
    "class AbsoluteError(Metric):\n",
    "    def __init__(self, compute_on_step: bool = True, dist_sync_on_step=False):\n",
    "        # call `self.add_state`for every internal state that is needed for the metrics computations\n",
    "        # dist_reduce_fx indicates the function that should be used to reduce\n",
    "        # state from multiple processes\n",
    "        super().__init__(\n",
    "            compute_on_step=compute_on_step, dist_sync_on_step=dist_sync_on_step\n",
    "        )\n",
    "\n",
    "        self.add_state(\"sum_abs_error\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: Tensor, target: Tensor, omegas: Tensor):\n",
    "        \"\"\"\n",
    "        Update state with predictions and targets, and area.\n",
    "        Args:\n",
    "            preds: Predictions from model\n",
    "            target: Ground truth values\n",
    "            omegas: Weights\n",
    "            area: Area of each cell\n",
    "        \"\"\"\n",
    "        sum_abs_error = _absolute_error_update(preds, target, omegas)\n",
    "        self.sum_abs_error += sum_abs_error\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Computes absolute error over state.\n",
    "        \"\"\"\n",
    "        return _absolute_error_compute(self.sum_abs_error)\n",
    "\n",
    "    @property\n",
    "    def is_differentiable(self):\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "0e03c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDDDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X,\n",
    "        Y,\n",
    "        omegas,\n",
    "        omegas_0,\n",
    "        batch_size: int = 128,\n",
    "        train_size: float = 0.9,\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.omegas = omegas\n",
    "        self.omegas_0 = omegas_0\n",
    "        self.batch_size = batch_size\n",
    "        self.train_size = train_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "\n",
    "        all_data = TensorDataset(self.X, self.Y, self.omegas, self.omegas_0)\n",
    "        self.all_data = all_data\n",
    "\n",
    "        training_data, val_data = train_test_split(\n",
    "            all_data, train_size=self.train_size, random_state=0\n",
    "        )\n",
    "        self.training_data = training_data\n",
    "        self.test_data = training_data\n",
    "\n",
    "        self.val_data = val_data\n",
    "        train_all_loader = DataLoader(\n",
    "            dataset=all_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.train_all_loader = train_all_loader\n",
    "        val_all_loader = DataLoader(\n",
    "            dataset=all_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.val_all_loader = val_all_loader\n",
    "        train_loader = DataLoader(\n",
    "            dataset=training_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = train_loader\n",
    "        val_loader = DataLoader(\n",
    "            dataset=val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def prepare_data(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self.val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "630b1793",
   "metadata": {},
   "outputs": [],
   "source": [
    "    n = 25\n",
    "    m = 12\n",
    "\n",
    "    lx = ly = 750000\n",
    "    x = np.linspace(-lx, lx, n)\n",
    "    y = np.linspace(-ly, ly, n)\n",
    "    t = (np.arange(12) + 0.5) / 12\n",
    "\n",
    "    # assign temperature and precipitation values\n",
    "    (yy, xx) = np.meshgrid(y, x)\n",
    "    temp = np.zeros((m, n, n), dtype=float)\n",
    "    prec = np.zeros((m, n, n), dtype=float)\n",
    "    stdv = np.zeros((m, n, n), dtype=float)\n",
    "    for i in range(len(t)):\n",
    "        temp[i] = -10 * yy / ly - 5 * np.cos(i * 2 * np.pi / 12)\n",
    "        prec[i] = xx / lx * (np.sign(xx) - np.cos(i * 2 * np.pi / 12))\n",
    "        stdv[i] = (2 + xx / lx - yy / ly) * (1 - np.cos(i * 2 * np.pi / 12))\n",
    "\n",
    "    T_obs = temp.reshape(1, -1)\n",
    "    P_obs = prec.reshape(1, -1)\n",
    "    std_dev = stdv.reshape(1, -1)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for k, row in df.iterrows():   \n",
    "        pdd = TorchPDDModel(\n",
    "            pdd_factor_snow=row[\"f_snow\"],\n",
    "            pdd_factor_ice=row[\"f_ice\"],\n",
    "            refreeze_snow=row[\"refreeze\"],\n",
    "            refreeze_ice=row[\"refreeze\"],\n",
    "            n_interpolate=1,\n",
    "        )\n",
    "        result = pdd(T_obs, P_obs, std_dev)\n",
    "\n",
    "        M_obs = result[\"melt\"]\n",
    "        Y.append(M_obs)\n",
    "        x = torch.from_numpy(np.hstack((T_obs.T, P_obs.T, std_dev.T)))\n",
    "        X.append(x)\n",
    "\n",
    "    X = torch.vstack(X).type(torch.FloatTensor)\n",
    "    Y = torch.hstack(Y).type(torch.FloatTensor).reshape(-1, 1)\n",
    "    n_samples, n_parameters = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "1834f5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "    import os\n",
    "    from scipy.stats import dirichlet\n",
    "\n",
    "    model_index = 0\n",
    "    torch.manual_seed(0)\n",
    "    pl.seed_everything(0)\n",
    "    np.random.seed(model_index)\n",
    "    emulator_dir = \"pddemulator\"\n",
    "\n",
    "    if not os.path.isdir(emulator_dir):\n",
    "        os.makedirs(emulator_dir)\n",
    "        os.makedirs(os.path.join(emulator_dir, \"emulator\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7a86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | l_first       | Linear        | 512   \n",
      "1 | norm_first    | LayerNorm     | 256   \n",
      "2 | dropout_first | Dropout       | 0     \n",
      "3 | dnn           | ModuleList    | 50.3 K\n",
      "4 | l_last        | Linear        | 129   \n",
      "5 | train_ae      | AbsoluteError | 0     \n",
      "6 | test_ae       | AbsoluteError | 0     \n",
      "------------------------------------------------\n",
      "51.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "51.2 K    Total params\n",
      "0.205     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96daa21430b43b6977b4fba2b6accd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    print(f\"Training model {model_index}\")\n",
    "    omegas = torch.Tensor(dirichlet.rvs(np.ones(n_samples))).T\n",
    "    omegas = omegas.type(torch.FloatTensor)\n",
    "    omegas_0 = torch.ones_like(omegas) / len(omegas)\n",
    "    area = torch.ones_like(omegas)\n",
    "    train_size = 1.0\n",
    "    num_workers = 1\n",
    "    hparams = {\"n_layers\": 5, \"n_hidden\": 128, \"batch_size\": 128, \"learning_rate\": 0.01}\n",
    "    \n",
    "    if train_size == 1.0:\n",
    "        data_loader = PDDDataModule(X, Y, omegas, omegas_0, num_workers=num_workers)\n",
    "    else:\n",
    "        data_loader = PDDDataModule(\n",
    "            X, Y, omegas, omegas_0, train_size=train_size, num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    data_loader.setup()\n",
    "    e = PDDEmulator(\n",
    "        n_parameters,\n",
    "        hparams,\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        deterministic=True,\n",
    "        num_sanity_val_steps=0,\n",
    "        max_epochs=10,\n",
    "    )\n",
    "    if train_size == 1.0:\n",
    "        train_loader = data_loader.train_all_loader\n",
    "        val_loader = data_loader.val_all_loader\n",
    "    else:\n",
    "        train_loader = data_loader.train_loader\n",
    "        val_loader = data_loader.val_loader\n",
    "\n",
    "    trainer.fit(e, train_loader, val_loader)\n",
    "    torch.save(e.state_dict(), f\"{emulator_dir}/emulator/emulator_{model_index}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "05eb6c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "5306e52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30000])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "38f711a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1cc0d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d6f0dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.type_as?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0f135a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "e6732b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1000,loss=2.701\n",
      "epoch:2000,loss=2.107\n",
      "epoch:3000,loss=1.855\n",
      "epoch:4000,loss=1.684\n",
      "epoch:5000,loss=1.516\n",
      "epoch:6000,loss=1.345\n",
      "epoch:7000,loss=1.182\n",
      "epoch:8000,loss=1.036\n",
      "epoch:9000,loss=0.908\n",
      "epoch:10000,loss=0.797\n",
      "epoch:11000,loss=0.702\n",
      "epoch:12000,loss=0.621\n",
      "epoch:13000,loss=0.552\n",
      "epoch:14000,loss=0.493\n",
      "epoch:15000,loss=0.442\n",
      "epoch:16000,loss=0.399\n",
      "epoch:17000,loss=0.362\n",
      "epoch:18000,loss=0.330\n",
      "epoch:19000,loss=0.303\n",
      "epoch:20000,loss=0.279\n",
      "epoch:21000,loss=0.257\n",
      "epoch:22000,loss=0.238\n",
      "epoch:23000,loss=0.221\n",
      "epoch:24000,loss=0.205\n",
      "epoch:25000,loss=0.192\n",
      "epoch:26000,loss=0.180\n",
      "epoch:27000,loss=0.168\n",
      "epoch:28000,loss=0.158\n",
      "epoch:29000,loss=0.148\n",
      "epoch:30000,loss=0.139\n",
      "epoch:31000,loss=0.130\n",
      "epoch:32000,loss=0.122\n",
      "epoch:33000,loss=0.115\n",
      "epoch:34000,loss=0.108\n",
      "epoch:35000,loss=0.102\n",
      "epoch:36000,loss=0.097\n",
      "epoch:37000,loss=0.092\n",
      "epoch:38000,loss=0.087\n",
      "epoch:39000,loss=0.083\n",
      "epoch:40000,loss=0.079\n",
      "epoch:41000,loss=0.075\n",
      "epoch:42000,loss=0.071\n",
      "epoch:43000,loss=0.068\n",
      "epoch:44000,loss=0.065\n",
      "epoch:45000,loss=0.063\n",
      "epoch:46000,loss=0.061\n",
      "epoch:47000,loss=0.058\n",
      "epoch:48000,loss=0.056\n",
      "epoch:49000,loss=0.054\n",
      "epoch:50000,loss=0.052\n",
      "epoch:51000,loss=0.051\n",
      "epoch:52000,loss=0.049\n",
      "epoch:53000,loss=0.048\n",
      "epoch:54000,loss=0.046\n",
      "epoch:55000,loss=0.045\n",
      "epoch:56000,loss=0.043\n",
      "epoch:57000,loss=0.042\n",
      "epoch:58000,loss=0.040\n",
      "epoch:59000,loss=0.039\n",
      "epoch:60000,loss=0.037\n",
      "epoch:61000,loss=0.036\n",
      "epoch:62000,loss=0.035\n",
      "epoch:63000,loss=0.034\n",
      "epoch:64000,loss=0.033\n",
      "epoch:65000,loss=0.032\n",
      "epoch:66000,loss=0.031\n",
      "epoch:67000,loss=0.030\n",
      "epoch:68000,loss=0.030\n",
      "epoch:69000,loss=0.029\n",
      "epoch:70000,loss=0.028\n",
      "epoch:71000,loss=0.028\n",
      "epoch:72000,loss=0.027\n",
      "epoch:73000,loss=0.027\n",
      "epoch:74000,loss=0.026\n",
      "epoch:75000,loss=0.026\n",
      "epoch:76000,loss=0.025\n",
      "epoch:77000,loss=0.025\n",
      "epoch:78000,loss=0.024\n",
      "epoch:79000,loss=0.024\n",
      "epoch:80000,loss=0.023\n",
      "epoch:81000,loss=0.023\n",
      "epoch:82000,loss=0.023\n",
      "epoch:83000,loss=0.022\n",
      "epoch:84000,loss=0.022\n",
      "epoch:85000,loss=0.022\n",
      "epoch:86000,loss=0.021\n",
      "epoch:87000,loss=0.021\n",
      "epoch:88000,loss=0.021\n",
      "epoch:89000,loss=0.021\n",
      "epoch:90000,loss=0.020\n",
      "epoch:91000,loss=0.020\n",
      "epoch:92000,loss=0.020\n",
      "epoch:93000,loss=0.019\n",
      "epoch:94000,loss=0.019\n",
      "epoch:95000,loss=0.019\n",
      "epoch:96000,loss=0.019\n",
      "epoch:97000,loss=0.018\n",
      "epoch:98000,loss=0.018\n",
      "epoch:99000,loss=0.018\n",
      "epoch:100000,loss=0.018\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.array([*range(-20,20)],dtype=np.float32)\n",
    "X = X*0.1\n",
    "y = [x**3+ x**2 -3*x -1 for x in X]\n",
    "plt.plot(X,y,'ro')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "X_t = torch.tensor(X,dtype=torch.float32)\n",
    "y_t = torch.tensor(y,dtype=torch.float32)\n",
    "X_t = X_t.view(X_t.shape[0],1)\n",
    "y_t = y_t.view(y_t.shape[0],1)\n",
    "\n",
    "class func_simulator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(func_simulator,self).__init__()\n",
    "        self.l1 = nn.Linear(1,128)\n",
    "        self.l2 = nn.Linear(128,10)\n",
    "        self.l3 = nn.Linear(10,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.l1(x))\n",
    "        out = F.relu(self.l2(out))\n",
    "        out = self.l3(out)\n",
    "        return out\n",
    "\n",
    "learning_rate,num_epochs    = 0.0001,100000\n",
    "model                       = func_simulator()\n",
    "loss                        = nn.MSELoss()\n",
    "gradient                    = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "# train on batch\n",
    "for epoch in range(num_epochs):\n",
    "    y_pred = model.forward(X_t)\n",
    "    l = loss(y_pred,y_t)\n",
    "    l.backward()\n",
    "    gradient.step()\n",
    "    gradient.zero_grad()\n",
    "\n",
    "    if(epoch+1)%1000==0:\n",
    "        print(f'epoch:{epoch+1},loss={l.item():.3f}')\n",
    "\n",
    "# test the result, use detach to detrack the parameters. \n",
    "predicted = model(X_t).detach().numpy()\n",
    "plt.plot(X_t,y_t,'ro')\n",
    "plt.plot(X_t,predicted,'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e06e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
