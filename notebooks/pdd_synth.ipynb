{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torchmetrics.utilities.checks import _check_same_shape\n",
    "from torchmetrics import Metric\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from pismemulator.metrics import AbsoluteError, absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDDEmulator(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_parameters: int,\n",
    "        n_outputs: int,\n",
    "        hparams,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        n_layers = self.hparams.n_layers\n",
    "        n_hidden = self.hparams.n_hidden\n",
    "\n",
    "        if isinstance(n_hidden, int):\n",
    "            n_hidden = [n_hidden] * (n_layers - 1)\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.l_first = nn.Linear(n_parameters, n_hidden[0])\n",
    "        self.norm_first = nn.LayerNorm(n_hidden[0])\n",
    "        self.dropout_first = nn.Dropout(p=0.0)\n",
    "\n",
    "        models = []\n",
    "        for n in range(n_layers - 2):\n",
    "            models.append(\n",
    "                nn.Sequential(\n",
    "                    OrderedDict(\n",
    "                        [\n",
    "                            (\"Linear\", nn.Linear(n_hidden[n], n_hidden[n + 1])),\n",
    "                            (\"LayerNorm\", nn.LayerNorm(n_hidden[n + 1])),\n",
    "                            (\"Dropout\", nn.Dropout(p=0.1)),\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        self.dnn = nn.ModuleList(models)\n",
    "        self.l_last = nn.Linear(n_hidden[-1], n_outputs)\n",
    "\n",
    "        self.train_ae = AbsoluteError()\n",
    "        self.test_ae = AbsoluteError()\n",
    "\n",
    "    def forward(self, x, add_mean=False):\n",
    "        # Pass the input tensor through each of our operations\n",
    "\n",
    "        a = self.l_first(x)\n",
    "        a = self.norm_first(a)\n",
    "        a = self.dropout_first(a)\n",
    "        z = torch.relu(a)\n",
    "\n",
    "        for dnn in self.dnn:\n",
    "            a = dnn(z)\n",
    "            z = torch.relu(a) + z\n",
    "\n",
    "        return self.l_last(z)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"NNEmulator\")\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "        parser.add_argument(\"--n_hidden\", default=128)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=0.1)\n",
    "\n",
    "        return parent_parser\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), self.hparams.learning_rate, weight_decay=0.0\n",
    "        )\n",
    "        # This is an approximation to Doug's version:\n",
    "        scheduler = {\n",
    "            \"scheduler\": ExponentialLR(optimizer, 0.9975, verbose=True),\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, f, o, _ = batch\n",
    "        f_pred = self.forward(x)\n",
    "        loss = absolute_error(f_pred, f, o)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, f, o, o_0 = batch\n",
    "        f_pred = self.forward(x)\n",
    "\n",
    "        self.log(\"train_loss\", self.train_ae(f_pred, f, o))\n",
    "        self.log(\"test_loss\", self.test_ae(f_pred, f, o_0))\n",
    "\n",
    "        return {\"x\": x, \"f\": f, \"f_pred\": f_pred, \"o\": o, \"o_0\": o_0}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            self.train_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_loss\",\n",
    "            self.test_ae,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchPDDModel(torch.nn.modules.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    # Copyright (c) 2013--2018, Julien Seguinot <seguinot@vaw.baug.ethz.ch>\n",
    "    # GNU General Public License v3.0+ (https://www.gnu.org/licenses/gpl-3.0.txt)\n",
    "\n",
    "    A positive degree day model for glacier surface mass balance\n",
    "\n",
    "    Return a callable Positive Degree Day (PDD) model instance.\n",
    "\n",
    "    Model parameters are held as public attributes, and can be set using\n",
    "    corresponding keyword arguments at initialization time:\n",
    "\n",
    "    *pdd_factor_snow* : float\n",
    "        Positive degree-day factor for snow.\n",
    "    *pdd_factor_ice* : float\n",
    "        Positive degree-day factor for ice.\n",
    "    *refreeze_snow* : float\n",
    "        Refreezing fraction of melted snow.\n",
    "    *refreeze_ice* : float\n",
    "        Refreezing fraction of melted ice.\n",
    "    *temp_snow* : float\n",
    "        Temperature at which all precipitation falls as snow.\n",
    "    *temp_rain* : float\n",
    "        Temperature at which all precipitation falls as rain.\n",
    "    *interpolate_rule* : [ 'linear' | 'nearest' | 'zero' |\n",
    "                           'slinear' | 'quadratic' | 'cubic' ]\n",
    "        Interpolation rule passed to `scipy.interpolate.interp1d`.\n",
    "    *interpolate_n*: int\n",
    "        Number of points used in interpolations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pdd_factor_snow=3,\n",
    "        pdd_factor_ice=8,\n",
    "        refreeze_snow=0.0,\n",
    "        refreeze_ice=0.0,\n",
    "        temp_snow=0.0,\n",
    "        temp_rain=2.0,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # set pdd model parameters\n",
    "        self.pdd_factor_snow = pdd_factor_snow\n",
    "        self.pdd_factor_ice = pdd_factor_ice\n",
    "        self.refreeze_snow = refreeze_snow\n",
    "        self.refreeze_ice = refreeze_ice\n",
    "        self.temp_snow = temp_snow\n",
    "        self.temp_rain = temp_rain\n",
    "\n",
    "    def forward(self, temp, prec):\n",
    "        \"\"\"Run the positive degree day model.\n",
    "\n",
    "        Use temperature, precipitation, and standard deviation of temperature\n",
    "        to compute the number of positive degree days, accumulation and melt\n",
    "        surface mass fluxes, and the resulting surface mass balance.\n",
    "\n",
    "        *temp*: array_like\n",
    "            Input near-surface air temperature in degrees Celcius.\n",
    "        *prec*: array_like\n",
    "            Input precipitation rate in meter per year.\n",
    "        \"\"\"\n",
    "\n",
    "        # ensure numpy arrays\n",
    "        temp = torch.asarray(temp)\n",
    "        prec = torch.asarray(prec)\n",
    "\n",
    "        # compute accumulation and pdd\n",
    "        accu_rate = self.accu_rate(temp, prec)\n",
    "        inst_pdd = self.inst_pdd(temp)\n",
    "\n",
    "        # initialize snow depth, melt and refreeze rates\n",
    "        snow_depth = torch.zeros_like(temp)\n",
    "        snow_melt_rate = torch.zeros_like(temp)\n",
    "        ice_melt_rate = torch.zeros_like(temp)\n",
    "        snow_refreeze_rate = torch.zeros_like(temp)\n",
    "        ice_refreeze_rate = torch.zeros_like(temp)\n",
    "\n",
    "        # snow_depth[:-1] = torch.clone(snow_depth[1:])\n",
    "        snow_depth = snow_depth + accu_rate\n",
    "        snow_melt_rate, ice_melt_rate = self.melt_rates(snow_depth, inst_pdd)\n",
    "        snow_depth = snow_depth - snow_melt_rate\n",
    "\n",
    "        melt_rate = snow_melt_rate + ice_melt_rate\n",
    "        snow_refreeze_rate = self.refreeze_snow * snow_melt_rate\n",
    "        ice_refreeze_rate = self.refreeze_ice * ice_melt_rate\n",
    "        refreeze_rate = snow_refreeze_rate + ice_refreeze_rate\n",
    "        runoff_rate = melt_rate - refreeze_rate\n",
    "        inst_smb = accu_rate - runoff_rate\n",
    "\n",
    "        # output\n",
    "        return {\n",
    "            \"temp\": temp,\n",
    "            \"prec\": prec,\n",
    "            \"pdds\": inst_pdd,\n",
    "            \"accu_rate\": accu_rate,\n",
    "            \"snow_melt_rate\": snow_melt_rate,\n",
    "            \"ice_melt_rate\": ice_melt_rate,\n",
    "            \"melt_rate\": melt_rate,\n",
    "            \"snow_refreeze_rate\": snow_refreeze_rate,\n",
    "            \"ice_refreeze_rate\": ice_refreeze_rate,\n",
    "            \"refreeze_rate\": refreeze_rate,\n",
    "            \"runoff_rate\": runoff_rate,\n",
    "            \"smb_rate\": inst_smb,\n",
    "            \"snow_depth\": snow_depth,\n",
    "        }\n",
    "\n",
    "\n",
    "    def inst_pdd(self, temp):\n",
    "        \"\"\"Compute instantaneous positive degree days from temperature.\n",
    "\n",
    "        Use near-surface air temperature to compute\n",
    "        positive degree days (effective temperature for melt,\n",
    "        unit degrees C).\n",
    "\n",
    "        *temp*: array_like\n",
    "            Near-surface air temperature in degrees Celcius.\n",
    "        \"\"\"\n",
    "\n",
    "        # compute positive part of temperature everywhere\n",
    "        pdd = torch.greater(temp, 0) * temp\n",
    "\n",
    "        # convert to degree-days\n",
    "        return pdd\n",
    "\n",
    "    def accu_rate(self, temp, prec):\n",
    "        \"\"\"Compute accumulation rate from temperature and precipitation.\n",
    "\n",
    "        The fraction of precipitation that falls as snow decreases linearly\n",
    "        from one to zero between temperature thresholds defined by the\n",
    "        `temp_snow` and `temp_rain` attributes.\n",
    "\n",
    "        *temp*: array_like\n",
    "            Near-surface air temperature in degrees Celcius.\n",
    "        *prec*: array_like\n",
    "            Precipitation rate in meter per year.\n",
    "        \"\"\"\n",
    "\n",
    "        # compute snow fraction as a function of temperature\n",
    "        reduced_temp = (self.temp_rain - temp) / (self.temp_rain - self.temp_snow)\n",
    "        snowfrac = torch.clip(reduced_temp, 0, 1)\n",
    "\n",
    "        # return accumulation rate\n",
    "        return snowfrac * prec\n",
    "\n",
    "    def melt_rates(self, snow, pdd):\n",
    "        \"\"\"Compute melt rates from snow precipitation and pdd sum.\n",
    "\n",
    "        Snow melt is computed from the number of positive degree days (*pdd*)\n",
    "        and the `pdd_factor_snow` model attribute. If all snow is melted and\n",
    "        some energy (PDD) remains, ice melt is computed using `pdd_factor_ice`.\n",
    "\n",
    "        *snow*: array_like\n",
    "            Snow precipitation rate.\n",
    "        *pdd*: array_like\n",
    "            Number of positive degree days.\n",
    "        \"\"\"\n",
    "\n",
    "        # parse model parameters for readability\n",
    "        ddf_snow = self.pdd_factor_snow / 1e3\n",
    "        ddf_ice = self.pdd_factor_ice / 1e3\n",
    "\n",
    "        # compute a potential snow melt\n",
    "        pot_snow_melt = ddf_snow * pdd\n",
    "\n",
    "        # effective snow melt can't exceed amount of snow\n",
    "        snow_melt = torch.minimum(snow, pot_snow_melt)\n",
    "\n",
    "        # ice melt is proportional to excess snow melt\n",
    "        ice_melt = (pot_snow_melt - snow_melt) * ddf_ice / ddf_snow\n",
    "\n",
    "        # return melt rates\n",
    "        return (snow_melt, ice_melt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0859e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.sample import saltelli\n",
    "from pyDOE import lhs\n",
    "from scipy.stats.distributions import truncnorm, gamma, uniform, randint\n",
    "method = \"lhs\"\n",
    "n_prior_samples = 10000\n",
    "np.random.seed(2)\n",
    "\n",
    "distributions = {\n",
    "    \"T\": uniform(loc=-20, scale=40),\n",
    "    \"P\": uniform(loc=0, scale=1), \n",
    "    \"f_snow\": uniform(\n",
    "        loc=2.0, scale=4.0\n",
    "    ),  # uniform between 2 and 6\n",
    "    \"f_ice\": uniform(\n",
    "        loc=3.0, scale=9\n",
    "    ),  # uniform between 3 and 12\n",
    "    \"refreeze\": uniform(loc=0, scale=1.0),  # uniform between 0 and 1\n",
    "}\n",
    "# Names of all the variables\n",
    "keys = [x for x in distributions.keys()]\n",
    "\n",
    "# Describe the Problem\n",
    "problem = {\"num_vars\": len(keys), \"names\": keys, \"bounds\": [[0, 1]] * len(keys)}\n",
    "\n",
    "# Generate uniform samples (i.e. one unit hypercube)\n",
    "if method == \"saltelli\":\n",
    "    unif_sample = saltelli.sample(problem, n_prior_samples, calc_second_order=False)\n",
    "elif method == \"lhs\":\n",
    "    unif_sample = lhs(len(keys), n_prior_samples)\n",
    "else:\n",
    "    print(f\"Method {method} not available\")\n",
    "\n",
    "# To hold the transformed variables\n",
    "dist_sample = np.zeros_like(unif_sample)\n",
    "\n",
    "# Now transform the unit hypercube to the prescribed distributions\n",
    "# For each variable, transform with the inverse of the CDF (inv(CDF)=ppf)\n",
    "for i, key in enumerate(keys):\n",
    "    dist_sample[:, i] = distributions[key].ppf(unif_sample[:, i])\n",
    "\n",
    "# Save to CSV file using Pandas DataFrame and to_csv method\n",
    "header = keys\n",
    "# Convert to Pandas dataframe, append column headers, output as csv\n",
    "df = pd.DataFrame(data=dist_sample, columns=header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52677e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X = []\n",
    "    Y = []\n",
    "    for k, row in df.iterrows():   \n",
    "        m_f_snow = row[\"f_snow\"]\n",
    "        m_f_ice = row[\"f_ice\"]\n",
    "        m_refreeze = row[\"refreeze\"]\n",
    "        m_T = np.copy(row[\"T\"])\n",
    "        m_P = np.copy(row[\"P\"])\n",
    "\n",
    "        pdd = TorchPDDModel(\n",
    "            pdd_factor_snow=m_f_snow,\n",
    "            pdd_factor_ice=m_f_ice,\n",
    "            refreeze_snow=m_refreeze,\n",
    "            refreeze_ice=m_refreeze,\n",
    "        )\n",
    "        result = pdd(m_T, m_P)\n",
    "\n",
    "        M_train = result[\"melt_rate\"]\n",
    "        A_train = result[\"accu_rate\"]\n",
    "        R_train = result[\"refreeze_rate\"]\n",
    "        B_train = result[\"smb_rate\"]\n",
    "        m_Y = torch.vstack((M_train, A_train, R_train,)).T\n",
    "        Y.append(m_Y)\n",
    "        X.append(torch.from_numpy(np.hstack((m_P, m_T, m_f_snow, m_f_ice, m_refreeze))))\n",
    "\n",
    "    X_train = torch.vstack(X).type(torch.FloatTensor)\n",
    "    Y_train = torch.vstack(Y).type(torch.FloatTensor)\n",
    "    n_samples, n_parameters = X_train.shape\n",
    "    n_outputs = Y_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a45752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "X_train_mean = X_train.mean(axis=0)\n",
    "X_train_std = X_train.std(axis=0)\n",
    "X_train_norm = (X_train - X_train_mean) / X_train_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99141a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237942c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import os\n",
    "    from scipy.stats import dirichlet\n",
    "\n",
    "    model_index = 0\n",
    "    torch.manual_seed(0)\n",
    "    pl.seed_everything(0)\n",
    "    np.random.seed(model_index)\n",
    "    emulator_dir = \"pddemulator\"\n",
    "\n",
    "    if not os.path.isdir(emulator_dir):\n",
    "        os.makedirs(emulator_dir)\n",
    "        os.makedirs(os.path.join(emulator_dir, \"emulator\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) 2021 Andy Aschwanden, Douglas C Brinkerhoff\n",
    "#\n",
    "# This file is part of pism-emulator.\n",
    "#\n",
    "# PISM-EMULATOR is free software; you can redistribute it and/or modify it under the\n",
    "# terms of the GNU General Public License as published by the Free Software\n",
    "# Foundation; either version 3 of the License, or (at your option) any later\n",
    "# version.\n",
    "#\n",
    "# PISM-EMULATOR is distributed in the hope that it will be useful, but WITHOUT ANY\n",
    "# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n",
    "# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n",
    "# details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with PISM; if not, write to the Free Software\n",
    "# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torchmetrics.utilities.checks import _check_same_shape\n",
    "from torchmetrics import Metric\n",
    "\n",
    "\n",
    "def _absolute_error_update(\n",
    "    preds: Tensor, target: Tensor, omegas: Tensor) -> Tensor:\n",
    "    _check_same_shape(preds, target)\n",
    "    diff = torch.abs(preds - target)\n",
    "    sum_abs_error = torch.sum(diff * diff, axis=1)\n",
    "    absolute_error = torch.sum(sum_abs_error * omegas.squeeze())\n",
    "    return absolute_error\n",
    "\n",
    "\n",
    "def _absolute_error_compute(absolute_error) -> Tensor:\n",
    "    return absolute_error\n",
    "\n",
    "\n",
    "def absolute_error(\n",
    "    preds: Tensor, target: Tensor, omegas: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes squared absolute error\n",
    "    Args:\n",
    "        preds: estimated labels\n",
    "        target: ground truth labels\n",
    "        omegas: weights\n",
    "        area: area of each cell\n",
    "    Return:\n",
    "        Tensor with absolute error\n",
    "    Example:\n",
    "        >>> x = torch.tensor([[0, 1, 2, 3], [1, 2, 3, 4]]).T\n",
    "        >>> y = torch.tensor([[0, 1, 2, 1], [2, 3, 4, 4]]).T\n",
    "        >>> o = torch.tensor([0.25, 0.25, 0.3, 0.2])\n",
    "        >>> a = torch.tensor([0.25, 0.25])\n",
    "        >>> absolute_error(x, y, o, a)\n",
    "        tensor(0.4000)\n",
    "    \"\"\"\n",
    "    sum_abs_error = _absolute_error_update(preds, target, omegas)\n",
    "    return _absolute_error_compute(sum_abs_error)\n",
    "\n",
    "\n",
    "class AbsoluteError(Metric):\n",
    "    def __init__(self, compute_on_step: bool = True, dist_sync_on_step=False):\n",
    "        # call `self.add_state`for every internal state that is needed for the metrics computations\n",
    "        # dist_reduce_fx indicates the function that should be used to reduce\n",
    "        # state from multiple processes\n",
    "        super().__init__(\n",
    "            compute_on_step=compute_on_step, dist_sync_on_step=dist_sync_on_step\n",
    "        )\n",
    "\n",
    "        self.add_state(\"sum_abs_error\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: Tensor, target: Tensor, omegas: Tensor):\n",
    "        \"\"\"\n",
    "        Update state with predictions and targets, and area.\n",
    "        Args:\n",
    "            preds: Predictions from model\n",
    "            target: Ground truth values\n",
    "            omegas: Weights\n",
    "            area: Area of each cell\n",
    "        \"\"\"\n",
    "        sum_abs_error = _absolute_error_update(preds, target, omegas)\n",
    "        self.sum_abs_error += sum_abs_error\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Computes absolute error over state.\n",
    "        \"\"\"\n",
    "        return _absolute_error_compute(self.sum_abs_error)\n",
    "\n",
    "    @property\n",
    "    def is_differentiable(self):\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDDDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X,\n",
    "        Y,\n",
    "        omegas,\n",
    "        omegas_0,\n",
    "        batch_size: int = 128,\n",
    "        train_size: float = 0.9,\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.omegas = omegas\n",
    "        self.omegas_0 = omegas_0\n",
    "        self.batch_size = batch_size\n",
    "        self.train_size = train_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "\n",
    "        all_data = TensorDataset(self.X, self.Y, self.omegas, self.omegas_0)\n",
    "        self.all_data = all_data\n",
    "\n",
    "        training_data, val_data = train_test_split(\n",
    "            all_data, train_size=self.train_size, random_state=0\n",
    "        )\n",
    "        self.training_data = training_data\n",
    "        self.test_data = training_data\n",
    "\n",
    "        self.val_data = val_data\n",
    "        train_all_loader = DataLoader(\n",
    "            dataset=all_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.train_all_loader = train_all_loader\n",
    "        val_all_loader = DataLoader(\n",
    "            dataset=all_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.val_all_loader = val_all_loader\n",
    "        train_loader = DataLoader(\n",
    "            dataset=training_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = train_loader\n",
    "        val_loader = DataLoader(\n",
    "            dataset=val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def prepare_data(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self.val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b35190",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(f\"Training model {model_index}\")\n",
    "    omegas = torch.Tensor(dirichlet.rvs(np.ones(n_prior_samples))).T\n",
    "    omegas = omegas.type(torch.FloatTensor)\n",
    "    omegas_0 = torch.ones_like(omegas) / len(omegas)\n",
    "    area = torch.ones_like(omegas)\n",
    "    train_size = 1.0\n",
    "    num_workers = 8\n",
    "    hparams = {\"n_layers\": 5, \"n_hidden\": 128, \"batch_size\": 128, \"learning_rate\": 0.01}\n",
    "    \n",
    "    if train_size == 1.0:\n",
    "        data_loader = PDDDataModule(X_train_norm, Y_train, omegas, omegas_0, num_workers=num_workers)\n",
    "    else:\n",
    "        data_loader = PDDDataModule(\n",
    "            X_train_norm, Y_train, omegas, omegas_0, train_size=train_size, num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    data_loader.setup()\n",
    "    e = PDDEmulator(\n",
    "        n_parameters,\n",
    "        n_outputs,\n",
    "        hparams,\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        auto_lr_find=True,\n",
    "        max_epochs=2000,\n",
    "        gpus=1,\n",
    "#        deterministic=True,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    if train_size == 1.0:\n",
    "        train_loader = data_loader.train_all_loader\n",
    "        val_loader = data_loader.val_all_loader\n",
    "    else:\n",
    "        train_loader = data_loader.train_loader\n",
    "        val_loader = data_loader.val_loader\n",
    "\n",
    "        \n",
    "    # lr_finder = trainer.tuner.lr_find(e, train_loader, val_loader)\n",
    "    # fig = lr_finder.plot(suggest=True) # Plot\n",
    "    # fig.show()\n",
    "    trainer.fit(e, train_loader, val_loader)\n",
    "    torch.save(e.state_dict(), f\"{emulator_dir}/emulator/emulator_{model_index}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val_samples = 1000\n",
    "np.random.seed(3)\n",
    "\n",
    "distributions = {\n",
    "    \"T\": uniform(loc=-20, scale=40),\n",
    "    \"P\": uniform(loc=0, scale=1), \n",
    "    \"f_snow\": uniform(\n",
    "        loc=2.0, scale=4.0\n",
    "    ), \n",
    "    \"f_ice\": uniform(\n",
    "        loc=3.0, scale=9\n",
    "    ),  # uniform between 3 and 3.5  AS16 best value: 3.25\n",
    "    \"refreeze\": uniform(loc=0, scale=1.0),  # uniform between 0.25 and 0.95\n",
    "}\n",
    "# Names of all the variables\n",
    "keys = [x for x in distributions.keys()]\n",
    "\n",
    "# Describe the Problem\n",
    "problem = {\"num_vars\": len(keys), \"names\": keys, \"bounds\": [[0, 1]] * len(keys)}\n",
    "\n",
    "unif_sample = lhs(len(keys), n_val_samples)\n",
    "\n",
    "# To hold the transformed variables\n",
    "dist_sample = np.zeros_like(unif_sample)\n",
    "\n",
    "# Now transform the unit hypercube to the prescribed distributions\n",
    "# For each variable, transform with the inverse of the CDF (inv(CDF)=ppf)\n",
    "for i, key in enumerate(keys):\n",
    "    dist_sample[:, i] = distributions[key].ppf(unif_sample[:, i])\n",
    "\n",
    "# Save to CSV file using Pandas DataFrame and to_csv method\n",
    "header = keys\n",
    "# Convert to Pandas dataframe, append column headers, output as csv\n",
    "df = pd.DataFrame(data=dist_sample, columns=header)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8761667",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X = []\n",
    "    Y = []\n",
    "    for k, row in df.iterrows():   \n",
    "        m_f_snow = row[\"f_snow\"]\n",
    "        m_f_ice = row[\"f_ice\"]\n",
    "        m_refreeze = row[\"refreeze\"]\n",
    "        m_T = np.copy(row[\"T\"])\n",
    "        m_P = np.copy(row[\"P\"])\n",
    "\n",
    "        pdd = TorchPDDModel(\n",
    "            pdd_factor_snow=m_f_snow,\n",
    "            pdd_factor_ice=m_f_ice,\n",
    "            refreeze_snow=m_refreeze,\n",
    "            refreeze_ice=m_refreeze,\n",
    "        )\n",
    "        result = pdd(m_T, m_P)\n",
    "\n",
    "        M = result[\"melt_rate\"]\n",
    "        A = result[\"accu_rate\"]\n",
    "        R = result[\"refreeze_rate\"]\n",
    "        B = result[\"smb_rate\"]\n",
    "        m_Y = torch.vstack((M, A, R)).T\n",
    "        Y.append(m_Y)\n",
    "        X.append(torch.from_numpy(np.hstack((m_P, m_T, m_f_snow, m_f_ice, m_refreeze))))\n",
    "\n",
    "    X_val = torch.vstack(X).type(torch.FloatTensor)\n",
    "    Y_val = torch.vstack(Y).type(torch.FloatTensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f097205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "device = \"cuda\"\n",
    "e.to(device)\n",
    "X_val = X_val.to(device)\n",
    "e.eval()\n",
    "Y_pred = e(X_val).detach().cpu()\n",
    "rmse = [np.sqrt(mean_squared_error(Y_pred.detach().cpu().numpy()[:,i], Y_val.detach().cpu().numpy()[:,i])) for i in range(Y_val.shape[1])]\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6249f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred - Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e61c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MALASampler(object):\n",
    "    \"\"\"\n",
    "    MALA Sampler\n",
    "\n",
    "    Author: Douglas C Brinkerhoff, University of Montana\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self, model, alpha_b=3.0, beta_b=3.0, alpha=0.01, emulator_dir=\"./emulator\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model.eval()\n",
    "        self.alpha = alpha\n",
    "        self.alpha_b = alpha_b\n",
    "        self.beta_b = beta_b\n",
    "        self.emulator_dir = emulator_dir\n",
    "\n",
    "    def find_MAP(self, X, X_I, Y_target, X_min, X_max, n_iters=50, print_interval=10):\n",
    "        print(\"***********************************************\")\n",
    "        print(\"***********************************************\")\n",
    "        print(\"Finding MAP point\")\n",
    "        print(\"***********************************************\")\n",
    "        print(\"***********************************************\")\n",
    "        # Line search distances\n",
    "        alphas = np.logspace(-4, 0, 11)\n",
    "        # Find MAP point\n",
    "        for i in range(n_iters):\n",
    "            log_pi, g, _, Hinv, log_det_Hinv = self.get_log_like_gradient_and_hessian(\n",
    "                X, X_I, Y_target, X_min, X_max, compute_hessian=True\n",
    "            )\n",
    "            p = Hinv @ -g\n",
    "            alpha_index = np.nanargmin(\n",
    "                [\n",
    "                    self.get_log_like_gradient_and_hessian(\n",
    "                        X + alpha * p, X_I, Y_target, X_min, X_max, compute_hessian=False\n",
    "                    )\n",
    "                    .detach()\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                    for alpha in alphas\n",
    "                ]\n",
    "            )\n",
    "            mu = X + alphas[alpha_index] * p\n",
    "            X.data = mu.data\n",
    "            if i % print_interval == 0:\n",
    "                print(\"===============================================\")\n",
    "                print(f\"iter: {i:d}, log(P): {log_pi:.1f}\\n\")\n",
    "                print(\n",
    "                    \"\".join(\n",
    "                        [\n",
    "                            f\"{key}: {(val * std + mean):.3f}\\n\"\n",
    "                            for key, val, std, mean in zip(\n",
    "                                X_P_keys,\n",
    "                                X.data.cpu().numpy(),\n",
    "                                X_P_std,\n",
    "                                X_P_mean,\n",
    "                            )\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "                print(\"===============================================\")\n",
    "        return X\n",
    "\n",
    "    def V(self, X, X_I, Y_target, X_bar):\n",
    "        # model result is in log space\n",
    "        X_IP = torch.hstack((X, X_I))\n",
    "        Y_pred = self.model(X_IP)\n",
    "        r = Y_pred - Y_target\n",
    "        L1 = torch.sum(\n",
    "            np.log(gamma((nu + 1) / 2.0))\n",
    "            - np.log(gamma(nu / 2.0))\n",
    "            - np.log(np.sqrt(np.pi * nu) * sigma_hat)\n",
    "            - (nu + 1) / 2.0 * torch.log(1 + 1.0 / nu * (r / sigma_hat) ** 2)\n",
    "        )\n",
    "        L2 = torch.sum(\n",
    "            (self.alpha_b - 1) * torch.log(X_bar)\n",
    "            + (self.beta_b - 1) * torch.log(1 - X_bar)\n",
    "        )\n",
    "\n",
    "        return -(self.alpha * L1 + L2)\n",
    "\n",
    "    def get_log_like_gradient_and_hessian(\n",
    "        self, X, X_I, Y_target, X_min, X_max, eps=1e-2, compute_hessian=False\n",
    "    ):\n",
    "\n",
    "        X_bar = (X - X_min) / (X_max - X_min)\n",
    "        log_pi = self.V(X, X_I, Y_target, X_bar)\n",
    "        if compute_hessian:\n",
    "            g = torch.autograd.grad(log_pi, X, retain_graph=True, create_graph=True)[0]\n",
    "            H = torch.stack(\n",
    "                [torch.autograd.grad(e, X, retain_graph=True)[0] for e in g]\n",
    "            )\n",
    "            lamda, Q = torch.linalg.eig(H)\n",
    "            lamda, Q = lamda.type(torch.float), Q.type(torch.float)\n",
    "            lamda_prime = torch.sqrt(lamda ** 2 + eps)\n",
    "            lamda_prime_inv = 1.0 / torch.sqrt(lamda ** 2 + eps)\n",
    "            H = Q @ torch.diag(lamda_prime) @ Q.T\n",
    "            Hinv = Q @ torch.diag(lamda_prime_inv) @ Q.T\n",
    "            log_det_Hinv = torch.sum(torch.log(lamda_prime_inv))\n",
    "            return log_pi, g, H, Hinv, log_det_Hinv\n",
    "        else:\n",
    "            return log_pi\n",
    "\n",
    "    def draw_sample(self, mu, cov, eps=1e-10):\n",
    "        L = torch.linalg.cholesky(cov + eps * torch.eye(cov.shape[0], device=device))\n",
    "        return mu + L @ torch.randn(L.shape[0], device=device)\n",
    "\n",
    "    def get_proposal_likelihood(self, Y, mu, inverse_cov, log_det_cov):\n",
    "        return -0.5 * log_det_cov - 0.5 * (Y - mu) @ inverse_cov @ (Y - mu)\n",
    "\n",
    "    def MALA_step(self, X, X_I, Y_target, X_min, X_max, h, local_data=None):\n",
    "        if local_data is not None:\n",
    "            pass\n",
    "        else:\n",
    "            local_data = self.get_log_like_gradient_and_hessian(\n",
    "                X, X_I, Y_target, X_min, X_max, compute_hessian=True\n",
    "            )\n",
    "\n",
    "        log_pi, _, H, Hinv, log_det_Hinv = local_data\n",
    "\n",
    "        X_ = self.draw_sample(X, 2 * h * Hinv).detach()\n",
    "        X_.requires_grad = True\n",
    "\n",
    "        log_pi_ = self.get_log_like_gradient_and_hessian(\n",
    "            X_, X_I, Y_target, X_min, X_max, compute_hessian=False\n",
    "        )\n",
    "\n",
    "        logq = self.get_proposal_likelihood(X_, X, H / (2 * h), log_det_Hinv)\n",
    "        logq_ = self.get_proposal_likelihood(X, X_, H / (2 * h), log_det_Hinv)\n",
    "\n",
    "        log_alpha = -log_pi_ + logq_ + log_pi - logq\n",
    "        alpha = torch.exp(min(log_alpha, torch.tensor([0.0], device=device)))\n",
    "        u = torch.rand(1, device=device)\n",
    "        if u <= alpha and log_alpha != np.inf:\n",
    "            X.data = X_.data\n",
    "            local_data = self.get_log_like_gradient_and_hessian(\n",
    "                X, X_I, Y_target, X_min, X_max, compute_hessian=True\n",
    "            )\n",
    "            s = 1\n",
    "        else:\n",
    "            s = 0\n",
    "        return X, local_data, s\n",
    "\n",
    "    def MALA(\n",
    "        self,\n",
    "        X,\n",
    "        X_I,\n",
    "        X_min,\n",
    "        X_max,\n",
    "        Y_target,\n",
    "        n_iters=10001,\n",
    "        h=0.1,\n",
    "        h_max=1.0,\n",
    "        acc_target=0.25,\n",
    "        k=0.01,\n",
    "        beta=0.99,\n",
    "        model_index=0,\n",
    "        save_interval=1000,\n",
    "        print_interval=50,\n",
    "    ):\n",
    "        print(\"***********************************************\")\n",
    "        print(\"***********************************************\")\n",
    "        print(\n",
    "            \"Running Metropolis-Adjusted Langevin Algorithm for model index {0}\".format(\n",
    "                model_index\n",
    "            )\n",
    "        )\n",
    "        print(\"***********************************************\")\n",
    "        print(\"***********************************************\")\n",
    "\n",
    "        posterior_dir = f\"{self.emulator_dir}/posterior_samples/\"\n",
    "        if not os.path.isdir(posterior_dir):\n",
    "            os.makedirs(posterior_dir)\n",
    "\n",
    "        local_data = None\n",
    "        m_vars = []\n",
    "        acc = acc_target\n",
    "        print(n_iters)\n",
    "        for i in range(n_iters):\n",
    "            X, local_data, s = self.MALA_step(\n",
    "                X, X_I, Y_target, X_min, X_max, h, local_data=local_data\n",
    "            )\n",
    "            m_vars.append(X.detach())\n",
    "            acc = beta * acc + (1 - beta) * s\n",
    "            h = min(h * (1 + k * np.sign(acc - acc_target)), h_max)\n",
    "            if i % print_interval == 0:\n",
    "                print(\"===============================================\")\n",
    "                print(\n",
    "                    \"sample: {0:d}, acc. rate: {1:4.2f}, log(P): {2:6.1f}\".format(\n",
    "                        i, acc, local_data[0].item()\n",
    "                    )\n",
    "                )\n",
    "                print(\n",
    "                    \" \".join(\n",
    "                        [\n",
    "                            f\"{key}: {(val * std + mean):.3f}\\n\"\n",
    "                            for key, val, std, mean in zip(\n",
    "                                X_P_keys,\n",
    "                                X.data.cpu().numpy(),\n",
    "                                X_P_std,\n",
    "                                X_P_mean,\n",
    "                            )\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "                print(\"===============================================\")\n",
    "\n",
    "            if i % save_interval == 0:\n",
    "                print(\"///////////////////////////////////////////////\")\n",
    "                print(\"Saving samples for model {0}\".format(model_index))\n",
    "                print(\"///////////////////////////////////////////////\")\n",
    "                X_posterior = torch.stack(m_vars).cpu().numpy()\n",
    "                df = pd.DataFrame(\n",
    "                    data=X_posterior.astype(\"float32\") * X_P_std.cpu().numpy()\n",
    "                    + X_P_mean.cpu().numpy(),\n",
    "                    columns=X_P_keys,\n",
    "                )\n",
    "                df.to_csv(\n",
    "                    posterior_dir + \"X_posterior_model_{0}.csv.gz\".format(model_index),\n",
    "                    compression=\"infer\",\n",
    "                )\n",
    "        X_posterior = torch.stack(m_vars).cpu().numpy()\n",
    "        return X_posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18975c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from scipy.stats import beta, uniform\n",
    "    from scipy.special import gamma\n",
    "\n",
    "    device=\"cuda\"\n",
    "    nu = 1\n",
    "    n_iters=100000\n",
    "    n_draws=100000\n",
    "    n_prior_samples=100000\n",
    "\n",
    "    distributions = {\n",
    "        \"T\": uniform(loc=-20, scale=40),\n",
    "        \"P\": uniform(loc=0, scale=1), \n",
    "        \"f_snow\": uniform(\n",
    "            loc=2.0, scale=4.0\n",
    "        ), \n",
    "        \"f_ice\": uniform(\n",
    "            loc=3.0, scale=9\n",
    "        ),  # uniform between 3 and 3.5  AS16 best value: 3.25\n",
    "        \"refreeze\": uniform(loc=0, scale=1.0),  # uniform between 0.25 and 0.95    \n",
    "    }\n",
    "    # Names of all the variables\n",
    "    keys = [x for x in distributions.keys()]\n",
    "\n",
    "    # Describe the Problem\n",
    "    problem = {\"num_vars\": len(keys), \"names\": keys, \"bounds\": [[0, 1]] * len(keys)}\n",
    "\n",
    "    # Generate uniform samples (i.e. one unit hypercube)\n",
    "    if method == \"saltelli\":\n",
    "        unif_sample = saltelli.sample(problem, n_prior_samples, calc_second_order=False)\n",
    "    elif method == \"lhs\":\n",
    "        unif_sample = lhs(len(keys), n_prior_samples)\n",
    "    else:\n",
    "        print(f\"Method {method} not available\")\n",
    "\n",
    "    # To hold the transformed variables\n",
    "    dist_sample = np.zeros_like(unif_sample)\n",
    "\n",
    "    # Now transform the unit hypercube to the prescribed distributions\n",
    "    # For each variable, transform with the inverse of the CDF (inv(CDF)=ppf)\n",
    "    for i, key in enumerate(keys):\n",
    "        dist_sample[:, i] = distributions[key].ppf(unif_sample[:, i])\n",
    "\n",
    "    # Save to CSV file using Pandas DataFrame and to_csv method\n",
    "    header = keys\n",
    "    # Convert to Pandas dataframe, append column headers, output as csv\n",
    "    df = pd.DataFrame(data=dist_sample, columns=header)\n",
    "    \n",
    "    f_snow_test = 3.0\n",
    "    f_ice_test = 8.0\n",
    "    refreeze_test = 0.0\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for k, row in df.iterrows():   \n",
    "\n",
    "        m_T = np.copy(row[\"T\"])\n",
    "        m_P = np.copy(row[\"P\"])\n",
    "        m_f_snow = np.copy(row[\"f_snow\"])\n",
    "        m_f_ice = np.copy(row[\"f_ice\"])\n",
    "        m_refreeze = np.copy(row[\"refreeze\"])\n",
    "\n",
    "        pdd = TorchPDDModel(\n",
    "            pdd_factor_snow=f_snow_test,\n",
    "            pdd_factor_ice=f_ice_test,\n",
    "            refreeze_snow=refreeze_test,\n",
    "            refreeze_ice=refreeze_test,\n",
    "        )\n",
    "        result = pdd(m_T, m_P)\n",
    "\n",
    "        M_train = result[\"melt_rate\"]\n",
    "        A_train = result[\"accu_rate\"]\n",
    "        R_train = result[\"refreeze_rate\"]\n",
    "        B_train = result[\"smb_rate\"]\n",
    "        m_Y = torch.vstack((M_train, A_train, R_train)).T\n",
    "        Y.append(m_Y)\n",
    "        X.append(torch.from_numpy(np.hstack((m_P, m_T, m_f_snow, m_f_ice, m_refreeze))))\n",
    "\n",
    "    X_test = torch.vstack(X).type(torch.FloatTensor)\n",
    "    Y_test = torch.vstack(Y).type(torch.FloatTensor)\n",
    "\n",
    "    X_test_mean = X_test.mean(axis=0)\n",
    "    X_test_std = X_test.std(axis=0)\n",
    "    \n",
    "    X_test_norm = (X_test - X_test_mean) / X_test_std\n",
    "    \n",
    "    X_P_mean = X_test_mean[-3::].to(device)\n",
    "    X_P_std = X_test_std[-3::].to(device)\n",
    "    \n",
    "    X_min = X_train_norm.cpu().numpy().min(axis=0)\n",
    "    X_max = X_train_norm.cpu().numpy().max(axis=0)\n",
    "\n",
    "    sigma = 0.001\n",
    "\n",
    "    rho = 1.0 / (1e4**2)\n",
    "    point_area = 1800 ** 2\n",
    "    K = point_area * rho\n",
    "    sigma_hat = np.sqrt(sigma**2 / K**2)\n",
    "\n",
    "    # Eq 52\n",
    "    # this is 2.0 in the paper\n",
    "    alpha_b = 3.0\n",
    "    beta_b = 3.0\n",
    "    X_P_prior =  beta.rvs(alpha_b, beta_b, size=(n_draws, 3)) * (X_max[-3:] - X_min[-3:]) + X_min[-3:]\n",
    "    X_I_prior = uniform.rvs(0, 1, size=(n_draws, 2)) * (X_max[:-3] - X_min[:-3]) + X_min[:-3]\n",
    "    # X_I_prior = beta.rvs(alpha_b, beta_b, size=(n_draws, 2)) * (X_max[:-3] - X_min[:-3]) + X_min[:-3]\n",
    "\n",
    "\n",
    "    X_min = torch.tensor(X_min, dtype=torch.float32, device=device)\n",
    "    X_max = torch.tensor(X_max, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Needs\n",
    "    # alpha_b, beta_b: float\n",
    "    # alpha: float\n",
    "    # nu: float\n",
    "    # gamma\n",
    "    # sigma_hat\n",
    "    X_P_0 = torch.tensor(X_P_prior.mean(axis=0),\n",
    "                         requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "    X_I_0 = torch.tensor(X_I_prior.mean(axis=0),\n",
    "                         requires_grad=True, dtype=torch.float, device=device)\n",
    "    X_I_prior = torch.tensor(X_I_prior, dtype=torch.float, device=device)\n",
    "    \n",
    "    X_P_min = X_min[-3:]\n",
    "    X_P_max = X_max[-3:]\n",
    "    \n",
    "    U_target = Y_test.to(device)\n",
    "\n",
    "    X_P_keys = [\"f_snow\", \"f_ice\", \"refreeze\"]\n",
    "    mala = MALASampler(e.to(device), emulator_dir=emulator_dir)\n",
    "    X_map = mala.find_MAP(X_P_0, X_I_0, U_target, X_P_min, X_P_max)\n",
    "    \n",
    "    # To reproduce the paper, n_iters should be 10^5\n",
    "    X_posterior = mala.MALA(\n",
    "        X_map,\n",
    "        X_I_0,\n",
    "        X_P_min,\n",
    "        X_P_max,\n",
    "        U_target,\n",
    "        n_iters=n_iters,\n",
    "        model_index=int(model_index),\n",
    "        save_interval=1000,\n",
    "        print_interval=100,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b78268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prior_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103cb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import beta, gaussian_kde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7567f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_I_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e292ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X_std = X_P_std\n",
    "    X_mean = X_P_mean\n",
    "    frac = 1.0\n",
    "    lw = 1\n",
    "    color_prior = \"b\"\n",
    "    X_list = []\n",
    "    X_prior = (X_P_prior* X_P_std[-3::] .detach().cpu().numpy() + X_P_mean[-3::] .detach().cpu().numpy())\n",
    "    keys_dict = {\"f_ice\": \"$f_{\\mathrm{ice}}$\", \"f_snow\": \"$f_{\\mathrm{snoe}}$\", \"refreeze\": \"$r$\"}\n",
    "    p = Path(f\"{emulator_dir}/posterior_samples/\")\n",
    "    print(\"Loading posterior samples\\n\")\n",
    "    for m, m_file in enumerate(sorted(p.glob(\"X_posterior_model_*.csv.gz\"))):\n",
    "        print(f\"  -- {m_file}\")\n",
    "        df = pd.read_csv(m_file).sample(frac=frac)\n",
    "        if \"Unnamed: 0\" in df.columns:\n",
    "            df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "        model = m_file.name.split(\"_\")[-1].split(\".\")[0]\n",
    "        df[\"Model\"] = int(model)\n",
    "        X_list.append(df)\n",
    "\n",
    "    print(f\"Merging posteriors into dataframe\")\n",
    "    posterior_df = pd.concat(X_list)\n",
    "\n",
    "    X_posterior = posterior_df.drop(columns=[\"Model\"]).values\n",
    "    C_0 = np.corrcoef((X_posterior - X_posterior.mean(axis=0)).T)\n",
    "    Cn_0 = (np.sign(C_0) * C_0 ** 2 + 1) / 2.0\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(5.4, 2.8))\n",
    "    fig.subplots_adjust(hspace=0.0, wspace=0.0)\n",
    "    for i in range(3):\n",
    "        min_val = min(X_prior[:, i].min(), X_posterior[:, i].min())\n",
    "        max_val = max(X_prior[:, i].max(), X_posterior[:, i].max())\n",
    "        bins = np.linspace(min_val, max_val, 30)\n",
    "        X_prior_hist, b = np.histogram(X_prior[:, i] , bins, density=True)\n",
    "        X_posterior_hist, _ = np.histogram(X_posterior[:, i], bins, density=True)\n",
    "        b = 0.5 * (b[1:] + b[:-1])\n",
    "        axs[i].plot(\n",
    "            b,\n",
    "            X_posterior_hist * 0.5,\n",
    "            color=\"0.5\",\n",
    "            linewidth=lw * 0.25,\n",
    "            linestyle=\"solid\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "\n",
    "    figfile = f\"{emulator_dir}/posterior.pdf\"\n",
    "    print(f\"Saving figure to {figfile}\")\n",
    "    fig.savefig(figfile)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(5.4, 5.6))\n",
    "    fig.subplots_adjust(hspace=0.0, wspace=0.0)\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if i > j:\n",
    "\n",
    "                axs[i, j].scatter(\n",
    "                    X_posterior[:, j],\n",
    "                    X_posterior[:, i],\n",
    "                    c=\"#31a354\",\n",
    "                    s=0.05,\n",
    "                    alpha=0.01,\n",
    "                    label=\"Posterior\",\n",
    "                    rasterized=True,\n",
    "                )\n",
    "\n",
    "                min_val = min(X_prior[:, i].min(), X_posterior[:, i].min())\n",
    "                max_val = max(X_prior[:, i].max(), X_posterior[:, i].max())\n",
    "                bins_y = np.linspace(min_val, max_val, 30)\n",
    "\n",
    "                min_val = min(X_prior[:, j].min(), X_posterior[:, j].min())\n",
    "                max_val = max(X_prior[:, j].max(), X_posterior[:, j].max())\n",
    "                bins_x = np.linspace(min_val, max_val, 30)\n",
    "\n",
    "                v = gaussian_kde(X_posterior[:, [j, i]].T)\n",
    "                bx = 0.5 * (bins_x[1:] + bins_x[:-1])\n",
    "                by = 0.5 * (bins_y[1:] + bins_y[:-1])\n",
    "                Bx, By = np.meshgrid(bx, by)\n",
    "\n",
    "                axs[i, j].contour(\n",
    "                    Bx,\n",
    "                    By,\n",
    "                    v(np.vstack((Bx.ravel(), By.ravel()))).reshape(Bx.shape),\n",
    "                    7,\n",
    "                    linewidths=0.5,\n",
    "                    colors=\"black\",\n",
    "                )\n",
    "\n",
    "                axs[i, j].set_xlim(X_prior[:, j].min(), X_prior[:, j].max())\n",
    "                axs[i, j].set_ylim(X_prior[:, i].min(), X_prior[:, i].max())\n",
    "\n",
    "            elif i < j:\n",
    "                patch_upper = Polygon(\n",
    "                    np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 1.0], [1.0, 0.0]]),\n",
    "                    facecolor=plt.cm.seismic(Cn_0[i, j]),\n",
    "                )\n",
    "                axs[i, j].add_patch(patch_upper)\n",
    "                if C_0[i, j] > -0.5:\n",
    "                    color = \"black\"\n",
    "                else:\n",
    "                    color = \"white\"\n",
    "                axs[i, j].text(\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    \"{0:.2f}\".format(C_0[i, j]),\n",
    "                    fontsize=6,\n",
    "                    horizontalalignment=\"center\",\n",
    "                    verticalalignment=\"center\",\n",
    "                    transform=axs[i, j].transAxes,\n",
    "                    color=color,\n",
    "                )\n",
    "\n",
    "            elif i == j:\n",
    "                min_val = min(X_prior[:, i].min(), X_posterior[:, i].min())\n",
    "                max_val = max(X_prior[:, i].max(), X_posterior[:, i].max())\n",
    "                bins = np.linspace(min_val, max_val, 30)\n",
    "                X_prior_hist, b = np.histogram(X_prior[:, i], bins, density=True)\n",
    "                X_posterior_hist, _ = np.histogram(\n",
    "                    X_posterior[:, i], bins, density=True\n",
    "                )\n",
    "                b = 0.5 * (b[1:] + b[:-1])\n",
    "\n",
    "                axs[i, j].plot(\n",
    "                    b,\n",
    "                    X_prior_hist,\n",
    "                    color=color_prior,\n",
    "                    linewidth=lw,\n",
    "                    label=\"Prior\",\n",
    "                    linestyle=\"solid\",\n",
    "                )\n",
    "\n",
    "                all_models = posterior_df[\"Model\"].unique()\n",
    "                for k, m_model in enumerate(all_models):\n",
    "                    m_df = posterior_df[posterior_df[\"Model\"] == m_model].drop(\n",
    "                        columns=[\"Model\"]\n",
    "                    )\n",
    "                    X_model_posterior = m_df.values\n",
    "                    X_model_posterior_hist, _ = np.histogram(\n",
    "                        X_model_posterior[:, i], _, density=True\n",
    "                    )\n",
    "                    if k == 0:\n",
    "                        axs[i, j].plot(\n",
    "                            b,\n",
    "                            X_model_posterior_hist * 0.5,\n",
    "                            color=\"0.5\",\n",
    "                            linewidth=lw * 0.25,\n",
    "                            linestyle=\"solid\",\n",
    "                            alpha=0.5,\n",
    "                            label=\"Posterior (BayesBag)\",\n",
    "                        )\n",
    "                    else:\n",
    "                        axs[i, j].plot(\n",
    "                            b,\n",
    "                            X_model_posterior_hist * 0.5,\n",
    "                            color=\"0.5\",\n",
    "                            linewidth=lw * 0.25,\n",
    "                            linestyle=\"solid\",\n",
    "                            alpha=0.5,\n",
    "                        )\n",
    "\n",
    "                axs[i, j].plot(\n",
    "                    b,\n",
    "                    X_posterior_hist,\n",
    "                    color=\"black\",\n",
    "                    linewidth=lw,\n",
    "                    linestyle=\"solid\",\n",
    "                    label=\"Posterior\",\n",
    "                )\n",
    "\n",
    "                axs[i, j].set_xlim(min_val, max_val)\n",
    "\n",
    "            else:\n",
    "                axs[i, j].remove()\n",
    "\n",
    "    for i, ax in enumerate(axs[:, 0]):\n",
    "        ax.set_ylabel(keys_dict[X_keys[i]])\n",
    "\n",
    "    for j, ax in enumerate(axs[-1, :]):\n",
    "        ax.set_xlabel(keys_dict[X_keys[j]])\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "        plt.setp(ax.xaxis.get_minorticklabels(), rotation=45)\n",
    "        if j > 0:\n",
    "            ax.tick_params(axis=\"y\", which=\"both\", length=0)\n",
    "            ax.yaxis.set_minor_formatter(NullFormatter())\n",
    "            ax.yaxis.set_major_formatter(NullFormatter())\n",
    "\n",
    "    for ax in axs[:-1, 0].ravel():\n",
    "        ax.xaxis.set_major_formatter(NullFormatter())\n",
    "        ax.xaxis.set_minor_formatter(NullFormatter())\n",
    "        ax.tick_params(axis=\"x\", which=\"both\", length=0)\n",
    "\n",
    "    for ax in axs[:-1, 1:].ravel():\n",
    "        ax.xaxis.set_major_formatter(NullFormatter())\n",
    "        ax.xaxis.set_minor_formatter(NullFormatter())\n",
    "        ax.yaxis.set_major_formatter(NullFormatter())\n",
    "        ax.yaxis.set_minor_formatter(NullFormatter())\n",
    "        ax.tick_params(axis=\"both\", which=\"both\", length=0)\n",
    "\n",
    "    l_prior = Line2D([], [], c=color_prior, lw=lw, ls=\"solid\", label=\"Prior\")\n",
    "    l_post = Line2D([], [], c=\"k\", lw=lw, ls=\"solid\", label=\"Posterior\")\n",
    "    l_post_b = Line2D(\n",
    "        [], [], c=\"0.25\", lw=lw * 0.25, ls=\"solid\", label=\"Posterior (BayesBag)\"\n",
    "    )\n",
    "\n",
    "    legend = fig.legend(\n",
    "        handles=[l_prior, l_post, l_post_b], bbox_to_anchor=(0.3, 0.955)\n",
    "    )\n",
    "    legend.get_frame().set_linewidth(0.0)\n",
    "    legend.get_frame().set_alpha(0.0)\n",
    "\n",
    "    figfile = f\"{emulator_dir}/emulator_posterior.pdf\"\n",
    "    print(f\"Saving figure to {figfile}\")\n",
    "    fig.savefig(figfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed74959",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84257391",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e5a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02617982",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ce49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join([f\"-{k} {d[k]}\" for k in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3727a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 100)\n",
    "a = torch.distributions.Binomial(total_count=9,probs=torch.tensor(x)).log_prob(torch.tensor([6])).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62557bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "plt.plot(x, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec308745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
